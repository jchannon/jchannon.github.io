<rss xmlns:a10="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Jonathan Channon Blog</title><link>http://blog.jonathanchannon.com/feed.xml</link><description>Jonathan Channon Blog</description><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/05/07/introducing-owin-statelessauth-with-nancy-angular-demo/</guid><link>http://blog.jonathanchannon.com/2014/05/07/introducing-owin-statelessauth-with-nancy-angular-demo/</link><title>Introducing Owin.StatelessAuth with Nancy/Angular demo</title><description>&lt;p&gt;If you're writing an API, current thinking is to provide a token in the &lt;code&gt;Authorization&lt;/code&gt; header for your app to validate when the request comes in.  I have used the &lt;a href="http://www.nuget.org/packages/Nancy.Authentication.Stateless/"&gt;Nancy.Authentication.Stateless&lt;/a&gt; package in the past for my APIs and even have a demo of it &lt;a href="https://github.com/jchannon/Nancy.Demo.StatelessAuth"&gt;here&lt;/a&gt; if you're interested (there are more Nancy demos at &lt;a href="http://samples.nancyfx.org/"&gt;http://samples.nancyfx.org&lt;/a&gt;). This is a great package and does a great job but what if one day you want to use &lt;a href="http://www.asp.net/signalr"&gt;SignalR&lt;/a&gt; v2 that uses &lt;a href="http://owin.org/"&gt;OWIN&lt;/a&gt; and you want to validate not just requests to your Nancy app but also the SignalR requests?  You're going to need to validate requests as they come in before they get to SignalR or Nancy.&lt;/p&gt;

&lt;p&gt;For those of you who are not quite up to date or unsure what OWIN is let me try and give you the tl:dr, no doubt others may say its something slightly different.  Imagine you are asked to create a ASP.Net MVC 3 app (ignore the fact that that person needs a slap) so you fire up Visual Studio and create the app.  So what has it done? Its created an app that runs on IIS and all requests come straight into your app.   &lt;/p&gt;

</description><pubDate>Tue, 06 May 2014 23:00:00 Z</pubDate><a10:updated>2014-05-06T23:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;If you're writing an API, current thinking is to provide a token in the &lt;code&gt;Authorization&lt;/code&gt; header for your app to validate when the request comes in.  I have used the &lt;a href="http://www.nuget.org/packages/Nancy.Authentication.Stateless/"&gt;Nancy.Authentication.Stateless&lt;/a&gt; package in the past for my APIs and even have a demo of it &lt;a href="https://github.com/jchannon/Nancy.Demo.StatelessAuth"&gt;here&lt;/a&gt; if you're interested (there are more Nancy demos at &lt;a href="http://samples.nancyfx.org/"&gt;http://samples.nancyfx.org&lt;/a&gt;). This is a great package and does a great job but what if one day you want to use &lt;a href="http://www.asp.net/signalr"&gt;SignalR&lt;/a&gt; v2 that uses &lt;a href="http://owin.org/"&gt;OWIN&lt;/a&gt; and you want to validate not just requests to your Nancy app but also the SignalR requests?  You're going to need to validate requests as they come in before they get to SignalR or Nancy.&lt;/p&gt;

&lt;p&gt;For those of you who are not quite up to date or unsure what OWIN is let me try and give you the tl:dr, no doubt others may say its something slightly different.  Imagine you are asked to create a ASP.Net MVC 3 app (ignore the fact that that person needs a slap) so you fire up Visual Studio and create the app.  So what has it done? Its created an app that runs on IIS and all requests come straight into your app.   &lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h3&gt;Enter OWIN&lt;/h3&gt;

&lt;p&gt;What OWIN introduces is an HTTP abstraction from the host to framework and therefore you have access to the whole request at any point. As an example, &lt;a href="https://github.com/bbaia/connect-owin-samples/tree/master/Samples.Nancy"&gt;here&lt;/a&gt; is a node.js web server (replacing IIS) and then calling out to Nancy.  Pretty cool huh!  As HTTP is abstracted you can have two applications, one in Nancy and one in WebAPI in the same project and via OWIN you can tell it which requests go to Nancy and which go to WebAPI.&lt;/p&gt;

&lt;h3&gt;Authentication&lt;/h3&gt;

&lt;p&gt;Due to the HTTP abstraction we can now inspect the requests and then determine whether we should return a 401 or let the request continue. So how does that look?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Startup
{
    public void Configuration(IAppBuilder app)
    {
         app
           .RequiresStatelessAuth(new MySecureTokenValidator())
           .MapSignalR() //This could be WebAPI etc
           .UseNancy();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In an OWIN app we need a Startup class to configure our application and we wire up the requests and how they may be handled in order of processing.  So as I stated earlier we want to use SignalR and Nancy and validate the requests before they hit our application, using &lt;a href="https://www.nuget.org/packages/Owin.StatelessAuth/"&gt;Owin.StatelessAuth&lt;/a&gt; we can do that.  It takes an implementation of &lt;code&gt;ITokenValidator&lt;/code&gt; where a method gets called to determine if the request is valid by passing in a token from the &lt;code&gt;Authorization&lt;/code&gt; header.  How you implement the interface and determine what is a valid request is up to you.  Luckily I have a demo available in the &lt;a href="https://github.com/jchannon/Owin.StatelessAuth"&gt;Github repository&lt;/a&gt; which I'll now explain.&lt;/p&gt;

&lt;h3&gt;Demo Time&lt;/h3&gt;

&lt;p&gt;About 2 days after publishing &lt;a href="https://github.com/jchannon/Owin.StatelessAuth"&gt;Owin.StatelessAuth&lt;/a&gt;, Mike Hadlow published a great &lt;a href="http://mikehadlow.blogspot.co.uk/2014/04/json-web-tokens-owin-and-angularjs.html"&gt;blog post&lt;/a&gt; on using JWT (JavaScript Web Tokens) &amp;amp; OWIN &amp;amp; Angular so I thought I would do a similar post just to throw my 2 cents in.  Its going to be hard not to say the same things as Mike so I may skip some stuff but it just means you should read his post too!  So lets get the code to do the talking...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Startup.cs&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Startup
{
    public void Configuration(IAppBuilder app)
    {
        app.RequiresStatelessAuth(
              new MySecureTokenValidator(new ConfigProvider()), 
              new RequireStatelessAuthOptions() {IgnorePaths = new List&amp;lt;string&amp;gt;(new []{"/login","/content"})})
            .UseNancy();

    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we pass in our implementation of ITokenValidator called MySecureTokenValidator and pass in some options to Owin.StatelessAuth which says if the paths contain the items in the list then Owin.StatelessAuth will not try and authenticate those requests.  In the demo we have javascript and images in the content folder so we don't want to authenticate those requests.  We also don't want to authenticate requests to the login path.  Why not? This is the route that will give us the token for all subsequent requests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nancy Module&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public HomeModule(IConfigProvider configProvider, IJwtWrapper jwtWrapper)
{
    Get["/login"] = _ =&amp;gt; View["Login"];

    Post["/login"] = _ =&amp;gt;
    {
        var user = this.Bind&amp;lt;UserCredentials&amp;gt;();

        //Verify user/pass
        if (user.User != "fred" &amp;amp;&amp;amp; user.Password != "securepwd")
        {
            return 401;
        }

        var jwttoken = new JwtToken()
        {
            Issuer = "http://issuer.com",
            Audience = "http://mycoolwebsite.com",
            Claims =
                new List&amp;lt;Claim&amp;gt;(new[]
                {
                    new Claim(ClaimTypes.Role, "Administrator"),
                    new Claim(ClaimTypes.Name, "Fred")
                }),
            Expiry = DateTime.UtcNow.AddDays(7)
        };

        var token = jwtWrapper.Encode(jwttoken, configProvider.GetAppSetting("securekey"), JwtHashAlgorithm.HS256);
        return Negotiate.WithModel(token);
    };

    Get["/"] = _ =&amp;gt; "Hello Secure World!";
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here on the GET request to login we return a view where Angular wil be used.  On the POST request to login we Bind the posted values to a class called UserCredentials, we then need to validate these credentials (I assume yours will be better than mine) and then create a new instance of JwtToken which is just another class in our application which has properties that relate to the &lt;a href="http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html"&gt;JWT spec&lt;/a&gt; and then we encode the object to return a token for our user using the &lt;a href="https://www.nuget.org/packages/JWT/"&gt;JWT&lt;/a&gt; library (I have created a wrapper for it in the demo as they are static methods out of the box).  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Angular View&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here's the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en" xmlns="http://www.w3.org/1999/xhtml" ng-app="owinstatelessauthexample"&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;meta charset="utf-8" /&amp;gt;
    &amp;lt;title&amp;gt;&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body ng-controller="appCtrl"&amp;gt;
    &amp;lt;h1&amp;gt;Login&amp;lt;/h1&amp;gt;
    &amp;lt;form ng-submit="getToken()"&amp;gt;
        &amp;lt;input type="text" name="user" ng-model="user" /&amp;gt;
        &amp;lt;input type="password" name="password" ng-model="password" /&amp;gt;
        &amp;lt;input type="submit" value="Login" /&amp;gt;
    &amp;lt;/form&amp;gt;
    &amp;lt;label&amp;gt;Status: {{loggedinstatus}}&amp;lt;/label&amp;gt;
    &amp;lt;span&amp;gt;{{secureresponse}}&amp;lt;/span&amp;gt;
    &amp;lt;button ng-click="getsecureresponse()"&amp;gt;Get Secure Response&amp;lt;/button&amp;gt;

    &amp;lt;script src="/content/localforage.js"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.2.16/angular.min.js"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src="/content/angular-localforage.js"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src="/Content/app.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have a form that will POST to our login route, a label to show our logged in status, a button to hit our route that should return "Hello Secure World"&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Angular Code&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(function () {
    'use strict';

    var app = angular.module('owinstatelessauthexample', ['LocalForageModule'])
        .controller('appCtrl', ['$scope', '$localForage', '$http', function ($scope, $localForage, $http) {
            // Start fresh
            $localForage.clearAll();

            $scope.user = 'fred';
            $scope.password = 'securepwd';
            $scope.secureresponse = '';
            $scope.loggedinstatus = 'Not Logged In';

            $scope.getToken = function () {
                $http({
                    method: 'POST',
                    url: '/login',
                    data: {
                        "user": $scope.user,
                        "password": $scope.password,
                    }
                })
                    .success(function (data, status) {
                        console.log('All ok : ' + data);
                        $localForage.setItem('mysecuretoken', JSON.parse(data));
                        $scope.loggedinstatus = 'Logged In';
                    })
                    .error(function (data, status) {
                        console.log('Oops : ' + data);
                    });

            };

            $scope.getsecureresponse = function () {
                $localForage.get('mysecuretoken').then(function (data) {
                    $http({
                        method: 'GET',
                        url: '/',
                        headers: { 'Authorization': data }

                    })
                   .success(function (data, status) {
                       console.log('All secure : ' + data);
                       $scope.secureresponse = data;
                   })
                   .error(function (data, status) {
                       console.log('Oops : ' + data);
                       $scope.secureresponse = "Oops!" + data;
                   });
                });

            };
        }]);

})();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the form from our view is posted to our login route we get take the response data and store it in localStorage.  However, here we are using a library called &lt;a href="https://github.com/mozilla/localForage"&gt;localForage&lt;/a&gt; which has a fallback option if you don't have HTML5 in your browser.  When the user clicks the button to hit our secure route it will retrieve the token from localForage and pass it in the request and hopefully we get the expected response as Owin.StatelessAuth will validate it via MySecureTokenValidator.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MySecureTokenValidator&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Claims don't deserialize :(
//var jwttoken = JsonWebToken.DecodeToObject&amp;lt;JwtToken&amp;gt;(token, configProvider.GetAppSetting("securekey"));

var decodedtoken = JsonWebToken.DecodeToObject(token, configProvider.GetAppSetting("securekey")) as Dictionary&amp;lt;string, object&amp;gt;;

var jwttoken = new JwtToken()
{
    Audience = (string)decodedtoken["Audience"],
    Issuer = (string)decodedtoken["Issuer"],
    Expiry = DateTime.Parse(decodedtoken["Expiry"].ToString()),
};

if (decodedtoken.ContainsKey("Claims"))
{
    var claims = new List&amp;lt;Claim&amp;gt;();

    for (int i = 0; i &amp;lt; ((ArrayList)decodedtoken["Claims"]).Count; i++)
    {
        var type = ((Dictionary&amp;lt;string, object&amp;gt;)((ArrayList)decodedtoken["Claims"])[i])["Type"].ToString();
        var value = ((Dictionary&amp;lt;string, object&amp;gt;)((ArrayList)decodedtoken["Claims"])[i])["Value"].ToString();
        claims.Add(new Claim(type, value));
    }

    jwttoken.Claims = claims;
}

if (jwttoken.Expiry &amp;lt; DateTime.UtcNow)
{
    return null;
}

return new ClaimsPrincipal(new ClaimsIdentity(jwttoken.Claims, "Token"));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My first comment in the code is that the class &lt;a href="http://msdn.microsoft.com/en-us/library/system.identitymodel.claims.claim(v=vs.110).aspx"&gt;Claim&lt;/a&gt; won't deserialize which would have made our code a one liner but unfortunately not. Possibly if the JWT library used JSON.Net or ServiceStack.Text it may work but for now I had to do some logic to assign the properties of the JwtToken class.  So we decode the token to a dictionary and then assign the values to our class, loop over the claims, see if the expiry date is before now and if so return null which will cause Owin.StatelessAuth to return a 401.  If all is well we return a &lt;a href="http://msdn.microsoft.com/en-GB/library/system.security.claims.claimsprincipal.aspx"&gt;ClaimsPrincipal&lt;/a&gt; instance.  Owin.StatelessAuth will add it to the Owin environment which can be read further down the request stack.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nancy Bootstrapper&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Bootstrapper : DefaultNancyBootstrapper
{
    protected override void RequestStartup(TinyIoCContainer container, IPipelines pipelines, NancyContext context)
    {
        base.RequestStartup(container, pipelines, context);
        var owinEnvironment = context.GetOwinEnvironment();
        var user = owinEnvironment["server.User"] as ClaimsPrincipal;
        if (user != null)
        {
            context.CurrentUser = new DemoUserIdentity()
            {
                UserName = user.Identity.Name,
                Claims = user.Claims.Where(x =&amp;gt; x.Type == "http://schemas.microsoft.com/ws/2008/06/identity/claims/role").Select(x =&amp;gt; x.Value)
            };
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nancy has a CurrentUser property on the NancyContext, if this is not null then we know the user is authenticated.  In the introduction of the blog post I mentioned Nancy.Authentication.Stateless (other Nancy.Authentication libraries are available) which does exactly that, it assigns the the CurrentUser to the validated user.  In our Bootstrapper we use the ClaimsPrincipal instance in the Owin environment that Owin.StatelessAuth put in there for us to assign the properties of &lt;code&gt;IUserIdentity&lt;/code&gt; in Nancy to assign the current user.  We can then use &lt;code&gt;RequiresAuthentication&lt;/code&gt; on our Nancy routes to secure routes based on extra security such as claim types.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;What we have now is a way using Owin.StatelessAuth to secure all incoming requests, the option to ignore some requests for authentication, a way for tokens to be issued, a way for them to be validated and the ability to assign Nancy's user to the user we validated using Owin.StatelessAuth.&lt;/p&gt;

&lt;p&gt;I enjoyed writing Owin.StatelessAuth middleware component and the demo with it so please take a look, any constructive feedback welcomed along with pull requests :)&lt;/p&gt;

&lt;p&gt;Finally just to prove this works, here's some pretty pictures:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;POST to generate a token&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/FlI4NAi.png" alt="Post Get Token" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GET with our token&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/GeCP9IJ.png" alt="Get Secure Request" /&gt;&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/03/22/using-node-and-ftp-with-promises/</guid><link>http://blog.jonathanchannon.com/2014/03/22/using-node-and-ftp-with-promises/</link><title>Using NodeJS and FTP with Promises</title><description>&lt;p&gt;I've played with node in the &lt;a href="http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/"&gt;past&lt;/a&gt; but as of the new year I decided to try and make a more concerted effort to get stuck into node properly.  I decided to go back to the beginning to try and get a better appreciation for the language so read "JavaScript: The Good Parts by Douglas Crockford".  I found that exercise fulfilling and resulted in a few light bulb moments that made some dots join up so I'd recommend reading it if you haven't already.&lt;/p&gt;

&lt;h3&gt;Real World App&lt;/h3&gt;

&lt;p&gt;As I stated earlier I have already played with node in the past using &lt;a href="http://expressjs.com/"&gt;Express&lt;/a&gt; and have read quite a bit on node and read many examples but I wanted to write a non-web app as I felt this would give me a better opportunity to get to grips with the language and Node. Using Express allows you to get up and running very quickly without to much head scratching so I felt a standalone script would give me more exposure to things.&lt;/p&gt;

</description><pubDate>Sat, 22 Mar 2014 00:00:00 Z</pubDate><a10:updated>2014-03-22T00:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;I've played with node in the &lt;a href="http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/"&gt;past&lt;/a&gt; but as of the new year I decided to try and make a more concerted effort to get stuck into node properly.  I decided to go back to the beginning to try and get a better appreciation for the language so read "JavaScript: The Good Parts by Douglas Crockford".  I found that exercise fulfilling and resulted in a few light bulb moments that made some dots join up so I'd recommend reading it if you haven't already.&lt;/p&gt;

&lt;h3&gt;Real World App&lt;/h3&gt;

&lt;p&gt;As I stated earlier I have already played with node in the past using &lt;a href="http://expressjs.com/"&gt;Express&lt;/a&gt; and have read quite a bit on node and read many examples but I wanted to write a non-web app as I felt this would give me a better opportunity to get to grips with the language and Node. Using Express allows you to get up and running very quickly without to much head scratching so I felt a standalone script would give me more exposure to things.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;During the previous couple of weeks at work I wrote a console app that downloaded zip file from a FTP server, extract the contents, read data in a XML file that was in the zip, do some string matching and upload the zip to another FTP server.  I figured this would be a good app to replicate in node so off I went.&lt;/p&gt;

&lt;p&gt;After a bit of &lt;a href="http://npmjs.org"&gt;npm&lt;/a&gt; research I found the modules I needed and managed to get to the point of downloading files pretty easily with the below code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var path = require('path');
var fs = require('fs');
var Promise = require('bluebird');
var Client = require('ftp');

var c = new Client();

var connectionProperties = {
    host: "myhost",
    user: "myuser",
    password: "mypwd"
};

c.on('ready', function () {
    console.log('ready');
    c.list(function (err, list) {
        if (err) throw err;
        list.forEach(function (element, index, array) {
            //Ignore directories
            if (element.type === 'd') {
                console.log('ignoring directory ' + element.name);
                return;
            }
            //Ignore non zips
            if (path.extname(element.name) !== '.zip') {
                console.log('ignoring file ' + element.name);
                return;
            }
            //Download files
            c.get(element.name, function (err, stream) {
                if (err) throw err;
                stream.once('close', function () {
                    c.end();
                });
                stream.pipe(fs.createWriteStream(element.name));
            });
        });
    });
});

c.connect(connectionProperties);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, I originally had that code in a function and wanted to call it and then call another function to read the files that I had downloaded but what I found was callback hell.&lt;/p&gt;

&lt;h3&gt;Enter Promises&lt;/h3&gt;

&lt;p&gt;I needed to know that all the files had downloaded and then I could read the files in a directory ready for zip extraction but I couldn't work out how.  I discovered promises and probably didn't read enough about all the ins and outs of them but I remember &lt;a href="http://twitter.com/gblock"&gt;Glenn Block&lt;/a&gt; giving a talk about &lt;a href="https://github.com/glennblock/codemash-async"&gt;async programming in node&lt;/a&gt; so I pestered him on Twitter and he kindly helped and me out and also pointed me towards his code and slides where I decided to use &lt;a href="https://github.com/petkaantonov/bluebird/"&gt;Bluebird&lt;/a&gt;, the promise library.  Unfortunately I just couldn't get the files downloaded. It would download one file but not the other and closed the streams.&lt;/p&gt;

&lt;p&gt;Here is a snippet of what I had (brace yourself)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var processListing = function (directoryItems) {
    var itemsToDownload = [];
    directoryItems.forEach(function (element, index, array) {
        //Ignore directories
        if (element.type === 'd') {
            console.log('directory ' + element.name);
            return;
        }
        //Ignore non zips
        if (path.extname(element.name) !== '.zip') {
            console.log('ignoring ' + element.name);
            return;
        }
        //Download zip
        itemsToDownload.push({
            source: element.name,
            destination: element.name
        });
    });
    return itemsToDownload;
};

var processItem = function (object) {
    return aFtpClient.getAsync(object.source);
};

var downloadFiles = function () {
    console.log('downloading files');
    aFtpClient.
    listAsync().
    then(processListing).
    map(function (object) {
        return processItem(object).then(function (processResult) {
            return {
                input: object,
                result: processResult
            };
        });
    }).
    map(function (downloadItem) {
        downloadItem.result.pipe(fs.createWriteStream(process.cwd() + "/zips/" + downloadItem.input.destination));
        return new Promise(function (resolve, reject) {
            downloadItem.result.once("close", function () {
                console.log('closed');
                resolve();
            });
        });
    }).done()
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not only is that a tad complicated but I could not for the life of me understand what the hell was happening and why it wasn't downloading all the files.  I reached out to &lt;a href="https://twitter.com/PrabirShrestha"&gt;@PrabirShrestha&lt;/a&gt; who agreed it was a tad over complicated and tried to help but recommended I take a look at Reactive Extensions, maybe I will in the future but at this point my frustration had kicked in and I wanted to give up.  I went through a mixture of emotions from frustration, which led to anger, fuming anger, denial, then apathy.  Although these emotions went by and after a couple of questions on stackoverflow that helped but didn't give the solution I explained the issue to a colleague and we both took a look.  I went through a few iterations with no luck and after a bit more reading I think we were closing in on it individually but I was beaten to it. All hail &lt;a href="http://twitter.com/iamnerdfury"&gt;@iamnerdfury&lt;/a&gt; who produced this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var connect = function() {
    c.connect(connectionProperties);
    return c.onAsync('ready');
};

var getList = function() {
    return c.listAsync();
};

var zipFiles = function(element) {
    return element.type !== 'd' &amp;amp;&amp;amp; path.extname(element.name) === '.zip';
};

var current = Promise.resolve();

var downloadFiles = function(file) {
    current = current.then(function() {
        return c.getAsync(file.name)
    }).then(function(stream) {
        stream.pipe(fs.createWriteStream(file.name));
        console.log(file.name + ' downloaded..');
    });
    return current;
};

connect().then(getList).filter(zipFiles).map(downloadFiles).done();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think the previous issues I had was I was returning &lt;code&gt;resolve()&lt;/code&gt; after the first file downloaded which is not what you want to do when multiple calls to it are executed as a promise can only resolve once.  I needed to find some way of concatenating a promise somehow for each file that is downloaded. I looked at the &lt;code&gt;all()&lt;/code&gt; command but I couldn't get it to fit but @iamnerdfury found that you could do this via creating an instance of a promise by calling resolve and then assign to it on each file that needed to be downloaded.&lt;/p&gt;

&lt;p&gt;Now I know the files are downloaded I can chain more functions to read the file system, extract the zip for each one, read the XML and upload to a new server.&lt;/p&gt;

&lt;p&gt;I hope this helps someone else because it wound me up something chronic and whilst I get pissed off with JavaScript when things like this happen I will keep at it because I think node is now becoming a serious contender and us developers need to keep a finger in many pies.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If you think there is a way to improve the solution above I'd love to hear it)&lt;/em&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/01/02/unit-testing-with-sqlexception/</guid><link>http://blog.jonathanchannon.com/2014/01/02/unit-testing-with-sqlexception/</link><title>Unit Testing with SqlException</title><description>&lt;p&gt;So after a nice Christmas break I get to some code that needs some unit testing around a try/catch. Something similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;try
{
    myService.DoSomethingThatMightTakeALongTime();
}
catch (EntityCommandExecutionException ex)
{
    var exception = ex.InnerException as SqlException;
    if (exception != null)
    {
        if (exception.Number == -2)
        {
            //Do something special
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Thu, 02 Jan 2014 00:00:00 Z</pubDate><a10:updated>2014-01-02T00:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;So after a nice Christmas break I get to some code that needs some unit testing around a try/catch. Something similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;try
{
    myService.DoSomethingThatMightTakeALongTime();
}
catch (EntityCommandExecutionException ex)
{
    var exception = ex.InnerException as SqlException;
    if (exception != null)
    {
        if (exception.Number == -2)
        {
            //Do something special
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Obviously &lt;code&gt;myService&lt;/code&gt; has a interface that can be mocked and I can tell it to throw a &lt;code&gt;EntityCommandExecutionException&lt;/code&gt; when &lt;code&gt;DoSomethingThatMightTakeALongTime&lt;/code&gt; is called and the constructor for that takes a string and an Exception as an inner exception.  However, you can't create a new instance of SqlException because its a sealed class therefore doing the below is impossible:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var fakeService = A.Fake&amp;lt;IMyService&amp;gt;();
A.CallTo(() =&amp;gt; fakeService.DoSomethingThatMightTakeALongTime()).Throws(new EntityCommandExecutionException("What a mistaka da maka", new SqlException());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can't create your own exception class and inherit off SqlException to get around it that way either.  You could use &lt;code&gt;System.Runtime.Serialization.FormatterServices.GetUninitializedObject&lt;/code&gt; to give you a &lt;code&gt;SqlException&lt;/code&gt; but that won't have the &lt;code&gt;Number&lt;/code&gt; property assigned to -2.  You could also setup a method in your test class that tries to connect to a non existant db that times out after 1 second but again that won't give you the Number property you may want plus its a lot of ugly and unnecessary code in a unit test project.  &lt;/p&gt;

&lt;h2&gt;How did you do it?&lt;/h2&gt;

&lt;p&gt;So after browsing all the stackoverflow answers and comments I came up with a solution that worked which I thought I'd share so here it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private SqlException GetSqlException()
{
    SqlErrorCollection collection = Construct&amp;lt;SqlErrorCollection&amp;gt;();
    SqlError error = Construct&amp;lt;SqlError&amp;gt;(-2, (byte)2, (byte)3, "server name", "error message", "proc", 100, (uint)1);

    typeof(SqlErrorCollection)
        .GetMethod("Add", BindingFlags.NonPublic | BindingFlags.Instance)
        .Invoke(collection, new object[] { error });    

    var e = typeof(SqlException)
        .GetMethod("CreateException", BindingFlags.NonPublic | BindingFlags.Static, null, CallingConventions.ExplicitThis, new[] { typeof(SqlErrorCollection), typeof(string) }, new ParameterModifier[] { })
        .Invoke(null, new object[] { collection, "11.0.0" }) as SqlException;

    return e;
}

private T Construct&amp;lt;T&amp;gt;(params object[] p)
{
    return (T)typeof(T).GetConstructors(BindingFlags.NonPublic | BindingFlags.Instance)[0].Invoke(p);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then amend the previous mocking code to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var fakeService = A.Fake&amp;lt;IMyService&amp;gt;();
A.CallTo(() =&amp;gt; fakeService.DoSomethingThatMightTakeALongTime()).Throws(new EntityCommandExecutionException("What a mistaka da maka", GetSqlException());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see it uses reflection to create instances of all the sealed classes required and it also calls sealed methods to assign properties ie/adding the error instance to the collection instance.  You'll see that the -2 value is the first argument in the parameters used to construct the SqlError object so if you're interested in using the Number property on the exception thats where to change it.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This approach works and allows me to test my code but all in all its not particualry elegant and the following gif can sum up what we've learnt from sealed methods and classes and thats they're &lt;em&gt;nasty&lt;/em&gt;: &lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/pR3tklc.gif" alt="Nasty" /&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/12/20/using-sql-server-with-nodejs/</guid><link>http://blog.jonathanchannon.com/2013/12/20/using-sql-server-with-nodejs/</link><title>Using SQL Server with node.js</title><description>&lt;p&gt;I like to keep eyes and ears open for new technologies and methodologies in order to become a better developer and I'd heard about &lt;a href="http://tjanczuk.github.io/edge/#/"&gt;edge.js&lt;/a&gt; many months ago but made a mental note of it and waved it goodbye.  edge.js lets you have two-way communication between node and C# libraries.  When I first looked at it I thought that sounded a bit hacky, I've spent my time communicating with COM libraries in Delphi and OCX libraries with C# and didn't like it so I felt this was pretty much the same thing.  A long time passed and I was writing a console based Windows app as a service and had wondererd whether I could quickly port it to node.  &lt;/p&gt;

&lt;p&gt;I was discussing with a colleague about using node at work and that we needed something seperate and small just to try it out and see how the whole developement process with it worked.  As the database that this app needed to communicate with was MSSQL I looked into a library on NPM that would communicate with MSSQL and maybe act as an ORM.  There was a Microsoft lib that seemed untouched and reading the comments on the issues list on Github it didnt favour too well.  There were libraries that would communicate with MySQL &amp;amp; PostgresSQL but not MSSQL.  In my search I came across edge.js again.  It had 2 samples, one that used edge-sql and one that used ScriptCS so in laymans terms, one that used a precompiled dll and one that used a C# script that was executed at runtime.&lt;/p&gt;

</description><pubDate>Fri, 20 Dec 2013 00:00:00 Z</pubDate><a10:updated>2013-12-20T00:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;I like to keep eyes and ears open for new technologies and methodologies in order to become a better developer and I'd heard about &lt;a href="http://tjanczuk.github.io/edge/#/"&gt;edge.js&lt;/a&gt; many months ago but made a mental note of it and waved it goodbye.  edge.js lets you have two-way communication between node and C# libraries.  When I first looked at it I thought that sounded a bit hacky, I've spent my time communicating with COM libraries in Delphi and OCX libraries with C# and didn't like it so I felt this was pretty much the same thing.  A long time passed and I was writing a console based Windows app as a service and had wondererd whether I could quickly port it to node.  &lt;/p&gt;

&lt;p&gt;I was discussing with a colleague about using node at work and that we needed something seperate and small just to try it out and see how the whole developement process with it worked.  As the database that this app needed to communicate with was MSSQL I looked into a library on NPM that would communicate with MSSQL and maybe act as an ORM.  There was a Microsoft lib that seemed untouched and reading the comments on the issues list on Github it didnt favour too well.  There were libraries that would communicate with MySQL &amp;amp; PostgresSQL but not MSSQL.  In my search I came across edge.js again.  It had 2 samples, one that used edge-sql and one that used ScriptCS so in laymans terms, one that used a precompiled dll and one that used a C# script that was executed at runtime.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Looking at the samples the Github repo gave you could do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var edge = require('edge');

var getTop10Products = edge.func('sql', function () {/*
    select top 10 * from Products
*/});

getTop10Products(null, function (error, result) {
    if (error) throw error;
    console.log(result);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thats it, you could call &lt;code&gt;node myscript&lt;/code&gt; and it would log out the values of the result variable.  &lt;/p&gt;

&lt;p&gt;What this did was in fact send the SQL string to a compiled dll which had a class and async method in it that was setup to respond to calls from node js.  This method essentially returned a C# &lt;code&gt;List&amp;lt;object&amp;gt;&lt;/code&gt; that was serialized to JSON so the node.js function could interact with it.  The one issue I saw with it was the actual format of the JSON.  It was a 2 dimentional array, with the first array in the parent array containing the column names and the subsequent arrays containing values from the rows in the SQL result.  &lt;/p&gt;

&lt;h2&gt;Time to roll up your sleeves&lt;/h2&gt;

&lt;p&gt;Whilst I liked the fact that I could now return data from MSSQL with node its format wasnt quite right.  I forked the project on Github and then looked at the way it was executing the SQL and storing it in a &lt;code&gt;List&amp;lt;object&amp;gt;&lt;/code&gt;.  Whilst I kept the &lt;code&gt;List&amp;lt;object&amp;gt;&lt;/code&gt; return type the information inside it differed.  I was now using &lt;code&gt;var dataObject = new ExpandoObject() as IDictionary&amp;lt;string, Object&amp;gt;;&lt;/code&gt; and for each field in the resulting SQL dataset I populated it like so &lt;code&gt;dataObject.Add(record.GetName(i), resultRecord[i]);&lt;/code&gt; ie/ the column name and corresponding value.  So this looped over the sql storing objects in a list and then returning it as JSON as it did before.  What this meant was that the API had now changed so I could refer to the column names as object properties on the node object.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;getTop10Products(null, function (error, result) {
    if (error) throw error;
    console.log(result[0].ProductName);
    console.log(result[1].UnitPrice);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bingo!&lt;/p&gt;

&lt;p&gt;So now just out of curisotiy I wanted to right a sample ExpressJS app to see how I could use this to have a JS file that acted as a C# repository to do all the data access.  I'll let you look into setting express up yourself but what I managed to do was this:&lt;/p&gt;

&lt;h4&gt;server.js&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;var express = require('express');
var edge = require('edge');
var index = require('./index.js');
var db = require('./db.js');

var app = express();

app.get('/', index.home(db));

app.listen(999);
console.log('Listening on port 999')
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;db.js&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;var edge = require('edge');

exports.getProducts = edge.func('sql', function() {/*
                    select top 10 * from Products 
                 */});
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;index.js&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;exports.home = function(db) {
    return function(req, res) {
        db.getProducts(null, function(error, result) {
            if (error) throw error;
            var data = {};
            data.all = result;
            data.Item1Name = result[0].ProductName;
            data.Item2ReorderLevel = result[1].ReorderLevel;
            res.send(data);
        });
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I fired up a browser and pointed it at http://localhost:999 and it returned showed me my 10 products, then my first item's product name and second item's re-order level. Consider me pleased!&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I know some people will think using MSSQL for a node app seems odd but if you want to spike something up and/or only have access to a MSSQL db for whatever reason you can now do it very easily and actually quite elegantly.  You execute your SQL and you get back a JSON object that represents your data, same as any other SQL/NOSQL database.  Give it a whirl and see how you get on!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/11/25/octopus-xml-transformation-in-services/</guid><link>http://blog.jonathanchannon.com/2013/11/25/octopus-xml-transformation-in-services/</link><title>Octopus XML Transformation in Services</title><description>&lt;p&gt;We use &lt;a href="http://octopusdeploy.com/"&gt;Octopus Deploy&lt;/a&gt; at work and its a superb tool for deploying your applications whether they be websites or *.exes.&lt;/p&gt;

&lt;p&gt;One of the great things it also provides is the ability to use &lt;a href="http://msdn.microsoft.com/en-us/library/dd465326.aspx"&gt;Microsoft's Transformation&lt;/a&gt; process for config files.  However, when deploying a exe application its a bit trickier than a website.  Unfortunately the documentation doesn't mention the steps needed to get this working so read on!  &lt;/p&gt;

&lt;p&gt;Typically a web application will have web.config and a web.Release.config as well as other derivations you may use.  Octopus also supports web.[Environment].config.&lt;/p&gt;

</description><pubDate>Mon, 25 Nov 2013 00:00:00 Z</pubDate><a10:updated>2013-11-25T00:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;We use &lt;a href="http://octopusdeploy.com/"&gt;Octopus Deploy&lt;/a&gt; at work and its a superb tool for deploying your applications whether they be websites or *.exes.&lt;/p&gt;

&lt;p&gt;One of the great things it also provides is the ability to use &lt;a href="http://msdn.microsoft.com/en-us/library/dd465326.aspx"&gt;Microsoft's Transformation&lt;/a&gt; process for config files.  However, when deploying a exe application its a bit trickier than a website.  Unfortunately the documentation doesn't mention the steps needed to get this working so read on!  &lt;/p&gt;

&lt;p&gt;Typically a web application will have web.config and a web.Release.config as well as other derivations you may use.  Octopus also supports web.[Environment].config.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;In a console application you have an app.config and maybe a app.Release.config if you create one.  Deploying this via Octopus won't invoke the XML transformation.&lt;/p&gt;

&lt;p&gt;The trick is to rename the app.Release.config to be the name of the final config file produced by the build along with the 'exe' extension in it and to make sure in Visual Studio you set the build to Copy Always on the MyApp.exe.Release.config file.&lt;/p&gt;

&lt;p&gt;So for example if your project is called MyApp and you have an app.config and app.Release.config, open Windows Explorer and rename it to MyApp.exe.Release.config.  Visual Studio won't allow you to rename these files that are dependent on another so you now have to open up MyApp.csproj and alter the references from app.Release.config to MyApp.exe.Release.config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Content Include="App.config" /&amp;gt;
&amp;lt;Content Include="MyApp.exe.Release.config" &amp;gt;
    &amp;lt;DependentUpon&amp;gt;App.Config&amp;lt;/DependentUpon&amp;gt;
&amp;lt;/Content&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go into VS, Reload All when it prompts you and set the Copy to Output Directory value to Copy Always&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/E8Kbezh.jpg" alt="VS Property Window" /&gt;&lt;/p&gt;

&lt;p&gt;Now when you deploy Octopus should run the transformation and replace connection strings etc as you would expect.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/11/07/using-git-to-update-youtrack-via-teamcity/</guid><link>http://blog.jonathanchannon.com/2013/11/07/using-git-to-update-youtrack-via-teamcity/</link><title>Using Git to update YouTrack via TeamCity</title><description>&lt;p&gt;This post is mainly a reminder for me as I keep forgetting the command in Git to integrate commits to YouTrack items.&lt;/p&gt;

&lt;p&gt;YouTrack uses TeamCity to get the information about the commits and then scans the commit comment for a YouTrack item id and any commands that it can apply such as item status or time spent on said item.&lt;/p&gt;

&lt;p&gt;There is some documentation &lt;a href="http://confluence.jetbrains.com/display/YTD4/Executing+Commands+from+Comment+to+VCS+Commit"&gt;here&lt;/a&gt; but its not the greatest in terms of clarity and I've spoken to &lt;a href="https://twitter.com/hhariri"&gt;Hadi Hariri&lt;/a&gt; from JetBrains about improving this so hopefully they're working on it.&lt;/p&gt;

&lt;p&gt;Anyhow here's some example Git commands to wire it all up&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git commit -am "I fixed a massive bug #PROJ-158 Complete"
git commit -am "I fixed a massive bug #PROJ-158 Complete add work 1h"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first command will update the status of YouTrack item #PROJ-158 to Complete.  The second item will do the same but also add Time Tracking information to the item in YouTrack.&lt;/p&gt;

&lt;p&gt;Hope that helps, Happy Coding!&lt;/p&gt;
</description><pubDate>Thu, 07 Nov 2013 00:00:00 Z</pubDate><a10:updated>2013-11-07T00:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;This post is mainly a reminder for me as I keep forgetting the command in Git to integrate commits to YouTrack items.&lt;/p&gt;

&lt;p&gt;YouTrack uses TeamCity to get the information about the commits and then scans the commit comment for a YouTrack item id and any commands that it can apply such as item status or time spent on said item.&lt;/p&gt;

&lt;p&gt;There is some documentation &lt;a href="http://confluence.jetbrains.com/display/YTD4/Executing+Commands+from+Comment+to+VCS+Commit"&gt;here&lt;/a&gt; but its not the greatest in terms of clarity and I've spoken to &lt;a href="https://twitter.com/hhariri"&gt;Hadi Hariri&lt;/a&gt; from JetBrains about improving this so hopefully they're working on it.&lt;/p&gt;

&lt;p&gt;Anyhow here's some example Git commands to wire it all up&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git commit -am "I fixed a massive bug #PROJ-158 Complete"
git commit -am "I fixed a massive bug #PROJ-158 Complete add work 1h"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first command will update the status of YouTrack item #PROJ-158 to Complete.  The second item will do the same but also add Time Tracking information to the item in YouTrack.&lt;/p&gt;

&lt;p&gt;Hope that helps, Happy Coding!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/10/21/running-mocha-tests-with-sublime-text/</guid><link>http://blog.jonathanchannon.com/2013/10/21/running-mocha-tests-with-sublime-text/</link><title>Running Mocha tests within Sublime Text</title><description>&lt;p&gt;I spend most of my day in Visual Studio with lots of the goodies an IDE can offer.  One of them being able to run your tests from a keystroke.&lt;/p&gt;

&lt;p&gt;In a bid to expand my mind I'm working on a little project that is made up of JS entirely so I've dug out &lt;a href="http://sublimetext.com"&gt;Sublime Text&lt;/a&gt;. It has lots of plugins that are very handy, especially &lt;a href="https://github.com/victorporof/Sublime-HTMLPrettify"&gt;Sublime-HTMLPrettify&lt;/a&gt; which will tidy your HTML, CSS &amp;amp; JS for you.&lt;/p&gt;

&lt;p&gt;When writing tests for JS there are many libraries you can use but I've chosen &lt;a href="http://visionmedia.github.io/mocha/"&gt;Mocha&lt;/a&gt; for now.  The one thing I couldn't work out was to run my tests within Sublime Text until now.&lt;/p&gt;

&lt;h3&gt;Build System&lt;/h3&gt;

&lt;p&gt;Sublime allows you to have build systems a bit like an IDE so you can tell it what to do when you invoke it via &lt;kbd&gt;cmd&lt;/kbd&gt;+&lt;kbd&gt;B&lt;/kbd&gt;.&lt;/p&gt;

&lt;p&gt;To get Mocha to run we need to create a new build system. To do this click Tools - Build System - New Build System and paste in the below:&lt;/p&gt;

</description><pubDate>Sun, 20 Oct 2013 23:00:00 Z</pubDate><a10:updated>2013-10-20T23:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;I spend most of my day in Visual Studio with lots of the goodies an IDE can offer.  One of them being able to run your tests from a keystroke.&lt;/p&gt;

&lt;p&gt;In a bid to expand my mind I'm working on a little project that is made up of JS entirely so I've dug out &lt;a href="http://sublimetext.com"&gt;Sublime Text&lt;/a&gt;. It has lots of plugins that are very handy, especially &lt;a href="https://github.com/victorporof/Sublime-HTMLPrettify"&gt;Sublime-HTMLPrettify&lt;/a&gt; which will tidy your HTML, CSS &amp;amp; JS for you.&lt;/p&gt;

&lt;p&gt;When writing tests for JS there are many libraries you can use but I've chosen &lt;a href="http://visionmedia.github.io/mocha/"&gt;Mocha&lt;/a&gt; for now.  The one thing I couldn't work out was to run my tests within Sublime Text until now.&lt;/p&gt;

&lt;h3&gt;Build System&lt;/h3&gt;

&lt;p&gt;Sublime allows you to have build systems a bit like an IDE so you can tell it what to do when you invoke it via &lt;kbd&gt;cmd&lt;/kbd&gt;+&lt;kbd&gt;B&lt;/kbd&gt;.&lt;/p&gt;

&lt;p&gt;To get Mocha to run we need to create a new build system. To do this click Tools - Build System - New Build System and paste in the below:&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;pre&gt;&lt;code&gt;{
    "cmd": ["make"],
    "file_regex": "^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$",
    "working_dir": "${project_path:${folder:${file_path}}}",
    "selector": "source.makefile",
    "shell": true,
    "variants": [{
        "name": "Clean",
        "cmd": ["make", "clean"]
    }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Click Save and call it Mocha&lt;/p&gt;

&lt;p&gt;Now when you have a project go to Tools - Build System and select Mocha&lt;/p&gt;

&lt;h3&gt;Make file&lt;/h3&gt;

&lt;p&gt;A Make file is a script that allows you to execute various commands and its what our build system looks for when we tell Sublime to build our project. We need a file called &lt;code&gt;makefile&lt;/code&gt; in the root of our project.  Inside that &lt;code&gt;makefile&lt;/code&gt; we can invoke Mocha to run our tests.&lt;/p&gt;

&lt;p&gt;Place this in your &lt;code&gt;makefile&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test:
    mocha --recursive --reporter spec moviebucketlist.tests/*.js
.PHONY: test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now when you invoke the build system via &lt;kbd&gt;cmd&lt;/kbd&gt;+&lt;kbd&gt;B&lt;/kbd&gt; in Sublime it will execute Mocha and give you the results in the console of Sublime.  Mocha by default will look for a folder called &lt;code&gt;test&lt;/code&gt; and execute the tests inside it. If you have a different folder name you can append the folder name and wildcard to js files like I have done above.&lt;/p&gt;

&lt;p&gt;Happy Coding!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/10/01/blogging-with-markdown-and-git/</guid><link>http://blog.jonathanchannon.com/2013/10/01/blogging-with-markdown-and-git/</link><title>Blogging with Markdown &amp; Deploying via Git - Introducing Sandra.Snow</title><description>&lt;p&gt;There are many markdown blogging engines out there such as &lt;a href="http://calepin.co/"&gt;Calepin&lt;/a&gt;, &lt;a href="http://scriptogr.am/"&gt;Scriptogram&lt;/a&gt; and even &lt;a href="http://wordpress.org/"&gt;WordPress&lt;/a&gt; allows you to write blog posts in Markdown but &lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Sandra.Snow&lt;/a&gt; tries to add something different.  Firstly, it is written in .Net and &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt;, secondly its a static blog generator and finally it supports Git deployment.&lt;/p&gt;

&lt;p&gt;Even if you don't want to use Git deployment you can use FTP, its a great tool.  To write your blog post in Markdown you need a custom header in your file so it knows some information about your post.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
layout: post
category: Azure
title: Setting up a ServiceStack Service
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It then parses this information along with your Markdown into its engine, uses a Markdown view engine to convert the file content into HTML, assign model properties based on the header and creates a HTML file using the model via a Razor viewengine.&lt;/p&gt;

&lt;p&gt;The "layout" refers to the Razor file it uses to render the final HTML file.  This allows you to style your pages and blog posts whichever way you'd prefer.  These "layout" files should exist in the "_layouts" folder for your site template.  The site template is a set of files and folders that Sandra.Snow uses to produce the final static website.&lt;/p&gt;

&lt;p&gt;The "category" or "categories" property, you can use both for singular or multiple comma-seperated values that refer to the category/categories of your blog post.&lt;/p&gt;

&lt;p&gt;The "title" should hopefully be self explanatory!&lt;/p&gt;

&lt;p&gt;You can optionally add an author and email properties to override the global config settings for example, if you wanted to allow guest author blog posts.  There is also an optional metadescription property you can use for SEO.
</description><pubDate>Mon, 30 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-30T23:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;There are many markdown blogging engines out there such as &lt;a href="http://calepin.co/"&gt;Calepin&lt;/a&gt;, &lt;a href="http://scriptogr.am/"&gt;Scriptogram&lt;/a&gt; and even &lt;a href="http://wordpress.org/"&gt;WordPress&lt;/a&gt; allows you to write blog posts in Markdown but &lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Sandra.Snow&lt;/a&gt; tries to add something different.  Firstly, it is written in .Net and &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt;, secondly its a static blog generator and finally it supports Git deployment.&lt;/p&gt;

&lt;p&gt;Even if you don't want to use Git deployment you can use FTP, its a great tool.  To write your blog post in Markdown you need a custom header in your file so it knows some information about your post.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
layout: post
category: Azure
title: Setting up a ServiceStack Service
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It then parses this information along with your Markdown into its engine, uses a Markdown view engine to convert the file content into HTML, assign model properties based on the header and creates a HTML file using the model via a Razor viewengine.&lt;/p&gt;

&lt;p&gt;The "layout" refers to the Razor file it uses to render the final HTML file.  This allows you to style your pages and blog posts whichever way you'd prefer.  These "layout" files should exist in the "_layouts" folder for your site template.  The site template is a set of files and folders that Sandra.Snow uses to produce the final static website.&lt;/p&gt;

&lt;p&gt;The "category" or "categories" property, you can use both for singular or multiple comma-seperated values that refer to the category/categories of your blog post.&lt;/p&gt;

&lt;p&gt;The "title" should hopefully be self explanatory!&lt;/p&gt;

&lt;p&gt;You can optionally add an author and email properties to override the global config settings for example, if you wanted to allow guest author blog posts.  There is also an optional metadescription property you can use for SEO.
&lt;!--excerpt--&gt;&lt;/p&gt;

&lt;h3&gt;Global Config&lt;/h3&gt;

&lt;p&gt;In the root of the site template is a snow.config file which is what Sandra.Snow uses to determine url format, where to look for posts and layouts and other related information. It is JSON formatted and looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  "blogTitle" : "Joe Bloggs Blog",
  "author" : "Mr.Guest",
  "email" : "guest@gmail.com",
  "siteUrl": "http://blog.joebloggs.com",
  "posts": "Snow/_posts",
  "layouts": "Snow/_layouts",
  "output": "../MYRelativeWebsiteFolder",
  "urlFormat": "yyyy/MM/dd/slug",
  "copyDirectories": [
    "Snow/images =&amp;gt; images",
    "Snow/js =&amp;gt; js",
    "Snow/css =&amp;gt; css"
  ],
  "processFiles": [{
    "file": "Snow/index.cshtml",
    "loop": "Posts"
  },{
    "file": "Snow/category.cshtml",
    "loop": "Categories"
  },{
    "file": "Snow/categories.cshtml =&amp;gt; categories"
  },{
    "file": "Snow/archive.cshtml =&amp;gt; archive"
  },{
    "file": "Snow/about.cshtml =&amp;gt; about"
  },{
    "file": "Snow/contact.cshtml =&amp;gt; contact"
  },{
    "file": "feed.xml",
    "loop": "RSS"
  },{
    "file": "sitemap.xml",
    "loop": "sitemap"
  }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is an explanation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;"blogTitle" : The title of the blog you want to appear on your RSS feed&lt;/li&gt;
&lt;li&gt;"author" : The author's name&lt;/li&gt;
&lt;li&gt;"email" : The author's email.(You can use an &lt;a href="https://github.com/Sandra/Sandra.Snow/wiki/Gravatar-Support"&gt;HTMLHelper&lt;/a&gt; in the view that gets the author's Gravatar from the global/post settings)&lt;/li&gt;
&lt;li&gt;"siteUrl": "This is used to enable Disqus support. Simply use an &lt;a href="https://github.com/Sandra/Sandra.Snow/wiki/Disqus-Support"&gt;HTMLHelper&lt;/a&gt; to render Disqus comments"&lt;/li&gt;
&lt;li&gt;"posts" : The location of the markdown files&lt;/li&gt;
&lt;li&gt;"layouts" : The location of the layout files&lt;/li&gt;
&lt;li&gt;"output" : The location where Sandra.Snow will put the static HTML. This is relative&lt;/li&gt;
&lt;li&gt;"urlFormat" : The format of the URL to your blog post&lt;/li&gt;
&lt;li&gt;"copyDirectories" : The directories in your template that it will copy to the output&lt;/li&gt;
&lt;li&gt;&lt;p&gt;"processFiles" : This takes an object of the filename and property information on how to render the file.  Each file/view will be called and rendered with model information availble.  The model information available to these views are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public List&amp;lt;Post&amp;gt; PostsInCategory { get; set; }
public Dictionary&amp;lt;int, Dictionary&amp;lt;int, List&amp;lt;Post&amp;gt;&amp;gt;&amp;gt; PostsGroupedByYearThenMonth { get; set; }
public List&amp;lt;Post&amp;gt; Posts { get; set; }
public List&amp;lt;Post&amp;gt; PostsPaged { get; set; }


public bool HasPreviousPage { get; set; }
public bool HasNextPage { get; set; }
public int NextPage { get; set; }
public int PreviousPage { get; set; }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the standard website template there are category, categories, about and index *.cshtml pages which accept this model information and render the relevant information. Based on the file name you can guess what each file outputs.  The "loop" in the settings is used internally by Sandra.Snow to process the relevant data. For example "RSS" creates a RSS file based on the list of posts while Posts/Categories create sub-directories for the relevant model type eg/categories/posts.  "sitemap" will use the &lt;code&gt;List&amp;lt;Post&amp;gt;&lt;/code&gt; to create a sitemap.xml in the root of your blog. &lt;/p&gt;

&lt;p&gt;As Sandra.Snow is a static HTML generator it will create folders with the relevant name for the post or file named in the config file eg/&lt;code&gt;http://mydomain.com/2013/08/18/this-is-a-great-article&lt;/code&gt; or &lt;code&gt;http://mydomain.com/categories&lt;/code&gt; and create a &lt;code&gt;index.html&lt;/code&gt; file for each folder.  In the root of the website it will create a &lt;code&gt;index.html&lt;/code&gt; with 10 blog posts inside it.  If you have 100 markdown posts it will it will page this for you and create links to &lt;code&gt;http://mydomain.com/page2&lt;/code&gt; etc.  If you use &lt;code&gt;&amp;lt;!--excerpt--&amp;gt;&lt;/code&gt; in your Markdown it will read up to that point so you can click a "read more" link otherwise it will use the whole Markdown content.    If you look at the model properties you'll see your index layout view can tell if there is a next/previous page and therefore create the relevant links in the HTML.&lt;/p&gt;

&lt;p&gt;Once you run the Sandra.Snow exe it will output the HTML and you can then FTP your files to your website.&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href="https://github.com/Sandra/Sandra.Snow/wiki"&gt;wiki&lt;/a&gt; for more details about other HTMLHelpers such as Google Analytics.&lt;/p&gt;

&lt;h3&gt;Git Integration&lt;/h3&gt;

&lt;p&gt;FTP is so 2001 so Sandra.Snow has a website called Sandra.Snow.Barbato which allows you to access it (final URL to be confirmed) and log in with your Github credentials.  It will then give you a list of your repositories, the idea being one of them is your blog with the markdown posts and snow.config etc.  A &lt;a href="https://github.com/Sandra/Sandra.Snow.BarbatoTemplate"&gt;base template&lt;/a&gt; is available in the Sandra repository for you to fork.  You can then choose whether you'd like to deploy to another Git repository that supports Git deployment eg/Azure, AppHarbor, Heroku or you can select FTP.  In either scenario, enter your details and off you go. The website will use Sandra.Snow to create the output and it will then wire it over to your chosen destination.  &lt;/p&gt;

&lt;p&gt;Sandra.Snow.Barbato is also setup to handle Github hooks so in Github if you tell your repository to do a post commit hook to the website, after you write a new blog post and push to Github it will post to the website and know if you've previously logged in and if so generate the HTML and re-deploy your blog.  It will also push the generated content back to your Github repository on the master branch. Very nice!&lt;/p&gt;

&lt;h3&gt;Setting up Sandra.Snow.Barbato&lt;/h3&gt;

&lt;p&gt;One you have forked the Barbato template you can begin to style your blog pages.  Obviously every time you want to make a style change you want to see the results.  There are 2 ways to do this.  i) Run Sandra.Snow locally, setup a webserver eg/IISExpress to point to the Snow output directory and open up your browser to see the changes. Keep making changes to the *.cshtml and *.css files until happy ii) Make the changes in your repo, sign up with Sandra.Snow.Barbato and go to your domain to check the changes that were deployed.&lt;/p&gt;

&lt;h4&gt;Azure&lt;/h4&gt;

&lt;p&gt;If deploying to Azure you must have a .deployment file in the root of your repository that contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[config]
project = Website
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is needed because when your template repository is pushed to Azure it needs to know what to deploy. This simply tells it to use the Website folder ie.the output folder from Sandra.Snow.&lt;/p&gt;

&lt;h4&gt;Github Pages&lt;/h4&gt;

&lt;p&gt;If deploying to Github you need a few things. Your repository needs to be called &lt;code&gt;username.github.io&lt;/code&gt;. You then need to create a CNAME file in your repo that has the domain you will be using inside it. Finally you need to setup the DNS on your domain to point to &lt;code&gt;username.github.io&lt;/code&gt; by creating a CNAME record in your DNS Manager. What you'll have to probably do is clone your &lt;code&gt;username.github.io&lt;/code&gt; repo and run Snow and set the output setting in snow.config to the path of your &lt;code&gt;username.github.io&lt;/code&gt; folder. You can then push the changes to Github. &lt;/p&gt;

&lt;h3&gt;Wordpress Migration&lt;/h3&gt;

&lt;p&gt;My blog was previously using Wordpress so I needed to get my data out.  The most common referred to tool was &lt;a href="https://github.com/dreikanter/wp2md"&gt;wp2md&lt;/a&gt; which uses Python to go through the exported Wordpress content and then convert to Markdown.  For some reason I didn't go with that choice and went with &lt;a href="http://heckyesmarkdown.com/"&gt;http://heckyesmarkdown.com/&lt;/a&gt;.  Its a bit more work because you have to give it your previous URLs to your blog posts and it reads the source of the page and converts it to Markdown.  It worked brilliantly for me.  I had to make a few changes on the output it provided by generally it was very good.&lt;/p&gt;

&lt;p&gt;As I didn't want to worry about HTTP 302, I made sure I saved my markdown files as the urls are on my live site so &lt;a href="http://blog.jonathanchannon.com/2012/12/19/why-use-nancyfx/"&gt;http://blog.jonathanchannon.com/2012/12/19/why-use-nancyfx/&lt;/a&gt; was saved in a file called &lt;code&gt;2012-12-19-why-use-nancyfx.md&lt;/code&gt;. This file naming format is currently enforced so Snow can gather date and slug information(unsafe characters in the slug/title for URLs will be removed).&lt;/p&gt;

&lt;p&gt;I then went through addind the meta headers to tell Sandra.Snow a bit more about the posts and also added in the &lt;code&gt;&amp;lt;!--excerpt--&amp;gt;&lt;/code&gt; information so not to render the whole blog content on the home pages.&lt;/p&gt;

&lt;p&gt;I then went through and styled the master page &lt;code&gt;default.cshtml&lt;/code&gt; in the _layouts folder as well as the &lt;code&gt;post.cshtml&lt;/code&gt; and the other files in the root of the site template folder.&lt;/p&gt;

&lt;p&gt;Once done I ran the .exe file to generate my content.  One of the great things about Sandra.Snow is its speed. It takes less than a second to do 100 blog posts, luckily I only have 25 so its really fast.  I opened up a browser and checked my files and if some styling needed tweaking I could do so and re-run.  Once all ok I can deploy or push the template folder to Github, setup the post commit hook and then use Sandra.Snow.Barbato to handle deployment from now on.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;If you're a Git and Markdown user and want to create a blog with complete simplicity this is a great tool.  No more messy Wordpress, no more running exe's on your machine (unless you want to), its completely automated apart from writing the blog posts!  In fact I'm so happy with this project, this blog is using it!  Give it a try and if you like the look of it get involved with its development.  &lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Sandra.Snow&lt;/a&gt; the new modern, simplistic and effective tool for blogging.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/09/20/returning-multiple-fake-objects-with-fakeiteasy/</guid><link>http://blog.jonathanchannon.com/2013/09/20/returning-multiple-fake-objects-with-fakeiteasy/</link><title>Returning multiple fake objects with FakeItEasy</title><description>&lt;p&gt;I was recently writing some unit tests where I needed to test that multiple calls to an interface returned different objects.  &lt;/p&gt;

&lt;p&gt;With &lt;a href="https://github.com/FakeItEasy/FakeItEasy"&gt;FakeItEasy&lt;/a&gt; this is easy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All very nice, but now if I have multiple calls to &lt;code&gt;myInterface&lt;/code&gt; I have to execute the above statement 'x' amount of times:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void Should_Do_Something()
{
  var myInterface = A.Fake&amp;lt;IApplication&amp;gt;();
  A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(2)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(3)).Returns(new Something());

  var result = sut.DoSomething(myInterface);

  Assert.Equal("Super Duper", result);
}
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Thu, 19 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-19T23:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;I was recently writing some unit tests where I needed to test that multiple calls to an interface returned different objects.  &lt;/p&gt;

&lt;p&gt;With &lt;a href="https://github.com/FakeItEasy/FakeItEasy"&gt;FakeItEasy&lt;/a&gt; this is easy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All very nice, but now if I have multiple calls to &lt;code&gt;myInterface&lt;/code&gt; I have to execute the above statement 'x' amount of times:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void Should_Do_Something()
{
  var myInterface = A.Fake&amp;lt;IApplication&amp;gt;();
  A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(2)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(3)).Returns(new Something());

  var result = sut.DoSomething(myInterface);

  Assert.Equal("Super Duper", result);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;There is a tidier way to do the above where you can return specific objects and its called &lt;code&gt;ReturnsLazily&lt;/code&gt;.  Lets take a look at this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Employee
{
    public string Name { get; set; }
}

public interface IEmployeeRepository
{
    Employee GetEmployeeById(int id);
}

public class App
{
    private readonly IEmployeeRepository employeeRepository;

    public App(IEmployeeRepository employeeRepository)
    {
        this.employeeRepository = employeeRepository;
    }

    public string GetNamesAsCsv(int[] ids)
    {
        var employees = ids.Select(id =&amp;gt; employeeRepository.GetEmployeeById(id).Name);
        return string.Join(",", employees);
    }
}

public class AppTests
{
    [Fact]
    public void AppReturnsNamesAsCsv()
    {
        //Given
        var employees = new Dictionary&amp;lt;int, Employee&amp;gt;
        {
            { 1, new Employee { Name = "Moss"} },
            { 2, new Employee { Name = "Roy"} },
        };

        var fakeRepository = A.Fake&amp;lt;IEmployeeRepository&amp;gt;();
        A.CallTo(() =&amp;gt; fakeRepository.GetEmployeeById(A&amp;lt;int&amp;gt;.Ignored))
            .ReturnsLazily&amp;lt;Employee, int&amp;gt;(id =&amp;gt; employees[id]);

        var app = new App(fakeRepository);

        //When
        var result = app.GetNamesAsCsv(employees.Keys.ToArray());

        //Then
        Assert.Equal("Moss,Roy", result);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have an &lt;code&gt;Employee&lt;/code&gt; object, a &lt;code&gt;IEmployeeRepository&lt;/code&gt; which returns an &lt;code&gt;Employee&lt;/code&gt; object and an App that returns a CSV.  We then want to test this and make sure we get back a CSV from multiple objects.&lt;/p&gt;

&lt;p&gt;So we set our fake setup and say that when &lt;code&gt;GetEmployeeById&lt;/code&gt; is called we want to return a specific object.  Our App class will call &lt;code&gt;GetEmployeeById&lt;/code&gt; twice with the id of 1 and 2.  This is done by passing in &lt;code&gt;employees.Keys.ToArray()&lt;/code&gt; to our GetNamesAsCsv method under test. &lt;/p&gt;

&lt;p&gt;When this is called with the id we want to return specific objects &lt;code&gt;.ReturnsLazily&amp;lt;Employee, int&amp;gt;(id =&amp;gt; employees[id]);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This says we want to return an Employee and the argument in the repository call is an int.  We can then use that to return a specific object based on that id which is where the &lt;code&gt;Dictionary&amp;lt;int, Employee&amp;gt;&lt;/code&gt; comes in handy.  Based on the key it will return either an Employee called Moss or Roy.  Our &lt;code&gt;GetNamesAsCsv&lt;/code&gt; will then join Moss &amp;amp; Roy together as a CSV and we can assert that our method works.&lt;/p&gt;

&lt;p&gt;Hope that helps someone!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/09/16/enabling-cors-in-iisexpress/</guid><link>http://blog.jonathanchannon.com/2013/09/16/enabling-cors-in-iisexpress/</link><title>Enabling CORS in IISExpress</title><description>&lt;p&gt;I was playing around with &lt;a href="https://github.com/wordnik/swagger-ui"&gt;swagger-ui&lt;/a&gt; and was trying to point it to a local endpoint that I started with IIS Express.  I was getting an error saying that it needed the endpoint to accept Access-Control-Allow-Origin requests.&lt;/p&gt;

&lt;p&gt;I went Googling and it couldn't find anything specific to IIS Express but managed to use some guidance for full blown IIS.&lt;/p&gt;

&lt;p&gt;The solution is to go to &lt;code&gt;C:\Program Files (x86)\IIS Express\AppServer&lt;/code&gt; and open the &lt;code&gt;applicationhost.config&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Search for &lt;code&gt;httpProtocol&lt;/code&gt; and you should see this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;httpProtocol&amp;gt;
    &amp;lt;customHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
        &amp;lt;add name="X-Powered-By" value="ASP.NET" /&amp;gt;
    &amp;lt;/customHeaders&amp;gt;
    &amp;lt;redirectHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
    &amp;lt;/redirectHeaders&amp;gt;
&amp;lt;/httpProtocol&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now add this to the &lt;code&gt;customHeaders&lt;/code&gt; node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;add name="Access-Control-Allow-Origin" value="*" /&amp;gt;
&amp;lt;add name="Access-Control-Allow-Headers" value="Content-Type" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just bear in mind this opens up your webserver so you may need to find something alternative for a live production environment.&lt;/p&gt;

&lt;p&gt;Anyway you should now be able to start accepting requests via CORS when you fire up IISExpress&lt;/p&gt;
</description><pubDate>Sun, 15 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-15T23:00:00Z</a10:updated><a10:content type="text">&lt;p&gt;I was playing around with &lt;a href="https://github.com/wordnik/swagger-ui"&gt;swagger-ui&lt;/a&gt; and was trying to point it to a local endpoint that I started with IIS Express.  I was getting an error saying that it needed the endpoint to accept Access-Control-Allow-Origin requests.&lt;/p&gt;

&lt;p&gt;I went Googling and it couldn't find anything specific to IIS Express but managed to use some guidance for full blown IIS.&lt;/p&gt;

&lt;p&gt;The solution is to go to &lt;code&gt;C:\Program Files (x86)\IIS Express\AppServer&lt;/code&gt; and open the &lt;code&gt;applicationhost.config&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Search for &lt;code&gt;httpProtocol&lt;/code&gt; and you should see this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;httpProtocol&amp;gt;
    &amp;lt;customHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
        &amp;lt;add name="X-Powered-By" value="ASP.NET" /&amp;gt;
    &amp;lt;/customHeaders&amp;gt;
    &amp;lt;redirectHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
    &amp;lt;/redirectHeaders&amp;gt;
&amp;lt;/httpProtocol&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now add this to the &lt;code&gt;customHeaders&lt;/code&gt; node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;add name="Access-Control-Allow-Origin" value="*" /&amp;gt;
&amp;lt;add name="Access-Control-Allow-Headers" value="Content-Type" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just bear in mind this opens up your webserver so you may need to find something alternative for a live production environment.&lt;/p&gt;

&lt;p&gt;Anyway you should now be able to start accepting requests via CORS when you fire up IISExpress&lt;/p&gt;
</a10:content></item></channel></rss>