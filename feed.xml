<rss xmlns:a10="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Jonathan Channon Blog</title><link>http://blog.jonathanchannon.com/feed.xml</link><description>Jonathan Channon Blog</description><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2017/06/07/debugging-netcore-docker/</guid><link>http://blog.jonathanchannon.com/2017/06/07/debugging-netcore-docker/</link><a10:author><a10:name /></a10:author><category>ASP.NET</category><category>C#</category><category>Docker</category><category>OSS</category><title>Debugging .Net Core apps inside Docker container with VSCode</title><description>&lt;p&gt;So by now using .Net Core on Linux is old news, everyone is doing it and deploying their production apps on Kubernetes to reach peak "I can scale" points.  However, one thing that can get tricky is when you have a requirement to debug an application in a container.  I believe VS on Windows and VS for Mac has some sort of capability to do that (I have no idea what it does underneath but hey who cares I can right click debug right!?) but the information about doing this in VSCode is a bit sketchy.  I tend to use VSCode on OSX the most so I wanted to see how I could do this.&lt;/p&gt;

&lt;p&gt;For demonstration purposes lets take a very simple application and we are going to publish it as a self contained application ie/one that has all the runtime and application binaries outputted so you don't have to install dotnet in a container.&lt;/p&gt;

&lt;p&gt;To be able to debug that application we are going to need VSDBG(the .Net Core command line debugger) inside the container.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l ~/vsdbg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We are also going to need to append the launch.json for VSCode in your project's root to have the below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "name": ".NET Core Remote Attach",
    "type": "coreclr",
    "request": "attach",
    "processId": "${command:pickRemoteProcess}",
    "pipeTransport": {
        "pipeProgram": "bash",
        "pipeArgs": [ "-c", "docker exec -i json ${debuggerCommand}" ],
        "debuggerPath": "/root/vsdbg/vsdbg",
        "pipeCwd": "${workspaceRoot}",
        "quoteArgs": true
    },
    "sourceFileMap": {
        "/Users/jonathan/Projects/jsonfile": "${workspaceRoot}"
    },
    "justMyCode": true
}
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Tue, 06 Jun 2017 23:00:00 Z</pubDate><a10:updated>2017-06-06T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;So by now using .Net Core on Linux is old news, everyone is doing it and deploying their production apps on Kubernetes to reach peak "I can scale" points.  However, one thing that can get tricky is when you have a requirement to debug an application in a container.  I believe VS on Windows and VS for Mac has some sort of capability to do that (I have no idea what it does underneath but hey who cares I can right click debug right!?) but the information about doing this in VSCode is a bit sketchy.  I tend to use VSCode on OSX the most so I wanted to see how I could do this.&lt;/p&gt;

&lt;p&gt;For demonstration purposes lets take a very simple application and we are going to publish it as a self contained application ie/one that has all the runtime and application binaries outputted so you don't have to install dotnet in a container.&lt;/p&gt;

&lt;p&gt;To be able to debug that application we are going to need VSDBG(the .Net Core command line debugger) inside the container.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l ~/vsdbg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We are also going to need to append the launch.json for VSCode in your project's root to have the below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "name": ".NET Core Remote Attach",
    "type": "coreclr",
    "request": "attach",
    "processId": "${command:pickRemoteProcess}",
    "pipeTransport": {
        "pipeProgram": "bash",
        "pipeArgs": [ "-c", "docker exec -i json ${debuggerCommand}" ],
        "debuggerPath": "/root/vsdbg/vsdbg",
        "pipeCwd": "${workspaceRoot}",
        "quoteArgs": true
    },
    "sourceFileMap": {
        "/Users/jonathan/Projects/jsonfile": "${workspaceRoot}"
    },
    "justMyCode": true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;They key things to note are the &lt;code&gt;pipeArgs&lt;/code&gt; and &lt;code&gt;sourceFileMap&lt;/code&gt;. Where it says &lt;code&gt;json&lt;/code&gt;, under &lt;code&gt;pipeArgs&lt;/code&gt; this will need to be replaced the name of the container that you are trying to debug.  The &lt;code&gt;sourceFileMap&lt;/code&gt; is a mapping between where it was compiled on your machine and where it is in VSCode.  The rest of the properties are explained &lt;a href="https://github.com/OmniSharp/omnisharp-vscode/wiki/Attaching-to-remote-processes#configuring-launchjson"&gt;here&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;The final Dockerfile looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM microsoft/dotnet:1.1-runtime-deps

RUN apt-get update

RUN apt-get install curl unzip

RUN curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l ~/vsdbg

COPY ./publish /app

WORKDIR /app

ENTRYPOINT ./jsonfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we're ready to go with the following steps:&lt;/p&gt;

&lt;p&gt;So we're ready to go, the steps are:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;dotnet publish -c Debug -f netcoreapp1.1 -r debian.8-x64 -o ./publish&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker build -t jchannon/jsonfile --rm .&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker run -t —name json jchannon/jsonfile&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Add a breakpoint to you application&lt;/p&gt;

&lt;p&gt;Go to VSCode Debug pane, select &lt;code&gt;.NET Core Remote Attach&lt;/code&gt; and hit F5&lt;/p&gt;

&lt;p&gt;SUCCESS!!&lt;/p&gt;

&lt;p&gt;One thing to note with this is, is that you cannot debug a project that has been compiled in Release mode.  Whilst the config above looks like it should work it doesn't. I tried! I believe there may be plans to allow this and the issue can be tracked &lt;a href="https://github.com/OmniSharp/omnisharp-vscode/issues/220"&gt;here&lt;/a&gt; .  A sample application and Dockerfile can be found &lt;a href="https://github.com/jchannon/DockerDebug"&gt;here&lt;/a&gt; &lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2017/05/15/using-docker-with-netcore-ci/</guid><link>http://blog.jonathanchannon.com/2017/05/15/using-docker-with-netcore-ci/</link><a10:author><a10:name /></a10:author><category>ASP.NET</category><category>C#</category><category>Docker</category><category>OSS</category><title>Using Docker with .Net Core in CI for OSS</title><description>&lt;p&gt;I recently wrote a &lt;a href="http://blog.jonathanchannon.com/2017/05/04/announcing-botwin/"&gt;project&lt;/a&gt; for &lt;a href="https://t.co/kpkdInRgwG"&gt;ASP.NET Core 2&lt;/a&gt;  and the time had come to get a CI system up and running.  I develop on OSX and mainly test on OSX &amp;amp; Linux and so the defacto place to go is TravisCI.  I've used it in the past and all has been great but I put out a tweet asking if Travis was still the place to go:&lt;/p&gt;

&lt;blockquote class="twitter-tweet" data-partner="tweetdeck"&gt;&lt;p lang="en" dir="ltr"&gt;Is Travis still the go to Linux CI tool for OSS?&lt;/p&gt;— Jonathan Channon (@jchannon) &lt;a href="https://twitter.com/jchannon/status/860979690462474240"&gt;May 6, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

</description><pubDate>Sun, 14 May 2017 23:00:00 Z</pubDate><a10:updated>2017-05-14T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I recently wrote a &lt;a href="http://blog.jonathanchannon.com/2017/05/04/announcing-botwin/"&gt;project&lt;/a&gt; for &lt;a href="https://t.co/kpkdInRgwG"&gt;ASP.NET Core 2&lt;/a&gt;  and the time had come to get a CI system up and running.  I develop on OSX and mainly test on OSX &amp;amp; Linux and so the defacto place to go is TravisCI.  I've used it in the past and all has been great but I put out a tweet asking if Travis was still the place to go:&lt;/p&gt;

&lt;blockquote class="twitter-tweet" data-partner="tweetdeck"&gt;&lt;p lang="en" dir="ltr"&gt;Is Travis still the go to Linux CI tool for OSS?&lt;/p&gt;— Jonathan Channon (@jchannon) &lt;a href="https://twitter.com/jchannon/status/860979690462474240"&gt;May 6, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;&lt;a href="http://twitter.com/adron"&gt;Adron Hall&lt;/a&gt; replied and said he'd been using &lt;a href="http://codeship.com"&gt;Codeship&lt;/a&gt; as a Docker based CI system.  Having experience in Docker I thought I'd take a look.  My requirements were simple, I needed the CI to run &lt;code&gt;dotnet restore&lt;/code&gt; and &lt;code&gt;dotnet build&lt;/code&gt; and &lt;code&gt;dotnet test&lt;/code&gt;.  I also thought to myself how am I going to handle releasing a NuGet package? Normally I tend to run a build script locally check everything is ok before I push to NuGet so I can avoid an "oh-shit" release but it happens! &lt;/p&gt;

&lt;p&gt;I looked at Codeship's basic plan (free) and they supported most things on their systems out of the box but not .NET and so I moved to the pro plan (also free).  At this point Codeship's co-founder &amp;amp; CEO got in touch (&lt;a href="https://twitter.com/moritzplassnig"&gt;Moritz Plassnig&lt;/a&gt;) and had said he had seen the conversation on Twitter with Adron and was there to help.  I asked him about .NET Core etc and he confirmed that my choice to use the Pro account was the best decision and any other issues to give him a ping. Good stuff I thought!&lt;/p&gt;

&lt;p&gt;I began to read Codeship's &lt;a href="https://documentation.codeship.com/pro/quickstart/getting-started/"&gt;documentation&lt;/a&gt; and quite comprehensive it is I must say.  Essentially you have three files; a services file, a steps file and a docker file.  The services file describes your service eg a name, the Dockerfile path and things like a path to encrypted envionment variables plus many other settings available.  The steps file is a file where you describe each step in your CI system.  For me the first step was obviously to run &lt;code&gt;dotnet restore&lt;/code&gt; then another step for &lt;code&gt;dotnet build&lt;/code&gt; then &lt;code&gt;dotnet test&lt;/code&gt;.  I pushed my files to my repo and watched on Codeship's dashboard. The dotnet restore worked but the build failed.  The thing I lost some time on was that each step is run in its own container and I couldn't work out why the build was failing after I had successfully installed all the packages required for it to build.  Ironically I was reading the documentation for golang projects where it mentioned this!  During this process and me scratching my head I was tweeting Adron and Codeship to see if they knew why I was having issues and &lt;a href="https://twitter.com/kellyjandrews"&gt;Kelly Andrews, Codeship's Developer Advocate&lt;/a&gt; starting helping me out which was great.  He suggested I could use my Dockerfile to do the dotnet restore and build and then have a step to do the dotnet test.  That got me thinking and in the end I decided I would put the restore, build and test in the Dockerfile so when each push to the repo or PR is sent it would build the Dockerfile and although not part of the steps file Codeship would still report a failed build if it couldn't build the Docker image.  What I could use the steps file for was releasing to Nuget.  This felt a bit scary as it increased the potential of releasing something I wasn't happy with and I would end up releasing a "oh shit" patch release but I thought I'd just give it a go.  The way I could control this is a feature of Codeship's step files. (The files Codeship use is YAML).  In my steps file I could filter when the step was executed.  Here's the resulting file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- service: app
    tag: ^\d+.\d+.\d+(-.*|$)
    command: bash -c "dotnet pack -c Release -o /code/artifacts src/Botwin.csproj &amp;amp;&amp;amp; dotnet nuget push -s https://www.nuget.org/api/v2/package -k $NUGETAPIKEY /code/artifacts/Botwin.$CI_BRANCH.nupkg"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This says the service it belongs to is &lt;code&gt;app&lt;/code&gt; which is what I define in my services file.  The tag is the way to filter the step so when it sees something in a commit message or git tag then it will execute. (You can also define the inverse using &lt;code&gt;exclude&lt;/code&gt;, see the docs for more)  Finally the command to execute if it passes the tag regex.  In my file I have said execute this step if it sees &lt;code&gt;number.number.number&lt;/code&gt; or &lt;code&gt;number.number.number-something&lt;/code&gt; in a git tag.  So if I've done a load of work and I'm happy that a new version is ready to be released I do a &lt;code&gt;git tag 1.2.69&lt;/code&gt; and then a git push and Codeship will see this tag and then build the Docker image then it see the tag and then execute &lt;code&gt;dotnet pack&lt;/code&gt; and &lt;code&gt;dotnet nuget push&lt;/code&gt;.  Pretty good I thought and started to test it.  &lt;/p&gt;

&lt;p&gt;Codeship provides the same tooling that controls the CI process on their servers avaialble as a binary that can be installed via Homebrew so you can test the pipeline locally.  This tooling is called Jet.  So I followed the instructions and away I went.  Again I was lucky as I had Kelly on hand to answer my questions but the documentation was very good.  For example, as I wanted to publish to NuGet I needed to supply my API key and obviously didn't want that sitting in my repo but Codeship's docs described how you could pass in a file with the raw values, encrypt it using Jet, put the encrypted file in your repo and tell the services YAML file to look at the encrypted file to get environment variables out.  So above you can see I use &lt;code&gt;$NUGETAPIKEY&lt;/code&gt; and that comes from the encrypted file.  You'll also see that I use &lt;code&gt;$CI_BRANCH&lt;/code&gt;.  This is part of a number of environmental variables that Codeship provides that you have access too.  Here I could use the git tag &lt;code&gt;5.6.7-rc79&lt;/code&gt; which is found inside the &lt;code&gt;$CI_BRANCH&lt;/code&gt; environmental variable, slightly badly named IMO but it means I can get access to the version I have just built in this scenario as just before I do my git tag and push I also change the csproj version number so they need to match, then I tag and push and Codeship builds and tests and releases to NuGet for me.&lt;/p&gt;

&lt;p&gt;The other odd thing I did spot was the need to do &lt;code&gt;bash -c "multiple statements go here"&lt;/code&gt; for multiple statements in a steps file because if I ran just &lt;code&gt;dotnet restore&lt;/code&gt; all was fine but &lt;code&gt;dotnet restore &amp;amp;&amp;amp; dotnet build&lt;/code&gt; it didn't like it so I needed to add the bash prefix.  Thinking about it now I could move the dotnet restore, build and test to another step rather than make it part of the Dockerfile.  I'm not sure there are any advantages/disadvantages to either approach really as I don't think the layers in the Dockerfile when doing a restore/build are cached so it doesn't speed up CI time.&lt;/p&gt;

&lt;p&gt;When I got it all working I was pretty impressed and was thankful for the help I got from Kelly.  The project I used it for (&lt;a href=""&gt;Botwin&lt;/a&gt;) has fairly small requirements and there was lots of documentation I didn't even delve into so I think Codeship can probably provide a solution to much larger projects so please check them out.  I'm hoping services like this expand as .NET Core gains more traction in the *nix worlds and the binding to Windows that .NET has always had truly disappears and .NET becomes a proper cross platform runtime.  My next desire is to have a Linux .NET profiler, none exist currently although Jetbrains tell me they have some plans but it's a gap in the market if you're interested!&lt;/p&gt;

&lt;p&gt;&lt;a href="https://documentation.codeship.com/pro/quickstart/getting-started/"&gt;Link&lt;/a&gt; to getting started and defining services and steps YAML&lt;/p&gt;

&lt;p&gt;&lt;a href="https://documentation.codeship.com/pro/builds-and-configuration/cli/"&gt;Link&lt;/a&gt; to JET docs&lt;/p&gt;

&lt;p&gt;&lt;a href="https://documentation.codeship.com/pro/builds-and-configuration/environment-variables/#encrypted-environment-variables"&gt;Link&lt;/a&gt; to environment variables encryption&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2017/05/04/announcing-botwin/</guid><link>http://blog.jonathanchannon.com/2017/05/04/announcing-botwin/</link><a10:author><a10:name /></a10:author><category>ASP.NET</category><category>Botwin</category><category>C#</category><category>OSS</category><title>Announcing Botwin</title><description>&lt;p&gt;Whilst keeping my eye on what's going on in .NET Core v2 I came across some planned changes for ASP.NET Core regarding the &lt;a href="https://github.com/aspnet/Routing/blob/dev/src/Microsoft.AspNetCore.Routing/RequestDelegateRouteBuilderExtensions.cs"&gt;routing&lt;/a&gt;.  I had also read this &lt;a href="https://www.strathweb.com/2017/01/building-microservices-with-asp-net-core-without-mvc/"&gt;blog post&lt;/a&gt; from &lt;a href="https://twitter.com/filip_woj"&gt;Filip&lt;/a&gt; about using the planned changes for microservices and a lightbulb went off in my head.  I thought to myself I wonder if I could adapt the new extensions to create Nancy-esque routing.  Turns out, I could!&lt;/p&gt;

&lt;h3&gt;Sample&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;public class ActorsModule : BotwinModule
{
    public ActorsModule()
    {
        this.Get("/", async (req, res, routeData) =&amp;gt;
        {
            await res.WriteAsync("Hello World!");
        });
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;</description><pubDate>Wed, 03 May 2017 23:00:00 Z</pubDate><a10:updated>2017-05-03T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Whilst keeping my eye on what's going on in .NET Core v2 I came across some planned changes for ASP.NET Core regarding the &lt;a href="https://github.com/aspnet/Routing/blob/dev/src/Microsoft.AspNetCore.Routing/RequestDelegateRouteBuilderExtensions.cs"&gt;routing&lt;/a&gt;.  I had also read this &lt;a href="https://www.strathweb.com/2017/01/building-microservices-with-asp-net-core-without-mvc/"&gt;blog post&lt;/a&gt; from &lt;a href="https://twitter.com/filip_woj"&gt;Filip&lt;/a&gt; about using the planned changes for microservices and a lightbulb went off in my head.  I thought to myself I wonder if I could adapt the new extensions to create Nancy-esque routing.  Turns out, I could!&lt;/p&gt;

&lt;h3&gt;Sample&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;public class ActorsModule : BotwinModule
{
    public ActorsModule()
    {
        this.Get("/", async (req, res, routeData) =&amp;gt;
        {
            await res.WriteAsync("Hello World!");
        });
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;!--excerpt--&gt;
Whilst the extensions in the routing allowed users to create some funcs I thought to myself once you get above 3 or 4 of them you are going to want to put them in their own file which tidies things up but then you would still have to register all the routes in your application at one central location ie. in a Startup class or as part of the WebHostBuilder setup.  Whilst that's ok for some I didn't like it particularly so I came up with the BotwinModule.  Now I'm sure many of you who are &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt; lovers are thinking this looks exactly the same as a NancyModule and you'd be correct but sometimes you can't improve on perfection so I took what I knew from Nancy and made it work in a similar fashion.  Each BotwinModule is found and each route is registered with ASP.NET Core.  This is all under the hood, all the user has to do is below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Startup
{
    public void ConfigureServices(IServiceCollection services)
    {
        services.AddBotwin();
    }

    public void Configure(IApplicationBuilder app)
    {
        app.UseBotwin();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Extensions&lt;/h2&gt;

&lt;p&gt;When I had got the initial routing complete I then started going through some simple scenarios and realised I needed to add some extensions to make usability better.  At the moment this comes at a cost of dependencies and an opinionated approach.&lt;/p&gt;

&lt;h3&gt;Binding &amp;amp; Validating&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;this.Put("/actors/{id:int}", async (req, res, routeData) =&amp;gt;
{
    var result = req.BindAndValidate&amp;lt;Actor&amp;gt;();

    if (!result.ValidationResult.IsValid)
    {
        res.StatusCode = 422;
        await res.Negotiate(result.ValidationResult.GetFormattedErrors());
        return;
    }

    //Update the user in your database

    res.StatusCode = 204;
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code uses FluentValidation under the hood and validates the incoming request body to &lt;code&gt;Actor&lt;/code&gt;. The result of BindAndValidate is a Tuple of ValidationResult and T.  At the moment the ValidationResult is from FV so that probably needs abstracting at some point however, as you can see you can check if validation is valid and if not act accordingly.  In this example I return the errors from the validation result using an extension &lt;code&gt;GetFormattedErrors&lt;/code&gt; and also use a &lt;code&gt;HttpRequest&lt;/code&gt; extension that negotiates the result ie. if the user asked for JSON with their &lt;code&gt;Accept&lt;/code&gt; header they get JSON, if they asked for XML or PDF they get that (as long as they implement a &lt;code&gt;IResponseNegotiator&lt;/code&gt;). Out of the box Botwin will return JSON.&lt;/p&gt;

&lt;p&gt;If the user doesn't want to validate incoming data but does want the body deserialized they can simply use &lt;code&gt;req.Bind&amp;lt;Actor&amp;gt;()&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;Global Before &amp;amp; After Hooks&lt;/h3&gt;

&lt;p&gt;There may be circumstances where you want to check something in the request before it hits the route handler or you may want to do something after the route handler has been executed.  This can be setup via options:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void Configure(IApplicationBuilder app)
{
    app.UseBotwin(new BotwinOptions(
        async (ctx) =&amp;gt; { await ctx.Response.WriteAsync("GlobalBefore"); return true; }, 
        async (ctx) =&amp;gt; await ctx.Response.WriteAsync("GlobalAfter")
    ));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we setup that for each route it will write to the response body on the before and after hook.  Notice that the before hook has a boolean to signify if the routing should continue.  You may not want to continue the route execution for some reason after inspecting it in the Global before hook so can return false.&lt;/p&gt;

&lt;h3&gt;Module Before &amp;amp; After Hooks&lt;/h3&gt;

&lt;p&gt;Like the global before &amp;amp; after hooks these can be applied at a module level:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class TestModule : BotwinModule
{
    public TestModule()
    {
        this.Before = async (req, res, routeData) =&amp;gt; { await res.WriteAsync("Before"); return res; };

        this.After = async (req, res, routeData) =&amp;gt; { await res.WriteAsync("After"); };

        this.Get("/", async (request, response, routeData) =&amp;gt; { await response.WriteAsync("Hello"); });
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again fairly similar to the global hook but you can return the response object to continue execution or return null to stop the request in the before hook.&lt;/p&gt;

&lt;h3&gt;IStatusCodeHandler&lt;/h3&gt;

&lt;p&gt;An implementation of &lt;code&gt;IStatusCodeHandler&lt;/code&gt; means you can determine what happens if your route returns a certain status code. ASP.NET Core provides middleware called &lt;code&gt;UseStatusCodePages&lt;/code&gt; but it is not very elegant for users to use so I felt this was a cleaner option:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class ConflictStatusCodeHandler : IStatusCodeHandler
{
    public bool CanHandle(int statusCode)
    {
        return statusCode == 409;
    }

    public async Task Handle(HttpContext ctx)
    {
        await ctx.Response.WriteAsync("Can't we all just get along?");
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can obviously do whatever you want in the Handle method.&lt;/p&gt;

&lt;h3&gt;IResponseNegotiator&lt;/h3&gt;

&lt;p&gt;Mentioned previously, implementing this interface allows you to handle content negotiation if selected in the route:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class TestResponseNegotiator : IResponseNegotiator
{
    public bool CanHandle(IList&amp;lt;MediaTypeHeaderValue&amp;gt; accept)
    {
        return accept.Any(x =&amp;gt; x.MediaType.IndexOf("foo/bar", StringComparison.OrdinalIgnoreCase) &amp;gt;= 0);
    }

    public async Task Handle(HttpRequest req, HttpResponse res, object model)
    {
        await res.WriteAsync("FOOBAR");
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously here you can make your response return CSV, PDF etc etc.  If you call &lt;code&gt;response.Negotiate&lt;/code&gt; and Botwin can't find a relevant implementation it will default to JSON.&lt;/p&gt;

&lt;p&gt;If you explicitly want to return JSON from your route you can use another extension like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;this.Get("/actors", async (req, res, routeData) =&amp;gt;
{
    var people = actorProvider.Get();
    await res.AsJson(people);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Dependency Injection&lt;/h3&gt;

&lt;p&gt;You can inject dependencies into Botwin modules and these are resolved automatically via the ASP.NET Core built in DI so if you use Structuremap, Autofac etc that is plugged into ASP.NET Core then it will work fine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class ActorsModule : BotwinModule
{
    public ActorsModule(IActorProvider actorProvider)
    {
       //Do stuff
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;So what have we got here? This is not Nancy re-imagined on ASP.NET Core, this is me wondering whether I could easily and quickly use some of the lower level parts of the routing to use Nancy-esque style routing.  It runs on pre-released binaries from Microsoft so just a warning for now!  The one thing I have never liked about ASP.NET is the routing whether that be configured in Global.asax or attribute routing or convention based methods in controllers.  This is not a framework.  Things like authentication and error handling etc should be handled by other middleware that comes with ASP.NET Core but Botwin contains enough functionality to get a decent sized app running.  My commitment to Nancy is still as strong but these days finding time to contribute to it is difficult so kind of makes me sad that there is no choice for web frameworks for .NET.  The performance is very good as it sits directly within the ASP.NET Core pipeline.  If you'd like to help out or have some ideas please visit the repo &lt;a href="https://github.com/jchannon/Botwin"&gt;here&lt;/a&gt; but today I'm happy to announce Botwin!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2016/07/13/building-all-current-dotnet-core-projects-vscode/</guid><link>http://blog.jonathanchannon.com/2016/07/13/building-all-current-dotnet-core-projects-vscode/</link><a10:author><a10:name /></a10:author><category>ASP.Net</category><category>VSCode</category><title>Building all and current dotnet core projects in VSCode</title><description>&lt;p&gt;As you may or may not know I try to work on OSX as much as possible and with .Net that's quite painful to be honest.  Things are moving along nicely with Jetbrains Rider,
VSCode, Xamarin and Omnisharp.  I'll be honest, none of them are perfect and I often find myself using Visual Studio in a VM because it just works (yes, its clunky etc etc).
Recently, VSCode got a 1.3 release with some new features, tabs being one of them.  I never really got on with VSCode so dismissed it most of the time but this new release
opened my eyes a bit more and thought I'd give it a go.  Its C# support now runs on .Net Core RTM and most of my work at the moment is porting projects to .Net Core so it seemed
this would be worthwhile.  I've tried to setup keybindings that are the ones I know from Visual Studio and installed couple of extensions to make things easier and prettier.  &lt;/p&gt;

&lt;p&gt;As VSCode is language agnostic the one thing I found was how to build .Net Core projects was a bit off.  For each project you have you have to configure a task runner.  VSCode tries to 
help you here and gives you a few languages to choose from.  For .Net Core it creates a &lt;code&gt;dotnet build&lt;/code&gt; task.  The problem with this is that it runs that command from the workspace root, 
ie the folder where VSCode is opened.  What if you open it from the git root folder and your project(s) are under a src/MyProject folder?  It will fail as it cant find project.json.
What you can do is set the &lt;code&gt;cwd&lt;/code&gt; to be a specific directory by hardcoding it in the task configuration but thats not great if you have multiple projects.  You could use some predefined
variables that VSCode provides eg/&lt;code&gt;${fileDirname}&lt;/code&gt; but again if you are in a folder 4 levels deep that wont work either.
</description><pubDate>Tue, 12 Jul 2016 23:00:00 Z</pubDate><a10:updated>2016-07-12T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;As you may or may not know I try to work on OSX as much as possible and with .Net that's quite painful to be honest.  Things are moving along nicely with Jetbrains Rider,
VSCode, Xamarin and Omnisharp.  I'll be honest, none of them are perfect and I often find myself using Visual Studio in a VM because it just works (yes, its clunky etc etc).
Recently, VSCode got a 1.3 release with some new features, tabs being one of them.  I never really got on with VSCode so dismissed it most of the time but this new release
opened my eyes a bit more and thought I'd give it a go.  Its C# support now runs on .Net Core RTM and most of my work at the moment is porting projects to .Net Core so it seemed
this would be worthwhile.  I've tried to setup keybindings that are the ones I know from Visual Studio and installed couple of extensions to make things easier and prettier.  &lt;/p&gt;

&lt;p&gt;As VSCode is language agnostic the one thing I found was how to build .Net Core projects was a bit off.  For each project you have you have to configure a task runner.  VSCode tries to 
help you here and gives you a few languages to choose from.  For .Net Core it creates a &lt;code&gt;dotnet build&lt;/code&gt; task.  The problem with this is that it runs that command from the workspace root, 
ie the folder where VSCode is opened.  What if you open it from the git root folder and your project(s) are under a src/MyProject folder?  It will fail as it cant find project.json.
What you can do is set the &lt;code&gt;cwd&lt;/code&gt; to be a specific directory by hardcoding it in the task configuration but thats not great if you have multiple projects.  You could use some predefined
variables that VSCode provides eg/&lt;code&gt;${fileDirname}&lt;/code&gt; but again if you are in a folder 4 levels deep that wont work either.
&lt;!--excerpt--&gt;
I wanted a Build All projects command and a Build Current project command but with the above limitations I set about investigating some terminal commands that could be run to get this to work
and below is what I came up with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    // See https://go.microsoft.com/fwlink/?LinkId=733558
    // for the documentation about the tasks.json format
    "version": "0.1.0",
    "command": "zsh",
    "isShellCommand": true,
    "showOutput": "always",
    "args": [
        "-c"
    ],
    "options": {
        "cwd": "${fileDirname}"
    },
    "tasks": [{
        "taskName": "Build Current Project",
        "suppressTaskName": true,
        "isBuildCommand": true,
        "args": [
            "setopt extended_glob &amp;amp;&amp;amp; print -l (../)#project.json(:h) | xargs dotnet build"
        ],
        "problemMatcher": "$msCompile"
    }, {
        "taskName": "Build All Projects",
        "suppressTaskName": true,
        "isBuildCommand": true,
        "args": [
            "cd ${workspaceRoot} &amp;amp;&amp;amp; dotnet build ./**/**/project.json &amp;amp;&amp;amp; echo Build Completed"
        ],
        "problemMatcher": "$msCompile"
    }]
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;One thing to note, this will only work for OSX/Linux users with ZSH.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So what we have is a build task that runs a command (the first task) which calls out to &lt;code&gt;zsh&lt;/code&gt; with the argument &lt;code&gt;-c&lt;/code&gt; that shows the output in the task panel within VSCode and it executes it
within the current file's directory.  This then calls &lt;code&gt;setopt extended_glob&lt;/code&gt; to turn on ZSH extended globbing, it finds the closest parent directory that has a project.json and then passes
that to &lt;code&gt;xargs&lt;/code&gt; which will execute &lt;code&gt;dotnet build&lt;/code&gt; with the output from the glob.  &lt;/p&gt;

&lt;p&gt;We also have another task which will build all projects by changing directory to the workspace root and then running &lt;code&gt;dotnet build&lt;/code&gt; with a glob pattern to find all the folders with project.json
inside of them.  You will have to change that glob pattern to fit your folder structure but this is what works for the &lt;a href="http://nancyfx.org"&gt;NancyFX&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;To invoke these presee &lt;code&gt;CMD + P&lt;/code&gt; and type &lt;code&gt;task&lt;/code&gt; and add a space after task, VSCode will list your tasks, you can then execute either Build Current Project or Build All Projects.&lt;/p&gt;

&lt;p&gt;Have fun!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2016/06/27/porting-owin-middleware-aspnetcore/</guid><link>http://blog.jonathanchannon.com/2016/06/27/porting-owin-middleware-aspnetcore/</link><a10:author><a10:name /></a10:author><category>ASP.Net</category><category>OSS</category><category>OWIN</category><title>Porting OWIN middleware to ASP.Net Core</title><description>&lt;p&gt;In our application at work we make use of various middleware and as we are making everything run on .Net Core the time has come to port said middleware to .Net Core.  If you don't already know ASP.Net Core has a bridge that allows you to use OWIN components in an ASP.Net Core application.  This will convert the HttpContext into a OWIN environment dictionary on input and then back again on output.&lt;/p&gt;

&lt;p&gt;Lets take an example of some middleware&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MyMiddleware
{
    private readonly Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc;
    private readonly OwinUserMiddlewareOptions options;

    public OwinUserMiddleware(Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc, MyMiddlewareOptions options)
    {
        this.options = options;
        this.nextFunc = nextFunc;
    }

    public Task Invoke(IDictionary&amp;lt;string, object&amp;gt; environment)
    {
        //Everything is awesome
        return nextFunc(environment);
    }
}

public static class MyMiddlewareExtensions
{
    public static IAppBuilder UseMyMiddleware(this IAppBuilder app, MyMiddlewareOptions options = null)
    {
        return app.Use(typeof(MyMiddleware), options);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;</description><pubDate>Sun, 26 Jun 2016 23:00:00 Z</pubDate><a10:updated>2016-06-26T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;In our application at work we make use of various middleware and as we are making everything run on .Net Core the time has come to port said middleware to .Net Core.  If you don't already know ASP.Net Core has a bridge that allows you to use OWIN components in an ASP.Net Core application.  This will convert the HttpContext into a OWIN environment dictionary on input and then back again on output.&lt;/p&gt;

&lt;p&gt;Lets take an example of some middleware&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MyMiddleware
{
    private readonly Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc;
    private readonly OwinUserMiddlewareOptions options;

    public OwinUserMiddleware(Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc, MyMiddlewareOptions options)
    {
        this.options = options;
        this.nextFunc = nextFunc;
    }

    public Task Invoke(IDictionary&amp;lt;string, object&amp;gt; environment)
    {
        //Everything is awesome
        return nextFunc(environment);
    }
}

public static class MyMiddlewareExtensions
{
    public static IAppBuilder UseMyMiddleware(this IAppBuilder app, MyMiddlewareOptions options = null)
    {
        return app.Use(typeof(MyMiddleware), options);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;!--excerpt--&gt;
Here we see some middleware and an extension so it can be used in an application that uses OWIN.  This would be called in most commonly in a &lt;code&gt;Startup.cs&lt;/code&gt; file like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void Configuration(IAppBuilder app)
{
    app.UseMyMiddleware(new MyMiddlewareOptions());  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As I said earlier, ASP.Net Core has a bridge to use OWIN components and as long as your middleware can return a &lt;code&gt;MidFunc&lt;/code&gt; there is very little required for you to do however if you take the example above there is a tiny bit more to do.&lt;/p&gt;

&lt;h2&gt;ASP.Net Core Startup.cs&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;public void Configure(IApplicationBuilder app)
{
    //Use the ASP.Net Core OWIN bridge
    app.UseOwin(x =&amp;gt; x.Invoke(MyMiddleware.ReturnAppFunc()));  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above shows how to use the OWIN bridge if your middleware can already return a &lt;code&gt;MidFunc&lt;/code&gt;.  For those unclear here's what that looks like &lt;code&gt;System.Func&amp;lt;System.Func&amp;lt;System.Collections.Generic.IDictionary&amp;lt;string, object&amp;gt;, System.Threading.Tasks.Task&amp;gt;, System.Func&amp;lt;System.Collections.Generic.IDictionary&amp;lt;string, object&amp;gt;, System.Threading.Tasks.Task&amp;gt;&amp;gt;;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Using our sample middleware above the extension class would now have to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using System;

using MidFunc = System.Func&amp;lt;System.Func&amp;lt;System.Collections.Generic.IDictionary&amp;lt;string, object&amp;gt;,
        System.Threading.Tasks.Task&amp;gt;, System.Func&amp;lt;System.Collections.Generic.IDictionary&amp;lt;string, object&amp;gt;,
        System.Threading.Tasks.Task&amp;gt;&amp;gt;;

public static class MyMiddlewareExtensions
{
    public static Action&amp;lt;MidFunc&amp;gt; UseMyMiddleware(this Action&amp;lt;MidFunc&amp;gt; builder, MyMiddlewareOptions options = null)
    {
        builder(next =&amp;gt; new MyMiddleware(next, options).Invoke);
        return builder;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can now be used in a ASP.NET Core Startup class like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void Configure(IApplicationBuilder app)
{
  //Use the ASP.Net Core OWIN bridge
  app.UseOwin(x =&amp;gt; 
  {
      x.UseMyMiddleware(new MyMiddlewareOptions());
      x.UseNancy();
  });  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tada!  Also note that I only call &lt;code&gt;UseOwin&lt;/code&gt; once, I don't need to call it for every OWIN middleware I have.  Also just to be clear, if you want your middleware to run on .Net Core you will still have to make sure your middleware is compatible.  The above shows  how you came make your OWIN middleware run in ASP.Net Core pipeline even if its on .Net 4.5 but if it is compatible with .Net Core you can target that from your application and bingo! That is exactly what I have done with my middleware. &lt;/p&gt;

&lt;p&gt;Happy coding!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2016/04/28/what-is-a-hypermedia-client/</guid><link>http://blog.jonathanchannon.com/2016/04/28/what-is-a-hypermedia-client/</link><a10:author><a10:name /></a10:author><category>hypermedia</category><category>REST</category><title>What is a Hypermedia client?</title><description>&lt;p&gt;I've been interested in Hypermedia for quite a while.  I bugged &lt;a href="http://twitter.com/darrelmiller"&gt;Darrel Miller&lt;/a&gt; and &lt;a href="http://twitter.com/gblock"&gt;Glenn Block&lt;/a&gt; (Glenn Miller) so much so they created a &lt;a href="https://www.youtube.com/playlist?list=PLbc9sDUxHqX60XJaTnNnKvI2mRighInDW"&gt;YouTube show&lt;/a&gt; called "In The Mood for HTTP".  I bought their book &lt;a href="http://webapibook.net/"&gt;"Designing Evolvable Web APIs with ASP.NET"&lt;/a&gt;, I am waiting for &lt;a href="http://shop.oreilly.com/product/0636920037958.do"&gt;"RESTful Web Clients Enabling Reuse Through Hypermedia"&lt;/a&gt; by &lt;a href="http://twitter.com/mamund"&gt;Mike Amundsen&lt;/a&gt;, I have &lt;a href="http://blog.jonathanchannon.com/2015/08/07/hypermedia-and-nancyfx/index.html"&gt;written&lt;/a&gt; about how to return different media types with NancyFX and I am looking at going to &lt;a href="http://2016.uk.restfest.org/"&gt;restfest.org&lt;/a&gt; in Edinburgh this year, a REST conference.  &lt;/p&gt;

&lt;p&gt;The one thing that I have always discussed with Glenn Miller is that there seems, or from my perception, that there is a lot of emphasis on the server returning media types(HAL,Siren,JSON-LD, Collection+Json) and very little information about hypermedia clients.  The information that I have come across which is very little, again coulkd be due to my lack of Google-fu, seems to generate a mis-conception.  The mis-conception I have come across is that if you have an API that returns hypermedia then your client should be able to magically work with it.  It should know everything that is required to browse the API and discover its way around.  I never quite grasped how that was supposed to happen and was serioulsy confused.  I had seen a video that showed when the server returned its responses, using Javascript it would loop over all the properties in the payload and then display them in a HTML page.  The emphasis was that if new bits of data were added then they would appear magically in the UI.  That seemed like a nice feature but I still didn't quite get how it went from hitting the root of the API to finding its way into the guts of it.  The server would return links in the payload with "rels" and I was baffled how this magic client knew what to do with a rel or even how it knew what rels it would return.&lt;br /&gt;
</description><pubDate>Wed, 27 Apr 2016 23:00:00 Z</pubDate><a10:updated>2016-04-27T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I've been interested in Hypermedia for quite a while.  I bugged &lt;a href="http://twitter.com/darrelmiller"&gt;Darrel Miller&lt;/a&gt; and &lt;a href="http://twitter.com/gblock"&gt;Glenn Block&lt;/a&gt; (Glenn Miller) so much so they created a &lt;a href="https://www.youtube.com/playlist?list=PLbc9sDUxHqX60XJaTnNnKvI2mRighInDW"&gt;YouTube show&lt;/a&gt; called "In The Mood for HTTP".  I bought their book &lt;a href="http://webapibook.net/"&gt;"Designing Evolvable Web APIs with ASP.NET"&lt;/a&gt;, I am waiting for &lt;a href="http://shop.oreilly.com/product/0636920037958.do"&gt;"RESTful Web Clients Enabling Reuse Through Hypermedia"&lt;/a&gt; by &lt;a href="http://twitter.com/mamund"&gt;Mike Amundsen&lt;/a&gt;, I have &lt;a href="http://blog.jonathanchannon.com/2015/08/07/hypermedia-and-nancyfx/index.html"&gt;written&lt;/a&gt; about how to return different media types with NancyFX and I am looking at going to &lt;a href="http://2016.uk.restfest.org/"&gt;restfest.org&lt;/a&gt; in Edinburgh this year, a REST conference.  &lt;/p&gt;

&lt;p&gt;The one thing that I have always discussed with Glenn Miller is that there seems, or from my perception, that there is a lot of emphasis on the server returning media types(HAL,Siren,JSON-LD, Collection+Json) and very little information about hypermedia clients.  The information that I have come across which is very little, again coulkd be due to my lack of Google-fu, seems to generate a mis-conception.  The mis-conception I have come across is that if you have an API that returns hypermedia then your client should be able to magically work with it.  It should know everything that is required to browse the API and discover its way around.  I never quite grasped how that was supposed to happen and was serioulsy confused.  I had seen a video that showed when the server returned its responses, using Javascript it would loop over all the properties in the payload and then display them in a HTML page.  The emphasis was that if new bits of data were added then they would appear magically in the UI.  That seemed like a nice feature but I still didn't quite get how it went from hitting the root of the API to finding its way into the guts of it.  The server would return links in the payload with "rels" and I was baffled how this magic client knew what to do with a rel or even how it knew what rels it would return.&lt;br /&gt;
&lt;!--excerpt--&gt;
After speaking to Darrel he told me that's the one thing clients do know ie/ what rels an API should return. See &lt;a href="https://twitter.com/jchannon/status/719486875484991488"&gt;here&lt;/a&gt;. I was still confused at this, I assumed that the client would have an in memory set of rels that it knew about and therefore understood them but then I was confused what would happen if a new rel was introduced by the API, how would the client know what to do?  &lt;/p&gt;

&lt;p&gt;I had it in my head and maybe from some of the hypermedia client articles and videos that I'd read/seen that a hypermedia client was some kind of magic client that just knew how to navigate an API.  I then came across &lt;a href="https://jeffknupp.com/blog/2014/06/03/why-i-hate-hateoas/"&gt;this&lt;/a&gt; article written by someone who also had the notion that a client is some magical thing and he states : &lt;code&gt;a single client that could make use of *every single (properly built) REST API in existence* without requiring documentation&lt;/code&gt; At this point I kind of agreed with him, where are the magical clients and libraries that I can just plug into my API?  They must exist as people keep going on about how if you have a client it should know how to work with your API.  I then came across &lt;a href="https://signalvnoise.com/posts/3373-getting-hyper-about-hypermedia-apis#comments"&gt;this&lt;/a&gt; article, it also poo-poos the idea of hypermedia and magic clients but then I started to read the comments and saw comments from &lt;a href="http://twitter.com/gblock"&gt;Glenn Block&lt;/a&gt;,  &lt;a href="http://twitter.com/mamund"&gt;Mike Amundsen&lt;/a&gt; and &lt;a href="https://twitter.com/mikekelly85"&gt;Mike Kelly&lt;/a&gt;, the heavy hitters of the API world and it clicked from one of Glenn's replies.  &lt;/p&gt;

&lt;p&gt;There is no magic client. Its that simple.  Yes they could potentially loop over a resource in a response and display data that could in time be added to by the server but the way it navigates the API by pre-defined rels is because the developer who is using the client has documentation about the rels and the payload. Glenn's comment &lt;code&gt;"Hypermedia api’s don’t prevent documentation, that is a central part. The documentation centers around the link rels and payload, not the uri structure."&lt;/code&gt;  This also confirms what Darrel said, I just didn't get it at the time.  &lt;/p&gt;

&lt;p&gt;Clients know about rels, or to be more precise, the developer writing the program that uses a client library to navigate an API has documentation about the rels the API uses and the payloads it returns.  So you explicitly put in your code "execute a request to the URL returned in the link that has the rel X", there is no discovery in that sense, it doesn't magically find its way around as such.  It doesn't hardcode URLs which is one thing where hypermedia clients aim to succeed ie/no hard coding of URLs to follow and new features and/or rels will need developer interaction to change how their client uses these new features.  Glenn sums it up quite well in his quote (I tried to link to the comment but that's not available):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Clients have knowledge of what to expect. In the real world the average hypermedia client knows about certain types of links that may come back. That is all the rel is, and identifier that the client can use to match up against links it cares about. If new links come back that it doesn’t know about, well it will ignore those links. Although the client knows about a set of rels, it does not know if they will or will not be present. The advantage is that logic sits with the server where it can, and often does at some point change. The change might be due to several reasons, including scale as the server can tell the client go get that resource over here rather than where you got it last time. The client also doesn’t know or care about the URL, all it knows is if the rel is this, and that is a resource I want to access, follow that URL.&lt;/p&gt;
  
  &lt;p&gt;As to the new links that were returned which that client ignored, newer clients can come along and they are coded to understand those rels. They know what to do with it so they follow it.&lt;/p&gt;
  
  &lt;p&gt;In both cases I mentioned there’s no magic and there’s still hard coding of some logic. It’s just the type of logic is different than it is today. Instead of hardcoding uris, and logic of whether or not those resources can be called, the logic is looking for the presence of rels, and decided which one to access. And often the decision of what to do is still decided by a human being, or it maybe be a combination where the machine first looks at the available set and if it’s logic allows it to proceed it does, otherwise it gets interaction from a human.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So there it is, hypermedia clients are not some magic tool/library that can see your API and go "I got this", the developer has to deal with new features however, the key is that if the server modifies existing urls in a rel the client will not have to change because it hasn't hardcoded anything inside it to go to a certain URL.  New features in an API should in theory not break existing clients.&lt;/p&gt;

&lt;p&gt;Bringing this back to all the different media types that the server can return and all the discussion I see around them.  In simple terms the client will always look for a rel, that is common across all media types.  The difference comes where the client looks for them in the response payload.  Each media type has its own format and therefore where the rels exist.  One thing to clarify, the different media types all contain rels for link objects but media types like Siren and Collection+Json have objects in the payload that describe how to add/modify data that may not have a rel but will have a href to show where to send the request to add/modify data.&lt;/p&gt;

&lt;p&gt;My next plan is to write a simple API and a simple client using libraries that know how to deal with the media type I use and to navigate it to see how I get on.  I've already been told that dynamic languages are more suited to clients than statically typed languages due to static languages requiring a binding of a payload to a resource/model class.  With dynamic languages you dont need to map a payload to a class, you can just use a property from the payload directly without the need to bind to a class of 10 properties and then only use 2 (although in theory you wouldn't need to create the 8 properties if you weren't going to use them).&lt;/p&gt;

&lt;p&gt;Anyway I hope that has highlighted and answered anyone's question of what a hypermedia client is.  If you read this and thought "jeez you're a dumbass, you still don't get it" feel free to let me know in the comments although if I still haven't grasped it by this point, I might cry a little inside. Thanks for reading. &lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2016/03/30/vq-communications-funds-coreclr-nancyfx/</guid><link>http://blog.jonathanchannon.com/2016/03/30/vq-communications-funds-coreclr-nancyfx/</link><a10:author><a10:name /></a10:author><category>.net</category><category>community</category><category>coreclr</category><category>nancyfx</category><category>oss</category><title>VQ Communications Funds NancyFX to run on CoreCLR</title><description>&lt;p&gt;Nearly 2 years ago I was employed by &lt;a href="http://www.vqcomms.com"&gt;VQ Communications&lt;/a&gt; primarily because of my open source contributions to &lt;a href="http://nancyfx.org"&gt;NancyFX&lt;/a&gt;.  They had started work on a v2 of their flagship product and had begun work with Nancy and needed someone to help drive a HTTP API and architect a scaling solution as their v2 product was addressing a requirement they had for it cope with large volumes of traffic.  Also of interest to me was their aim to deliver all of this as a black box appliance to customers on a VM running a custom embedded version of Linux using Postgres as the database.  I would work four days a week remotely and go into the office one day a week.  They already had completely remote employees and since I have been there they have taken on more. There are lots more juicy technical examples in the stack I could go into however, this is not the point of this post.&lt;/p&gt;

</description><pubDate>Tue, 29 Mar 2016 23:00:00 Z</pubDate><a10:updated>2016-03-29T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Nearly 2 years ago I was employed by &lt;a href="http://www.vqcomms.com"&gt;VQ Communications&lt;/a&gt; primarily because of my open source contributions to &lt;a href="http://nancyfx.org"&gt;NancyFX&lt;/a&gt;.  They had started work on a v2 of their flagship product and had begun work with Nancy and needed someone to help drive a HTTP API and architect a scaling solution as their v2 product was addressing a requirement they had for it cope with large volumes of traffic.  Also of interest to me was their aim to deliver all of this as a black box appliance to customers on a VM running a custom embedded version of Linux using Postgres as the database.  I would work four days a week remotely and go into the office one day a week.  They already had completely remote employees and since I have been there they have taken on more. There are lots more juicy technical examples in the stack I could go into however, this is not the point of this post.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Two years on and our API has developed well, our v2 version goes from strength to strength thanks to a great team and we are still using Nancy however, we got to a point at the end of 2015 where we were seeing occasional seg faults when running under &lt;a href="http://www.mono-project.com/"&gt;Mono&lt;/a&gt;.  We spent a lot of time delving deep into the issues, finding different behaviour under different Linux kernel versions and even submitted a &lt;code&gt;keep-alive&lt;/code&gt; HTTP PR to Mono to try and fix it.  &lt;/p&gt;

&lt;p&gt;Around the same time I was involved with &lt;a href="http://ominsharp.net"&gt;OmniSharp&lt;/a&gt; and trying to make developing .Net applications in Sublime Text a viable thing on OSX.  It worked well up to a point.  Whilst this was going on Microsoft announced that .Net was going open source and that they were working on CoreFX a set of base class libraries that would work cross platform.  This was very impressive.  Once could also interpret this as fate.  &lt;/p&gt;

&lt;p&gt;A few weeks after the MS announcement, &lt;a href="http://twitter.com/thecodejunkie"&gt;@thecodejunkie&lt;/a&gt; released a &lt;a href="http://thecodejunkie.com/2015/11/27/support-the-development-of-nancy-financially/"&gt;blog post&lt;/a&gt; asking the community to help fund the development of Nancy. Him, myself and the other folks that help maintain and drive Nancy forward had always done this in our own time but we were now asking for some donations to keep the project going.  It was felt that if companies are using and shipping applications built on top of Nancy it would be nice if they could contribute pull requests and/or donate some money to the project.  (We have over 200 contributors to Nancy and we get some great PRs from the community but I assume the majority are from people using Nancy not on company time.)  Whilst ASP.Net has the backing of a multi billion dollar company, Nancy doesn't so to keep it going, to give the maintainers a drive and in the big picture keep the project open source to aid .Net developers a choice when developing web based applications it would be great if users could see the benefits of donating a few dollars here and there.  Four months on from the blog post, Nancy has received $50.  When the blog post came out, I sent it to my boss as a fire and forget email, what happened next I hope will be an example for other businesses using open source software.&lt;/p&gt;

&lt;p&gt;On my next trip into the office, myself and my boss were having a chat about a few things and the topic of donating some funds to Nancy came up.  We discussed a couple of options and over the next two weeks or so we had a couple more discussions but by the end of it we had decided that what a good thing to do would be to pay to get Nancy running on CoreCLR so we could use it on our Linux system.  One thing to note is that I knew &lt;a href="http://twitter.com/thecodejunkie"&gt;@thecodejunkie&lt;/a&gt; was in a position where the company he worked for, &lt;a href="http://tretton37.com"&gt;tretton37&lt;/a&gt; would accept contract work for him to work on Nancy (see his blog post &lt;a href="http://thecodejunkie.com/2015/08/28/i-am-now-taking-contract-work-for-nancy/"&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;VQ felt that by funding this, it would address issues we had on Mono and therefore give value to the company whilst also allowing them to give back to the community.&lt;/p&gt;

&lt;p&gt;We at VQ have other code, not just the API so we embarked on making our other code CoreCLR compatible and over time record the advantages/disadvantages of it running on this platform.  This is still on-going but initial feedback showed all good things so there was no turning back.&lt;/p&gt;

&lt;p&gt;We at Nancy, had a call to discuss how our plans could be met with this funding.  We already had a big list of things we wanted to get into a v2 of Nancy (current version 1.x) so what could we do about them if we were to accept funding to get Nancy on CoreCLR.  One thing we were adamant about was that even with funding, we were not prepared to introduce something that did not align with the vision of how we saw Nancy running.  Luckily this was never going to be a problem as VQ simply wanted it run as it did on Mono &amp;amp; .Net.  We decided we could trim our list of features for a v2 but we would still need to get some things done for v2 which would enable CorecLR development.  The CoreCLR work was not a VQ, me and &lt;a href="http://twitter.com/thecodejunkie"&gt;@thecodejunkie&lt;/a&gt; thing only, nothing in Nancy's contribution workflow would change, it was simply that there was people working on Nancy full time.  &lt;/p&gt;

&lt;p&gt;VQ agreed that it would fund the development of Nancy on CorecLR from February 1st 2016 initially until April 30th 2016 but if another month was required so be it.  We are now two months in, Nancy runs on CoreCLR on RC1 packages.  RC2 will introduce some major changes so the plan is we get onto that as soon as its released.  Nancy's unit tests and Fluent Validation library is still not compatible with CoreCLR as they are dependent on upstream sources that are still in the progress of transitioning their code to CoreCLR.  Nicely, some work has been done by Microsoft in this regard.  Nancy reached out to Microsoft informing them of VQ's plan to move their code and fund Nancy onto CoreCLR and they have helped with discussions surrounding &lt;a href="https://github.com/castleproject/Core/issues/90"&gt;Castle.Core&lt;/a&gt; and code contributions to repositories such as &lt;a href="https://github.com/FakeItEasy/FakeItEasy/pull/617"&gt;FakeItEasy&lt;/a&gt; so this has been welcomed greatly.&lt;/p&gt;

&lt;p&gt;Whilst we wait for RC2, &lt;a href="http://twitter.com/thecodejunkie"&gt;@thecodejunkie&lt;/a&gt; is working on performance improvements to Nancy and as a team Nancy is looking for feedback for a &lt;a href="https://www.nuget.org/packages/Nancy/2.0.0-alpha"&gt;v2-alpha release&lt;/a&gt; that has been made possible thanks to VQ.&lt;/p&gt;

&lt;p&gt;From an open source contributor's perspective it's great to see a company out there with the vision of funding a project to enable it to move forward and address needs they have as a company.  This benefits the open source project as well as the company.&lt;/p&gt;

&lt;p&gt;From the perspective of an employee there was no day long meetings discussing implications due to licences, what it would mean regarding support issues, no legal department involvement.  It was simple, the company used an open source project, the company had a vision for their product that the open source project did not meet yet so rather than waiting or look to use another project why not fund that development to meet the requirements?&lt;/p&gt;

&lt;p&gt;It can be that simple.  Does your company hit a bug with an open source project? Allow your developers to spend time and submit a PR to fix the issue.  Does your company have a vision that an open source project could help with? Fund that project to make it a reality.&lt;/p&gt;

&lt;p&gt;Whilst each open source project is unique, how you enable full time work on it and how companies execute the funding for it will all be different, in essence what I believe VQ has managed to do is set a precedent to say it is possible for companies to fund open source projects.  This doesn't mean companies funding projects kill people's passion in contributing to open source projects, it means passionate developers can contribute as they do now but if there are companies out there able to contribute by funding or having developers contribute whilst in working hours this can only be a good thing for the project.  It's not just a discussion for a couple of developers in the company to have over the water cooler which never materialises into anything and it doesn't mean months on end checking on potential legal/licencing/support issues, its something any company can do if they really want to.  Just to be clear however, in case you're screaming "what a cavalier attitude", VQ were not cavalier in their approach to this, what we did was have a balanced discussion, things like licence issues pretty much went away at the start as Nancy is MIT so we were happy to donate any work paid for by us to the Nancy team, there were no legal issues and any other issues that were raised were either discussed and resolved or we checked with &lt;a href="http://twitter.com/thecodejunkie"&gt;@thecodejunkie&lt;/a&gt; for clarification.  I'm just highlighting that each circumstance is different but luckily for VQ and Nancy it all came together nicely and I believe there can be many companies and open source projects that can have this relationship.&lt;/p&gt;

&lt;p&gt;Checkout the &lt;a href="http://www.vqcomms.com/blog/2016-03-30/nancyfx-coreclr/"&gt;blog post&lt;/a&gt; from VQ about how the process above came about and what it means to them and the &lt;a href="https://tretton37.com/blog/tretton37_open_source"&gt;blog post&lt;/a&gt; from tretton37 about the relationship has evolved between VQ and NancyFX.&lt;/p&gt;

&lt;p&gt;If your company has a problem, if no one else can help, and if you can find them, maybe you can fund an open source project.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2016/02/11/profiling-coreclr-application/</guid><link>http://blog.jonathanchannon.com/2016/02/11/profiling-coreclr-application/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>coreclr</category><category>oss</category><title>Profiling a CoreCLR application with dotMemory</title><description>&lt;p&gt;I had ported an application over to CoreCLR (that's a whole other blog post), along with my colleague &lt;a href="http://twitter.com/yantrio"&gt;James Humphries&lt;/a&gt; put it in a docker image and sat back and watched it do its thing.  After 6 hours of running the docker container had crashed.  Ah nuts we thought,  so pulled up the logs from docker and the last line looked like this &lt;code&gt;2016-02-10T20:18:31.728783069Z Killed&lt;/code&gt;.  I'm pretty sure when you have a log entry with &lt;code&gt;Killed&lt;/code&gt; in it, things can't be good. To the interweb...&lt;/p&gt;

&lt;p&gt;I opened up the CoreFX repository on Github to search for the term &lt;code&gt;Killed&lt;/code&gt; and there were 2 comments but nothing that was logged out anywhere.  I then Googled for docker and killed and there was an entry that someone else had spotted on their container and the feedback was essentially it was probably out of memory.&lt;/p&gt;

</description><pubDate>Thu, 11 Feb 2016 00:00:00 Z</pubDate><a10:updated>2016-02-11T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I had ported an application over to CoreCLR (that's a whole other blog post), along with my colleague &lt;a href="http://twitter.com/yantrio"&gt;James Humphries&lt;/a&gt; put it in a docker image and sat back and watched it do its thing.  After 6 hours of running the docker container had crashed.  Ah nuts we thought,  so pulled up the logs from docker and the last line looked like this &lt;code&gt;2016-02-10T20:18:31.728783069Z Killed&lt;/code&gt;.  I'm pretty sure when you have a log entry with &lt;code&gt;Killed&lt;/code&gt; in it, things can't be good. To the interweb...&lt;/p&gt;

&lt;p&gt;I opened up the CoreFX repository on Github to search for the term &lt;code&gt;Killed&lt;/code&gt; and there were 2 comments but nothing that was logged out anywhere.  I then Googled for docker and killed and there was an entry that someone else had spotted on their container and the feedback was essentially it was probably out of memory.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;I had spotted when I was debugging the appliation that memory usage was creeping up relatively slowly but assumed all would be ok when garbage collection kicked in.  Well it seems its true what they say about assumptions as running it for 6 hours caused the container to be killed.  I had mentioned to &lt;a href="http://twitter.com/citizenmatt"&gt;Matt Ellis&lt;/a&gt;, he of Jetbrains fame, a few months ago that having profiling tools such as dotMemory &amp;amp; dotTrace would be awesome for apps running on Linux.  Last I heard there may be plans to have a remote agent that could run on Linux and this would send the info back to dotMemory which would run on Windows as its all in WPF I believe.  Anyhow he pointed me to a blog post JB had done.  This illustrated dotMemory profiling a &lt;code&gt;*.exe&lt;/code&gt; that had been built for CoreCLR but I wanted to do it from Visual Studio and not have to produce a binary.  Remeber CoreCLR apps don't produce binaries unless you explicitly tell dnx/dotnet cli to do so.  Anyhow long story short, we couldn't get VS to launch the startup project and monitor my app.  I assume they will solve this issue in the long run.&lt;/p&gt;

&lt;p&gt;To get it working, we opened dotMemory and told it to profile a standalone application.  That application was dnx.exe.   Depending on your runtime target on Windows this executable will either be in &lt;code&gt;C:\Program Files\DNX\runtimes&lt;/code&gt; or &lt;code&gt;C:\Users\[USERNAME]\.dnx\runtimes&lt;/code&gt;.  The next thing to do is click the Advanced option.  Here you can enter any arguments which in this case it will be the command you use in &lt;code&gt;project.json&lt;/code&gt; to start your application.  The final option you have is the working directory.  For me this was where the source code was.  So you should have something looking like this:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/dotmemoryrun.png" alt="dotMemory Usage" /&gt;&lt;/p&gt;

&lt;p&gt;Once you click Run it will start you application up and begin doing its thing.  What we did was to create a snapshot as soon as the app seemed to have started up, wait 60secs, take another snapshot, wait 120 seconds and repeat.  You can then compare the snapshots by selecting the snapshots checkbox and then clicking compare.  You can then see the differences.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/dotmemorysnapshotselect.png" alt="dotMemory Snapshots" /&gt;
&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/dotmemorysnapshot.png" alt="dotMemory Differences" /&gt;
&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/dotmemorydiff.png" alt="dotMemory Drilldown" /&gt;&lt;/p&gt;

&lt;p&gt;On the comparison I was a bit confused with all this information and fumbled around for a bit but we came across the Back Traces option and then picked the object with the largest amount of allocated bytes and drilled down until we saw something that might have been in my code.  As we drilled down and saw system classes we knew roughly where in the app it would be until we finally hit the class and method name.&lt;/p&gt;

&lt;p&gt;Now being a guy on the cutting edge I knew the issue I had was this new piece of technology called XML, you should try it.  We looked in the code and found it was doing this, which was getting called every second.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var stream = new MemoryStream(Encoding.UTF8.GetBytes(myXml)));
var sr = new StreamReader(stream));
var serializer = new XmlSerializer(typeof(MyObject), new XmlRootAttribute("rootNode"));
var obj = (MyObject)xml.Deserialize(stream);                             
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So did you spot it? Clue : Its not the fact that the memorystream or streamreader isn't within a &lt;code&gt;using&lt;/code&gt; statement (although that was fixed).&lt;/p&gt;

&lt;p&gt;From the dotMemory data it said it was the &lt;code&gt;XmlSerializer&lt;/code&gt; constructor.  I had a look at that object and there was no &lt;code&gt;Dispose&lt;/code&gt; method so decided I would create a static instance of it at the top of the class to save recreating it every time.&lt;/p&gt;

&lt;p&gt;I made the changes, put them into a new docker image and watched the memory usage of my app with &lt;a href="https://t.co/gR1uTzKCwq"&gt;cAdvisor&lt;/a&gt; and bingo, rock solid and stable memory usage!&lt;/p&gt;

&lt;p&gt;After a quick Google just out of curiosity, I found the same exact issue on Stackoverflow and luckily the answer was what we had decided to do with the static instance.  Basically &lt;code&gt;XmlSerializer&lt;/code&gt; uses assembly generation and they cannot be collected hence the memory issues we saw.&lt;/p&gt;

&lt;p&gt;Anyway hope that helps you on the path to profiling your CoreCLR app and hopefully things with Jetbrains will improve so we can profile apps running on Linux.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2015/11/16/content-negotiation-golang/</guid><link>http://blog.jonathanchannon.com/2015/11/16/content-negotiation-golang/</link><a10:author><a10:name /></a10:author><category>golang</category><category>nancyfx</category><category>oss</category><title>Introducing Negotiator  - a GoLang content negotiation library</title><description>&lt;p&gt;In my continued experience learning GoLang I started looking at how to best use it when dealing with HTTP.  The idiomatic way to use GoLang and HTTP is to use the standard library which keeps things minimal but there are a few features missing.  The first thing is a router.  OOTB GoLang doesn't have a router and the majority seem to suggest using a package called Mux from Gorilla Toolkit, a set of libraries that aims to improve the standard library from Go.  After having a play with it I didn't really warm to it so spent some time looking into the alternatives (and there are plenty!) and eventually decided upon &lt;a href="https://goji.io"&gt;Goji&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once I had started using Goji I then wanted to handle content negotiation in my HTTP handler.  As I said earlier GoLang is minimal in its offerings OOTB and this is a good thing.  Just for the record there are a few frameworks out there if you want/need and all encompassing framework such as Martini, Revel and Echo.  These tend to bend the idioms  of GoLang a bit and even the author of Martini blogged on this fact due to feedback from the community that although its capabilities are great they aren't idiomatic Go.
</description><pubDate>Mon, 16 Nov 2015 00:00:00 Z</pubDate><a10:updated>2015-11-16T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;In my continued experience learning GoLang I started looking at how to best use it when dealing with HTTP.  The idiomatic way to use GoLang and HTTP is to use the standard library which keeps things minimal but there are a few features missing.  The first thing is a router.  OOTB GoLang doesn't have a router and the majority seem to suggest using a package called Mux from Gorilla Toolkit, a set of libraries that aims to improve the standard library from Go.  After having a play with it I didn't really warm to it so spent some time looking into the alternatives (and there are plenty!) and eventually decided upon &lt;a href="https://goji.io"&gt;Goji&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once I had started using Goji I then wanted to handle content negotiation in my HTTP handler.  As I said earlier GoLang is minimal in its offerings OOTB and this is a good thing.  Just for the record there are a few frameworks out there if you want/need and all encompassing framework such as Martini, Revel and Echo.  These tend to bend the idioms  of GoLang a bit and even the author of Martini blogged on this fact due to feedback from the community that although its capabilities are great they aren't idiomatic Go.
&lt;!--excerpt--&gt;&lt;/p&gt;

&lt;h3&gt;Introducing Negotiator&lt;/h3&gt;

&lt;p&gt;After realising that Goji didn't have content negotiation seeing as its just a router (although there are Goji compatible middleware, which in turn are standard library compatible) I started playing on how to implement conneg.&lt;/p&gt;

&lt;p&gt;My first attempt was a piece of middleware that allowed the request to go to Goji and then on the way back up it would interrogate the context for a model which the HTTP handler would have inserted, it would then interrogate the Accept header obviously and then write out the JSON/XML to the response.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//***** HTTP Handler *****

func HelloWorldHTTPHandler(ctx web.C, w http.ResponseWriter, req *http.Request) {
    user := &amp;amp;User{"Joe","Bloggs"}

    ctx.Env["model"] = user
}

//*****First stab at content negotiation midleware *****

package conneg

import (
"encoding/json"
"encoding/xml"
"net/http"

"github.com/zenazn/goji/web"
)

func Conneg(c *web.C, h http.Handler) http.Handler {
    fn := func(w http.ResponseWriter, r *http.Request) {

        h.ServeHTTP(w, r)

        accept := r.Header.Get("Accept")
        if model := c.Env["model"]; model != nil {

            switch accept {
            case "application/json":

                w.Header().Set("Content-Type", "application/json")

                js, err := json.Marshal(model)

                if err != nil {
                    http.Error(w, err.Error(), http.StatusInternalServerError)
                    return
                }
                if statuscode := c.Env["statuscode"]; statuscode != nil {
                    w.WriteHeader(statuscode.(int))
                }
                w.Write(js)

            case "application/xml":
                x, err := xml.MarshalIndent(model, "", "  ")
                if err != nil {
                    http.Error(w, err.Error(), http.StatusInternalServerError)
                    return
                }

                w.Header().Set("Content-Type", "application/xml")
                w.Write(x)
            }
        }

    }
    return http.HandlerFunc(fn)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see its pretty rudimentary but does the job but if I developed multiple web applications I would have to copy and paste this into every app that I wrote.  I would also have to add to the switch statement for every media type I wanted to handle.&lt;/p&gt;

&lt;p&gt;I wanted to write a library that I could reference for every web application, separate out each response processor instead of having a switch statement, have the ability to write new response processors that conformed to an interface plus get more experience with Go and of course make it OSS.&lt;/p&gt;

&lt;p&gt;To cut a long story short if you install a reference to &lt;code&gt;github.com/jchannon/negotiator&lt;/code&gt; you can how have a HTTP handler like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func getUser(w http.ResponseWriter, req *http.Request) {
    user := &amp;amp;User{"Joe","Bloggs"}
    negotiator.Negotiate(w, req, user)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This in my humble opinion keeps things pretty tidy.  If you want to extend the base functionality of JSON/XML handling you can implement this interface for your own response processor:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type ResponseProcessor interface {
    CanProcess(mediaRange string) bool
    Process(w http.ResponseWriter, model interface{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CanProcess is called when &lt;code&gt;negotiator&lt;/code&gt; loops over the media types in the Accept header.  This loop is also ordered by the weighted value in the Accept header eg. &lt;code&gt;Accept: application/json,application/xml;q=0.8,text/plain;q=0.5&lt;/code&gt;, some great work by &lt;a href="https://twitter.com/pdoh00"&gt;Phil Cleveland&lt;/a&gt; who helped with writing &lt;code&gt;negotiator&lt;/code&gt; (note: if there is no accept header or relevant response processor then &lt;code&gt;negotiator&lt;/code&gt; will return a 406).  The response processor will return a boolean saying whether it can handle the current media type.  If it returns true then &lt;code&gt;Process&lt;/code&gt; is called and it will then handle writing the body to the response in the format that is applicable to that response processor.&lt;/p&gt;

&lt;p&gt;To add your new custom processor to &lt;code&gt;negotiator&lt;/code&gt; simple pass it to the &lt;code&gt;New&lt;/code&gt; method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func customHandler(w http.ResponseWriter, req *http.Request) {
    user := &amp;amp;user{"Joe", "Bloggs"}
    textplainNegotiator := negotiator.New(&amp;amp;PlainTextResponseProcessor{})
    textplainNegotiator.Negotiate(w, req, user)
}

type PlainTextResponseProcessor struct {
}

func (*PlainTextResponseProcessor) CanProcess(mediaRange string) bool {
    return strings.EqualFold(mediaRange, "text/plain")
}

func (*PlainTextResponseProcessor) Process(w http.ResponseWriter, model interface{}) {

    w.Header().Set("Content-Type", "text/plain")

    val := reflect.ValueOf(model).Elem()

    for i := 0; i &amp;lt; val.NumField(); i++ {
        valueField := val.Field(i).String()
        typeField := val.Type().Field(i)

        w.Write([]byte(typeField.Name + " : " + valueField + " "))
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a slightly contrived example but you can see what needs to be done to add your own response processor for it to be used by &lt;code&gt;negotiator&lt;/code&gt;.  One thing I don't like about this is that you need to call &lt;code&gt;New&lt;/code&gt; in every handler however, you may only want this processor in certain route handlers in your application.  Going back to my first example, what you could do is insert the pointer returned from calling &lt;code&gt;New&lt;/code&gt; and insert it into the http context and then in the handlers pull it out and then call &lt;code&gt;Negotiate&lt;/code&gt;.  You can see a demo of this in the Github repo &lt;a href="https://github.com/jchannon/negotiator/blob/master/demo/main.go"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hopefully other Go developers will find this library useful but I'm happy with what I've done here and its allowed me to learn more Go and interact with the community.  I'm also pretty chuffed that its got 100% unit test coverage according to the built in golang tools.  I'm  very impressed with the receptiveness of the Go community and the amount of libraries and blog posts out there to learn from.  I've managed to pick up Go pretty easily and I'm really loving it.  Another big thanks to &lt;a href="https://twitter.com/pdoh00"&gt;Phil Cleveland&lt;/a&gt;, another .Net developer trying to pick up Go, for his help with &lt;code&gt;negotiator&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For more information check out the &lt;a href="http://github.com/jchannon/negotiator"&gt;Github repository&lt;/a&gt; and the &lt;code&gt;godoc&lt;/code&gt; documentation &lt;a href="https://godoc.org/github.com/jchannon/negotiator"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Shout out to &lt;a href="http://nancyfx.org"&gt;NancyFX&lt;/a&gt;, my first love, for inspiring the design of the ResponseProcessor interface.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2015/10/27/introducting-pogo-golang-twitter-pocket/</guid><link>http://blog.jonathanchannon.com/2015/10/27/introducting-pogo-golang-twitter-pocket/</link><a10:author><a10:name /></a10:author><category>golang</category><category>oss</category><title>Introducing PoGo  - a GoLang, Twitter favourites to Pocket importer</title><description>&lt;p&gt;I've always kept myself up to date with the latest languages arriving on the scene and I've spent time in the past learning Node and last year I learnt Python by doing the Omnisharp plugin for Sublime.  I have recently been looking for a static language that I can transfer my C# skills too and I had narrowed it down to 3; Swift, Kotlin and GoLang.  I started out with Kotlin and setting up a dev environment with IntelliJ and running the koans that Jetbrains advise you step through to pick up the language.  Whilst it seemed relatively straightforward I got "noob confused" when I saw examples of Java calling into Kotlin with get/set prefixes somehow magically added to Kotlin properties. It turns out the Kotlin compiler does this for Java libraries so it can communicate with it, to me it seemed strange that I code a library in one language and the compiler then exposes these methods and properties slightly differently. Superficial as this sounds I also didn't really like the mammoth that appears to be IntelliJ.  Coming from a predominantly Visual Studio background but working with Omnisharp I wanted a lightweight editor with some refactoring, intellisense and error highlighting.&lt;/p&gt;

</description><pubDate>Tue, 27 Oct 2015 00:00:00 Z</pubDate><a10:updated>2015-10-27T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I've always kept myself up to date with the latest languages arriving on the scene and I've spent time in the past learning Node and last year I learnt Python by doing the Omnisharp plugin for Sublime.  I have recently been looking for a static language that I can transfer my C# skills too and I had narrowed it down to 3; Swift, Kotlin and GoLang.  I started out with Kotlin and setting up a dev environment with IntelliJ and running the koans that Jetbrains advise you step through to pick up the language.  Whilst it seemed relatively straightforward I got "noob confused" when I saw examples of Java calling into Kotlin with get/set prefixes somehow magically added to Kotlin properties. It turns out the Kotlin compiler does this for Java libraries so it can communicate with it, to me it seemed strange that I code a library in one language and the compiler then exposes these methods and properties slightly differently. Superficial as this sounds I also didn't really like the mammoth that appears to be IntelliJ.  Coming from a predominantly Visual Studio background but working with Omnisharp I wanted a lightweight editor with some refactoring, intellisense and error highlighting.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;I had come across &lt;a href="http://openmymind.net/The-Little-Go-Book/"&gt;The Little Go Book&lt;/a&gt; a while ago and even went to the extent of getting it printed as a book via &lt;a href="https://www.epubli.co.uk"&gt;ePubli&lt;/a&gt; as I'm old school and would like to read something that lengthy on paper not a screen.  Some months passed with it gathering dust but I had made the decision that I wanted to branch out into another language and community so I ditched Kotlin and started reading this book.  Its only 50 pages long but due to work and family life this took a week to complete but it also got me questioning my understanding of C#.  &lt;/p&gt;

&lt;p&gt;After listening to &lt;a href="https://twitter.com/vansimke"&gt;Michael Van Sickle&lt;/a&gt; on a recent podcast I setup my Atom editor with the &lt;a href="https://atom.io/packages/go-plus"&gt;Golang plugin&lt;/a&gt; and also used the Go website (which has an &lt;a href="https://tour.golang.org/welcome/1"&gt;online editor&lt;/a&gt; which you can use to execute Go code) to test my understanding of how to use Go.&lt;/p&gt;

&lt;p&gt;After a week of reading the book and executing the code I thought it was time to try and write a real world application.  &lt;/p&gt;

&lt;p&gt;For a while I had been wanting to use &lt;a href="https://getpocket.com/"&gt;Pocket&lt;/a&gt; a tool which helps you save links you want to go back and read at a later date. Currently I use Twitter favourites as a way to save interesting links that I will go back and read but I wondered if a tool like Pocket might be a better.  &lt;/p&gt;

&lt;p&gt;I had scowered the internet looking for a tool that would go through your favourites and import them into Pocket.  I found one website that aimed to do that but basically didn't work.  I found &lt;a href="https://ifttt.com/"&gt;IFTTT&lt;/a&gt; which would automatically move favourites to Pocket but only starting as of now.&lt;/p&gt;

&lt;h3&gt;Lets do this&lt;/h3&gt;

&lt;p&gt;So I was going to write a tool that would import all your favourites into Pocket.  First thing I would need to do is connect to Twitter to get my favourites from my Go app so I fired up Google to see what was out there.  I came across the &lt;a href="https://github.com/ChimeraCoder/anaconda"&gt;Anaconda&lt;/a&gt; library which is a Go wrapper for the Twitter API.  This would get my list of favourites but it wasn't going to do the authentication for me.  Google to the rescue again, I came across this &lt;a href="https://github.com/mrjones/oauth"&gt;OAuth&lt;/a&gt; library which would allow me to authenticate against Twitter with a consumer key and secret and then prompt the user to allow the app to have access to tweets.  By doing this, Twitter would give the user a code that they could then paste into the app and in return it would make a request to Twitter to get a access token. Easy peasy!&lt;/p&gt;

&lt;p&gt;Now I needed to communicate with Pocket so more Googling resulted in this &lt;a href="https://github.com/quekshuy/pocket-golang-sdk"&gt;library&lt;/a&gt;.  Whilst this had the basics to connect to Pocket I had to add extra steps to get authentication working properly.  I also had to read their &lt;a href="https://getpocket.com/developer/docs/authentication"&gt;documentation&lt;/a&gt; about 20 times to understand their workflow.  I think their authentication workflow is aimed at mobile clients and websites authenticating against them rather than console applications.  Two of their steps require passing in redirect urls which is no good if you're a terminal app.  Also their documentation states that if the user authorises the app to access the user's Pocket account it will redirect back to a url, it will also redirect to the same url if the user doesn't allow access. Yeah that makes sense!?!  Rather than having it redirect it would have been helpful if it did what Twitter did and provide a code the user can paste into the terminal but alas that's not an option so I had to fire up a webserver in my app that Pocket would redirect to.  Surprisingly that only took two lines of code!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http.Handle("/", http.FileServer(http.Dir("./static")))
http.ListenAndServe(":3000", nil)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once I had authenticated I had to do a bit more reading regarding the Twitter API, I had to have logic to keep calling the Twitter API to get favourites until I had got them all.  By default you can only return 200 favourites in one go and I had over that stored on Twitter.&lt;/p&gt;

&lt;p&gt;I then had to ensure that my array of tweets were oldest first because when I looped over them adding them to Pocket I wanted the most recent favourites first.  Reversing an array is not as easy as it sounds unfortunately and especially when in C# I'm used to &lt;code&gt;Array.Reverse(array);&lt;/code&gt;  You could write a &lt;code&gt;for&lt;/code&gt; loop starting from the length of the array rather than zero and populate a new array with the last item first but I didn't want to do that really although it might have been simpler in the long run. Instead of doing that I could have looped over my array in a for loop but I wanted to use a &lt;code&gt;foreach&lt;/code&gt; loop or in Go thats called &lt;code&gt;range&lt;/code&gt;.  What I had to do is similar to what you do in C# when doing custom comparisons and that is I had to implement an interface that would implement &lt;code&gt;Len,Less,Swap&lt;/code&gt; methods.  It was the &lt;code&gt;Less&lt;/code&gt; method which was what did the magic for us, ie/order the tweets by date.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (slice Tweets) Less(i, j int) bool {
    firstTime, err := slice[i].CreatedAtTime()  
    if err != nil {
        fmt.Println("oops")
    }
    secondTime, err2 := slice[j].CreatedAtTime()
    if err2 != nil {
        fmt.Println("oops again!")
    }

    return firstTime.Before(secondTime)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we take in a slice (a wrapped array) of tweets, we get our first and second tweet and call a method on it that parses a datetime in string format and return a &lt;code&gt;time&lt;/code&gt; object specific to Go.  We then return a boolean to say whether the first tweet's date is before the second tweet's date. Then to reverse our array now we have that interface implemented we can call &lt;code&gt;sort.Sort(myFavourites)&lt;/code&gt;. As I say when used to a one line statement reversing an array or using LINQ to do custom ordering of an array doing all this code seems a bit over the top but I think this is just because Go is reasonably young and the authors are trying to keep it fairly simple.  Whether or not they will add LINQ type abstractions to the language in time, who knows but I think they are looking at languages like C# where things have been added and added over time and not wanting to populate Go too much. That's what I've heard anyway but what do I know I've only been using it for 7 days.&lt;/p&gt;

&lt;p&gt;Some other things I noticed was that when you import a package it is case sensitive otherwise Go doesn't really like it. Also naming your variables matters. I have a package called &lt;code&gt;twitter&lt;/code&gt;, I created a type called &lt;code&gt;Twitter&lt;/code&gt; inside that package so to instantiate that I did &lt;code&gt;twitter := twitter.Twitter{}&lt;/code&gt;. So that's &lt;code&gt;package.type&lt;/code&gt;.  I could then call methods by doing &lt;code&gt;twitter.&lt;/code&gt; however once I added another type in the &lt;code&gt;twitter&lt;/code&gt; package when I did  &lt;code&gt;otherVariable := twitter.MyNewType{}&lt;/code&gt; it complained that &lt;code&gt;twitter&lt;/code&gt; didn't have anything called &lt;code&gt;MyNewType&lt;/code&gt; inside it.  What it was saying was that my variable called &lt;code&gt;twitter&lt;/code&gt; didn't have it not that my package didn't have it so we have to be careful on naming things which is where developers fail!&lt;/p&gt;

&lt;p&gt;What I will say is that when I was scratching my head on a Go issue I found a lot of blog posts and resources for Go which is a nice surprise due to its age however, its not actually that young. It was announced in 2009 and seeing its now 2015, 6 years in the tech world is a long time so its good to see all the resources out there for it.&lt;/p&gt;

&lt;p&gt;Anyway, back to the code, once I had my array of favourites I could loop over them and do some logic on the tweets.  Usually when I favourite something it would have a link to an article so I had to decide how I was going to handle this.  I decided upon if the url had a host of github.com or if the extension was empty or it ended in either html, pdf, aspx or md then we would post that link to Pocket otherwise we wouldn't add it.  If the tweet didn't contain a link we would post the url of the tweet to Pocket.  What this exposed me to was the libraries in Go that dealt with urls and files.&lt;/p&gt;

&lt;p&gt;So now I had my app working I needed a decent name and some decent styling on the web page that Pocket redirected to after the user had been authorised.  I put out a Tweet asking for help and had a couple of suggestions for a name but I decided upon PoGo which came from &lt;a href="https://twitter.com/AquaBirdConsult"&gt;Khalid Abuhakmeh&lt;/a&gt; and I had a great pull request from &lt;a href="https://twitter.com/hougasian"&gt;Kevin Hougasian&lt;/a&gt; which had a great design for the authorised page.&lt;/p&gt;

&lt;h3&gt;Introducing PoGo&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/pogo.png" alt="PoGo App" title="PoGo App" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/pogoauthorised.png" alt="PoGo Authorised Page" title="PoGo Authorised Page" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://github.com/jchannon/pogo"&gt;Github link&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I haven't touched on a lot of things in Go that I've learnt for example, &lt;code&gt;goroutines&lt;/code&gt; but I hope I've introduced some simple examples to demonstrate how Go can be used quite easily.  I would say that Go is quite similar to Python but obviously has its own unique features.  It is a statically typed language so has features like intellisense and error information when it compiles.  Nicely the Atom plugin gives the intellisense in the editor and also error messages when you save the documents you are working on.  I've dabbled quite a bit with Node in the past but never fully immersed myself in the community because I always had the day job of C# needing my attention and that still applies today.  However, the company I work for deploys their product on Linux using Mono and so I am getting more involved in Linux and I think its pretty clear that Linux has won the deployment story so much so Microsoft are making ASPNet5 run on Linux and the number of tools on Linux vs Windows is considerably highly so with that I think its important to learn a language that is a first class citizen on that platform.  Starting afresh in a new community is daunting as you are starting from scratch but I'm at a point where I'd like to get involved with new people and learn a new language that is a first class citizen on Linux.  The problem I have is making the time to make this happen!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2015/08/07/hypermedia-and-nancyfx/</guid><link>http://blog.jonathanchannon.com/2015/08/07/hypermedia-and-nancyfx/</link><a10:author><a10:name /></a10:author><category>.net</category><category>hypermedia</category><category>nancyfx</category><category>oss</category><category>REST</category><title>NancyFX and Hypermedia</title><description>&lt;p&gt;I've been slowly educating my self on hypermedia; what it is, how does it help and how to use it.  I must say I've found it a very interesting topic and I thought it was time I put some information into a blog post just in case the 2 people that read this blog might find it useful.&lt;/p&gt;

&lt;p&gt;In my day job I'm responsible for a HTTP API (notice I didn't use REST) and some months ago I spoke to Glenn Block around a general discussion about hypermedia.  Glenn put this on YouTube if you want to watch it.&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/G4YeQMIdO6Q" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

</description><pubDate>Thu, 06 Aug 2015 23:00:00 Z</pubDate><a10:updated>2015-08-06T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I've been slowly educating my self on hypermedia; what it is, how does it help and how to use it.  I must say I've found it a very interesting topic and I thought it was time I put some information into a blog post just in case the 2 people that read this blog might find it useful.&lt;/p&gt;

&lt;p&gt;In my day job I'm responsible for a HTTP API (notice I didn't use REST) and some months ago I spoke to Glenn Block around a general discussion about hypermedia.  Glenn put this on YouTube if you want to watch it.&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/G4YeQMIdO6Q" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;At the time I was deliberating whether our API should return payloads that adhere to a hypermedia type but from what I could tell its quite hard if you are the client consuming these payloads.  For example, if you have an Angular SPA, you need code to traverse the links that the server returns.  What happens if the user bookmarks a page and then comes back to it at a later date? The app will need to go back to the entry point of the API then traverse. What happens if the user presses the refresh button in the browser? Again more traversal.  My conclusions were that there is very little tooling and libraries to allow web applications to be hypermedia clients.  This was October 2014 and at the time of writing, August 2015 I think there have been some improvements but not a great deal.&lt;/p&gt;

&lt;p&gt;After my chat with Glenn, he was using a hypermedia type called &lt;a href="http://amundsen.com/media-types/collection/"&gt;Collection+Json&lt;/a&gt; and he wondered if I'd be interested in writing support for it for a &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt; application.  I obliged as I wanted to learn more about hypermedia and Nancy now has a library you can use for Collection+Json if you choose to use that hypermedia type.  You can find this on nuget &lt;a href="http://www.nuget.org/packages/Nancy.CollectionJson/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In simple terms you may have a module such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class OrderModule : NancyModule
{
  public OrderModule(IOrderRepository orderRepo) : base("/orders")
  {
    Get["/"] = _ =&amp;gt;
    {
      var orders = orderRepo.GetOrders();
      return orders;
    };
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you send in a request with &lt;code&gt;Accept : application/json&lt;/code&gt; you will get a JSON representation of an array of orders&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
    {
        "id" : 1,
        "status":"Complete",
        "itemcount":3
    },
    {
        "id" : 2,
        "status":"Pending",
        "itemcount":1
    }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However if you send in a request &lt;code&gt;Accept : application/vnd.collection+json&lt;/code&gt; you will get a Collection+Json representation&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "version": "1.0",
    "href": "http://localhost:9200/orders/",
    "links": [
        {
            "rel": "Feed",
            "href": "http://localhost:9200/orders/rss"
        }
    ],
    "items": [
        {
            "href": "http://localhost:9200/orders/1",
            "data": [
                {
                    "name": "id",
                    "value": "1",
                    "prompt": "Id"
                },
                {
                    "name": "itemcount",
                    "value": "3",
                    "prompt": "Item Count"
                },
                {
                    "name": "status",
                    "value": "complete",
                    "prompt": "Status"
                }
            ],
            "links": [
                {
                    "rel": "items",
                    "href": "http://localhost:9200/orders/1/items",
                    "prompt": "Items"
                }
            ]
        },
        {
            "href": "http://localhost:9200/orders/2",
            "data": [
                {
                    "name": "id",
                    "value": "2",
                    "prompt": "Id"
                },
                {
                    "name": "itemcount",
                    "value": "1",
                    "prompt": "Item Count"
                },
                {
                    "name": "status",
                    "value": "pending",
                    "prompt": "Status"
                }
            ],
            "links": [
                {
                    "rel": "items",
                    "href": "http://localhost:9200/orders/2/items",
                    "prompt": "Items"
                }
            ]
        }
    ],
    "queries": [
        {
            "rel": "search",
            "href": "http://localhost:9200/orders/search",
            "prompt": "Search",
            "data": [
                {
                    "name": "name",
                    "prompt": "Value to match against the order number"
                }
            ]
        }
    ],
    "template": {
        "data": [
            {
                "name": "productcode",
                "prompt": "Product Code"
            },
            {
                "name": "quantity",
                "prompt": "Quantity"
            }
        ]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How this works is by content negotiation. As with most things, Nancy makes this very simple.  In Nancy.Collection+Json I wrote a &lt;code&gt;ResponseProcessor&lt;/code&gt; that handles Accept headers for Collection+Json and then finds a "writer" responsible for the entity being requested, which writes all the JSON properties seen above and then this is returned in the response.  The code and demo can be found &lt;a href="https://github.com/jchannon/Nancy.CollectionJson"&gt;here&lt;/a&gt; and more documentation is available &lt;a href="https://github.com/WebApiContrib/CollectionJson.Net#returning-a-read-document-from-a-server"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After some months had passed I saw &lt;a href="https://twitter.com/mamund"&gt;Mike Amundsen&lt;/a&gt; had done a talk at NDC Oslo 2015 about building clients that consume hypermedia payloads.  This spiked my interest again in hypermedia and I thoroughly recommend you give it a watch.&lt;/p&gt;

&lt;iframe src="https://player.vimeo.com/video/131642790" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Leading on from this video I realised some of the differences between hypermedia types were how they enabled clients to perform.  For example, with &lt;a href="http://stateless.co/hal_specification.html"&gt;HAL&lt;/a&gt; this hypermedia type will return links about the resource you requested but it won't directly tell how you to add an order to the system in its payload, you have to follow a link to find out that information.  Collection+Json does give you that information as a &lt;code&gt;template&lt;/code&gt; and so does &lt;a href="https://github.com/kevinswiber/siren"&gt;Siren&lt;/a&gt; in its &lt;code&gt;action&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It was at this point I thought I'd write another Nancy library that would allow you to return Siren payloads which you can check out &lt;a href="https://github.com/jchannon/Nancy.Siren"&gt;here&lt;/a&gt;.  This works in a similar way as the Nancy.Collection+Json library, in that you as the programmer create a "writer" class that forms the Siren response for a specific resource.  Check out the demo &lt;a href="https://github.com/jchannon/Nancy.Siren/tree/master/src/Nancy.Siren.Demo"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At around the same time I also listened to a podcast that Mike was on which discussed REST, hypermedia and clients.  I would strongly recommend you listen to this.  I'd come to a lot of conclusions and had thoughts about REST and hypermedia and they were all validated in this podcast so &lt;a href="https://t.co/V9NoBlWLOc"&gt;check it out&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One thing the podcast did touch on is browsers as clients and the complexities involved when they consume hypermedia.  I think this area is in need of huge improvement regarding tooling and libraries and I know Mike is working on a &lt;a href="https://twitter.com/LCHBook"&gt;book&lt;/a&gt; which may help that but I think we need more people talking about this and people hacking on it to try and make it "a thing".  Tomorrow, Aug 8th 2015, &lt;a href="https://twitter.com/darrel_miller"&gt;Darrel Miller&lt;/a&gt; is about to do a keynote at DDD Melbourne titled "Consuming REST APIs, for all interpretations of REST" which will be something that illustrates how we can write client apps that consumer hypermedia APIs and gets that information out there.  Without this information we have the potential of making hypermedia a tool that only serves the purpose of machine to machine communication and I'm not sure we want that if we can use it to make our client applications easier to maintain whether they be browsers, desktop apps or mobile apps.  &lt;/p&gt;

&lt;p&gt;So in conclusion I recommend you go learn a bit more about REST and hypermedia if it interests you, watch the videos and listen to the podcast.  Then have a play with returning hypermedia payloads with your Nancy app and then trying to consume them.&lt;/p&gt;

&lt;p&gt;One thing I will briefly touch on is when deciding upon the hypermedia type I think you need to discuss this with your consumers.  Its very easy to write an API and say "we are going to return X" but if you're not the client developer that has to handle that then you haven't made an informed decision.  You should try and make it as easy as possible for your consumers.  Discuss the types that they find easiest to use.  You as the API developer should try writing a client that consumes the content and realise the difficulties involved which may help you make a more rounded decision on what hypermedia type to return from your API.  Now this is assuming you only decide to return one hypermedia type from your API, of course if you wanted to go purist then you would return all the major hypermedia types from your API and all your clients would be happy but that's probably not reality.&lt;/p&gt;

&lt;p&gt;A quick shout out to &lt;a href="https://twitter.com/danbarua"&gt;Dan Barua&lt;/a&gt; who has written Nancy.HAL, a ResponseProcessor that allows your Nancy API to return HAL payloads.  Check this library out &lt;a href="https://github.com/danbarua/Nancy.Hal"&gt;here&lt;/a&gt;. Great work Dan!&lt;/p&gt;

&lt;p&gt;So in summary if you're using Nancy and want to return hypermedia payloads we have you covered:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/danbarua/Nancy.Hal"&gt;Nancy.Hal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jchannon/Nancy.CollectionJson"&gt;Nancy.Collection+Json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jchannon/Nancy.Siren"&gt;Nancy.Siren&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally a big thank you to Glenn Block and Darrel Miller on answering all my API and hypermedia questions over the last year, you've been a great help.  If I now have any questions on API and hypermedia I bring out the big guns and call "Glenn Miller". I really need to get their phone numbers so I can have this ringtone. (For all you youngsters who have no idea who Glenn Miller is, Google it!)&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/xPXwkWVEIIw?list=PLc9JoCRiZqs2us9wpl46_eKGO3_AkaIE-" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2015/04/27/cookie-authentication-angularjs-csrf-owin-mono/</guid><link>http://blog.jonathanchannon.com/2015/04/27/cookie-authentication-angularjs-csrf-owin-mono/</link><a10:author><a10:name /></a10:author><category>.net</category><category>angularjs</category><category>mono</category><category>nancyfx</category><category>oss</category><category>owin</category><title>Cookie Authentication &amp; CRSF with AngularJs, Owin &amp; Mono</title><description>&lt;p&gt;I'm currently working on a project that has &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt; serving up an API.  For the UI there is AngularJS.  We were using JWT for authentication just to get us up and running but then as things became more final in the product we knew it would be better to swap to cookies for security plus we may as well leverage the browser capabilities  for cookie handling. I'm not going to get into the arguments about JWT security vs cookie security, there are advantages/disadvantages for using both in this scenario.  Our API is built on top of OWIN and Microsoft provide cookie middleware so I thought this would be nice and simple to plug in.  Lets just remember I'm working on Mono!&lt;/p&gt;

&lt;p&gt;In our Startup class I added the below&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.UseCookieAuthentication(new CookieAuthenticationOptions
{
    AuthenticationMode = AuthenticationMode.Active,
    CookieHttpOnly = true,
    CookieSecure = Microsoft.Owin.Security.Cookies.CookieSecureOption.SameAsRequest,
    SlidingExpiration = true,
    AuthenticationType = "MyCookie",
    CookieName = "MyCookie"
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hopefully thats pretty self explanatory. So I fired up my application and BOOM!&lt;/p&gt;

</description><pubDate>Sun, 26 Apr 2015 23:00:00 Z</pubDate><a10:updated>2015-04-26T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I'm currently working on a project that has &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt; serving up an API.  For the UI there is AngularJS.  We were using JWT for authentication just to get us up and running but then as things became more final in the product we knew it would be better to swap to cookies for security plus we may as well leverage the browser capabilities  for cookie handling. I'm not going to get into the arguments about JWT security vs cookie security, there are advantages/disadvantages for using both in this scenario.  Our API is built on top of OWIN and Microsoft provide cookie middleware so I thought this would be nice and simple to plug in.  Lets just remember I'm working on Mono!&lt;/p&gt;

&lt;p&gt;In our Startup class I added the below&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.UseCookieAuthentication(new CookieAuthenticationOptions
{
    AuthenticationMode = AuthenticationMode.Active,
    CookieHttpOnly = true,
    CookieSecure = Microsoft.Owin.Security.Cookies.CookieSecureOption.SameAsRequest,
    SlidingExpiration = true,
    AuthenticationType = "MyCookie",
    CookieName = "MyCookie"
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hopefully thats pretty self explanatory. So I fired up my application and BOOM!&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Could not load type 'Microsoft.Owin.Security.DataProtection.DpapiDataProtector' &lt;/p&gt;

&lt;p&gt;Turns out the default security for cookie auth doesn't work on Mono. &lt;strong&gt;Fix&lt;/strong&gt; : Install &lt;a href="https://www.nuget.org/packages/Owin.Security.AesDataProtectorProvider"&gt;Owin.Security.AesDataProtectorProvider&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now we add this to be the bottom of our OWIN middleware:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.UseAesDataProtectorProvider();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fire up our app and bingo!&lt;/p&gt;

&lt;p&gt;Now we need to handle logging in our user. As we are using OWIN we need to use Claims for our authenticated users. &lt;strong&gt;Fix&lt;/strong&gt; Install &lt;a href="https://www.nuget.org/packages/Nancy.MSOwinSecurity/"&gt;Nancy.MSOwinSecurity&lt;/a&gt;, this gives us Claims support inside Nancy (in v2 of Nancy the default authentication model will be using Claims, keep an eye out for release information). Below is our login code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeModule : NancyModule
{
    public HomeModule()
    {
        Post["/login"] = _ =&amp;gt;
        {
            //probably best we validate the user here!!!

            var claims = new List&amp;lt;Claim&amp;gt;(new[]
                {
                    new Claim(ClaimTypes.Email, "blah@blah.com"), 
                    new Claim(ClaimTypes.Name, "Mr Blah")
                });

            this.Context.GetAuthenticationManager().SignIn(new ClaimsIdentity(claims, "MyCookie"));

            return Response.AsRedirect("/");
        };
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What will happen here is as the request comes in it will fall through the cookie middleware, fall in to Nancy, we will login and as the request reverses back up the OWIN pipeline the cookie middleware will see there is someone logged in and return a cookie in the response.&lt;/p&gt;

&lt;p&gt;Super duper!&lt;/p&gt;

&lt;p&gt;One thing that authentication middleware tends to do however, is still allows requests through the pipeline even if there isn't a authenticated user. The reason being there may be other middleware that may be responsible for authenticating users in another manner.  We plan to add JWT back into our API so we would end up with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.UseJwtBearerAuthentication(); 
app.UseCookieAuthentication();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our previous use of &lt;a href="https://github.com/jchannon/Owin.StatelessAuth/"&gt;JWT&lt;/a&gt; would return a 401 before it dropped through to the next middleware so we needed to swap out Owin.StatelessAuth to the MS middleware above.  The problem now is we need some more middleware to protect our API after the request has dropped through the JWT and cookie middleware:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public Task Invoke(IDictionary&amp;lt;string, object&amp;gt; environment)
{
    if (!environment.ContainsKey("owin.RequestPath"))
    {
        throw new ApplicationException("Invalid OWIN request. Expected owin.RequestPath, but not present.");
    }


    if (!environment.ContainsKey(ServerUser) || environment[ServerUser] == null)
    {
        return AuthChallengeResponse(environment);
    }

    return nextFunc(environment);
}

private Task AuthChallengeResponse(IDictionary&amp;lt;string, object&amp;gt; environment)
{
    environment["owin.ResponseStatusCode"] = 401;

    if (this.options != null &amp;amp;&amp;amp; !string.IsNullOrWhiteSpace(this.options.WWWAuthenticateChallenge))
    {
        var wwwauthenticatechallenge = this.options.WWWAuthenticateChallenge;

        if (!environment.ContainsKey("owin.ResponseHeaders"))
        {
            environment.Add("owin.ResponseHeaders", new Dictionary&amp;lt;string, string[]&amp;gt;());
        }

        var responseHeaders = (IDictionary&amp;lt;string, string[]&amp;gt;)environment["owin.ResponseHeaders"];
        responseHeaders.Add("WWW-Authenticate", new[] { wwwauthenticatechallenge });
    }

    return Task.FromResult(0);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we end up with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.UseJwtBearerAuthentication(); 
app.UseCookieAuthentication();
app.CheckLoggedInUser();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now we have it all working but what about &lt;a href="http://en.wikipedia.org/wiki/Cross-site_request_forgery"&gt;CRSF&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;Luckily Angular has some built in mechanisms to cater for CSRF and in its documentation is this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To take advantage of this, your server needs to set a token in a JavaScript readable session cookie called XSRF-TOKEN on first HTTP GET request. On subsequent non-GET requests the server 
  can verify that the cookie matches X-XSRF-TOKEN HTTP header, and therefore be sure that only JavaScript running on your domain could have read the token. The token must be unique for each 
  user and must be verifiable by the server (to prevent the JavaScript making up its own tokens). We recommend that the token is a digest of your site's authentication cookie with salt for 
  added security.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So now we need some middleware to create this &lt;code&gt;XSRF-TOKEN&lt;/code&gt; cookie:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class XSRFCookieMiddleware
{
    private readonly Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc;

    public XSRFCookieMiddleware(Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc)
    {
        this.nextFunc = nextFunc;
    }

    public  Task Invoke(IDictionary&amp;lt;string, object&amp;gt; env)
    {
        var context = new OwinContext(env);

        context.Response.OnSendingHeaders(_ =&amp;gt;
            {
                if ((string)env["owin.RequestPath"] == "/login" &amp;amp;&amp;amp; (string)env["owin.RequestMethod"] == "POST")
                {
                    var responseHeaders = (IDictionary&amp;lt;string, string[]&amp;gt;)env["owin.ResponseHeaders"];
                    if (responseHeaders.ContainsKey("Set-Cookie"))
                    {
                        var setcookies = responseHeaders["Set-Cookie"].ToList();
                        var authcookie = setcookies.FirstOrDefault(x =&amp;gt; x.StartsWith("VQCookie"));
                        if (authcookie != null)
                        {
                            var authcookieValue = authcookie.Split(new[]{ '=' }, StringSplitOptions.RemoveEmptyEntries)[1].Split(new [] { ';' })[0];
                            var csrfToken = new CsrfTokenHelper().GenerateCsrfTokenFromAuthToken(authcookieValue);
                            setcookies.Add("XSRF-TOKEN=" + csrfToken + ";path=/");
                            responseHeaders["Set-Cookie"] = setcookies.ToArray();
                        }
                    }
                }
                else if ((string)env["owin.RequestPath"] == "/api/authenticate/logout" &amp;amp;&amp;amp; (string)env["owin.RequestMethod"] == "GET")
                {
                    context.Response.Cookies.Append("XSRF-TOKEN", "DEAD", new CookieOptions{ Expires = DateTime.UtcNow.AddDays(-1) });
                }
            }, null);

        return nextFunc(env);
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the usage of &lt;code&gt;OnSendingHeaders&lt;/code&gt;, this is required because when a response stream is first written to in an OWIN pipeline the headers are flushed and using this event from Microsoft they will be called when the response is returned.  I had previously written this middleware similar to the below beforehand but this approach could potentially have unwanted effects:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public async Task Invoke(IDictionary&amp;lt;string, object&amp;gt; env)
{
  await nextFunc(env);
  if ((string)env["owin.RequestPath"] == "/login" &amp;amp;&amp;amp; (string)env["owin.RequestMethod"] == "POST")
  {
      //
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One thing to note that cost me a few days is the ordering of these events.  Suffice to say there is no specification for this but HttpListener, Nowin &amp;amp; System.Web call these events via LIFO (Last In First Out) which means that our middleware needs to be above the &lt;code&gt;app.UseCookieAuthentication()&lt;/code&gt; line so that our event gets called after the auth cookie has been set.&lt;/p&gt;

&lt;p&gt;So now we return the &lt;code&gt;XSRF-TOKEN&lt;/code&gt; cookie on login Angular will now detect this and then send the &lt;code&gt;X-XSRF-TOKEN&lt;/code&gt; header on every request. We now need some middleware to obviously validate this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class XSRFMiddleware
{
    private readonly Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc;

    public XSRFMiddleware(Func&amp;lt;IDictionary&amp;lt;string, object&amp;gt;, Task&amp;gt; nextFunc)
    {
        this.nextFunc = nextFunc;
    }

    public async Task Invoke(IDictionary&amp;lt;string, object&amp;gt; environment)
    {
        var owinContext = new OwinContext(environment);

        var requestHeaders = (IDictionary&amp;lt;string, string[]&amp;gt;)environment["owin.RequestHeaders"];

        if (owinContext.Request.Method == "GET")
        {
            await nextFunc(environment);
        }
        else if (requestHeaders.ContainsKey("Authorization") &amp;amp;&amp;amp; !requestHeaders.ContainsKey("Cookie"))
        {
            await nextFunc(environment);
        }
        else
        {
            if (owinContext.Request.Cookies.Count() == 0)
            {
                environment["owin.ResponseStatusCode"] = 401;
                return;   
            }

            if (string.IsNullOrWhiteSpace(owinContext.Request.Cookies["MyCookie"]))
            {
                environment["owin.ResponseStatusCode"] = 401;
                return;
            }

            if (!requestHeaders.ContainsKey("X-XSRF-TOKEN"))
            {
                environment["owin.ResponseStatusCode"] = 401;
                return;
            }

            var authCookie = owinContext.Request.Cookies["MyCookie"];

            var csrfToken = requestHeaders["X-XSRF-TOKEN"].FirstOrDefault();

            var valid = new CsrfTokenHelper().DoesCsrfTokenMatchAuthToken(csrfToken, authCookie);

            if (!valid)
            {
                environment["owin.ResponseStatusCode"] = 401;
                return;
            }

            await nextFunc(environment);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now we have&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.UseXSRFCookieMiddleware();  //Add XSRF-TOKEN cookie on login
app.UseJwtBearerAuthentication(); 
app.UseCookieAuthentication();
app.UseXSRFMiddleware(); //Validate XSRF requests
app.UseOwinUserVerifcation();  //Check server.User key is populated
app.UseNancy();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One thing to note is the CsrfTokenHelper, below is the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class CsrfTokenHelper
{
    const string ConstantSalt = "MYSALT";

    public string GenerateCsrfTokenFromAuthToken(string authToken)
    {
        return GenerateCookieFriendlyHash(authToken);
    }

    public bool DoesCsrfTokenMatchAuthToken(string csrfToken, string authToken)
    {
        return csrfToken == GenerateCookieFriendlyHash(authToken);
    }

    private static string GenerateCookieFriendlyHash(string authToken)
    {
        using (var sha = SHA256.Create())
        {
            var computedHash = sha.ComputeHash(Encoding.Unicode.GetBytes(authToken + ConstantSalt));
            var cookieFriendlyHash = UrlTokenEncode(computedHash);  //HttpServerUtility.UrlTokenEncode
            return cookieFriendlyHash;
        }
    }

    //Borrowed from Mono System.Web ;)
    private static string UrlTokenEncode(byte[] input)
    {
        if (input == null)
            throw new ArgumentNullException("input");
        if (input.Length &amp;lt; 1)
            return String.Empty;
        string base64 = Convert.ToBase64String(input);
        int retlen;
        if (base64 == null || (retlen = base64.Length) == 0)
            return String.Empty;

        // MS.NET implementation seems to process the base64
        // string before returning. It replaces the chars:
        //
        //  + with -
        //  / with _
        //
        // Then removes trailing ==, which may appear in the
        // base64 string, and replaces them with a single digit
        // that's the count of removed '=' characters (0 if none
        // were removed)
        int equalsCount = 0x30;
        while (retlen &amp;gt; 0 &amp;amp;&amp;amp; base64[retlen - 1] == '=')
        {
            equalsCount++;
            retlen--;
        }
        char[] chars = new char[retlen + 1];
        chars[retlen] = (char)equalsCount;
        for (int i = 0; i &amp;lt; retlen; i++)
        {
            switch (base64[i])
            {
                case '+':
                    chars[i] = '-';
                    break;

                case '/':
                    chars[i] = '_';
                    break;

                default:
                    chars[i] = base64[i];
                    break;
            }
        }
        return new string(chars);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You'll notice I had to pinch &lt;code&gt;UrlTokenEncode&lt;/code&gt; from Github from System.Web! OSS FTW! I didn't want to take a dependency on System.Web just for one method!&lt;/p&gt;

&lt;p&gt;Once I'd finally got this all sorted I wanted to write some tests to check all was working for example I was getting 2 cookies after logging in.&lt;/p&gt;

&lt;p&gt;I was already using &lt;a href="https://www.nuget.org/packages/Microsoft.Owin.Testing/"&gt;Microsoft.Owin.Testing&lt;/a&gt; and all seemed ok however I discovered I couldn't actually test for cookies as there was no handler to pass to the HttpClient that the TestServer returns.  Luckily in all situations OWIN related &lt;a href="http://twitter.com/randompunter"&gt;@randompunter&lt;/a&gt; always has another better option, this time called &lt;a href="http://www.nuget.org/packages/OwinHttpMessageHandler/"&gt;OwinHttpMessageHandler&lt;/a&gt;. This allowed me to test against cookies which the previous library didn't but then I spotted an issue on Windows. My test passed on Mono and failed on Windows, its usually the other way around.  This is where the ordering of the middleware and &lt;code&gt;OnSendingHeaders&lt;/code&gt; played a big part.  Anyway after a bit of toing and froing we discovered a bug and it was fixed however, &lt;strong&gt;beware&lt;/strong&gt; Microsoft.Owin.Testing also has this bug in that it uses FIFO(first in first out).  So MS hosts act in one way but their test library works the opposite!&lt;/p&gt;

&lt;p&gt;Here's the test&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class CookieTests
{
    [Fact]
    public async Task Should_Have_2_Headers()
    {
        var app = GetAppBuilder();
        var handler = GetHandler(app);
        var client = CreateHttpClient(handler);

        var response = await client.PostAsync("http://localhost/login", new StringContent(""));

        var authCookie = handler
           .CookieContainer
           .GetCookies(new Uri("http://localhost"))["MyCookie"];

        var xsrfCookie = handler
            .CookieContainer
            .GetCookies(new Uri("http://localhost/login"))["XSRF-TOKEN"];

        //Then
        Assert.NotNull(authCookie);
        Assert.NotNull(xsrfCookie);
    }

    private AppBuilder GetAppBuilder()
    {
        var app = new AppBuilder();
        new Startup().Configuration(app);
        return app;
    }

    private OwinHttpMessageHandler GetHandler(AppBuilder builder)
    {
        return new OwinHttpMessageHandler(builder.Build())
        {
            UseCookies = true
        };
    }

    private HttpClient CreateHttpClient(OwinHttpMessageHandler handler)
    {
        var client = new HttpClient(handler)
        {
            BaseAddress = new Uri("http://example.com")
        };

        return client;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So after some lessons learnt and bugs fixed we have Cookie Authentication &amp;amp; CRSF with AngularJs, Owin &amp;amp; Mono.  After getting this all working it was pointed out to me that Kestrel, Microsoft's new cross platform web server behaves the opposite to HttpListener, Nowin &amp;amp; System.Web in its ordering of &lt;code&gt;OnSendingHeaders&lt;/code&gt; Isn't this all fun!!&lt;/p&gt;

&lt;p&gt;As you will see I mentioned having token and cookie middleware in the same app and that is my plan but again we have another long story and heads being bashed on the desk and that can be another blog post but the short answer is it should all work in Mono 4 from looking at the code but at the moment Microsoft's &lt;code&gt;app.UseJwtBearerAuthentication();&lt;/code&gt; does not work properly on Mono at the time of writing.&lt;/p&gt;

&lt;p&gt;I hope this blog post is helpful and prevents the pain I experienced with this but I guess on a positive note it was a learning experience!&lt;/p&gt;

&lt;p&gt;One last thing, I can't take the credit for the token generation etc.  I had read a Stackoverflow post about how to do CSRF with Angular and came across &lt;a href="http://stackoverflow.com/questions/15574486/angular-against-asp-net-webapi-implement-csrf-on-the-server"&gt;this&lt;/a&gt;, all I have done is make it work for OWIN.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href="https://twitter.com/PinpointTownes"&gt;@PinpointTownes&lt;/a&gt; and &lt;a href="http://twitter.com/randompunter"&gt;@randompunter&lt;/a&gt; for helping me out and getting this all working in an OWIN app.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/11/24/running-gulp-in-visual-xamarin-studio/</guid><link>http://blog.jonathanchannon.com/2014/11/24/running-gulp-in-visual-xamarin-studio/</link><a10:author><a10:name /></a10:author><category>.net</category><category>gulp</category><category>javascript</category><category>visual studio</category><category>xamarin studio</category><title>Running Gulp in Visual &amp; Xamarin Studio</title><description>&lt;p&gt;I was going to write a long post explaining about all the pain I went through to get this working but then realised you probably don't really care and you just want the code!&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/iYjnPK0.png" alt="Show the code" /&gt;&lt;/p&gt;

</description><pubDate>Mon, 24 Nov 2014 00:00:00 Z</pubDate><a10:updated>2014-11-24T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I was going to write a long post explaining about all the pain I went through to get this working but then realised you probably don't really care and you just want the code!&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/iYjnPK0.png" alt="Show the code" /&gt;&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Where I work we are moving to a Linux stack with .Net using Nancy &amp;amp; Postgres on the backend.  At the moment we have developers working in Windows and others on OSX using Visual Studio and Xamarin Studio respectively.  At the moment when we want to run the whole app, we build our project, drop to the command line and run gulp for the frontend stuff.  In Visual Studio we already have a post build event to copy the static assets to the bin directory (we have a self host and a IIS host) but in Xamarin Studio we don't so if that bin directory gets nuked we have to copy assets across VMs and its one big PITA.&lt;/p&gt;

&lt;p&gt;So here's the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;PostBuildEvent Condition=" '$(OS)' != 'Windows_NT' AND $(ConfigurationName) == Debug"&amp;gt;  

    export PATH=$PATH:/usr/local/bin

    cd $(ProjectDir)..\MyApp.Clients.AngularJS\

    npm install -verbose

    gulp --minify false --bincopy true --copyto "../MyApp.Hosting.Self/bin/debug/static/"
&amp;lt;/PostBuildEvent&amp;gt;
&amp;lt;PostBuildEvent Condition=" '$(OS)' != 'Windows_NT' AND $(ConfigurationName) == Release"&amp;gt;  

    export PATH=$PATH:/usr/local/bin

    cd $(ProjectDir)..\MyApp.Clients.AngularJS\

    npm install -verbose

    gulp --minify true --bincopy true --copyto "../MyApp.Hosting.Self/bin/debug/static/"
&amp;lt;/PostBuildEvent&amp;gt;
&amp;lt;PostBuildEvent Condition=" '$(OS)' == 'Windows_NT'  "&amp;gt;  
  if $(ConfigurationName) == Debug  (
    cd $(ProjectDir)..\MyApp.Clients.AngularJS\

    npm install -verbose

    cd $(ProjectDir)..\MyApp.Clients.AngularJS\

    gulp --minify false --bincopy true --copyto "../MyApp.Hosting.Self/bin/debug/static/"
  ) else (
    cd $(ProjectDir)..\MyApp.Clients.AngularJS\

    npm install -verbose

    cd $(ProjectDir)..\MyApp.Clients.AngularJS\

    gulp --minify true --bincopy true --copyto "../MyApp.Hosting.Self/bin/debug/static/"
  )

&amp;lt;/PostBuildEvent&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Idiosyncrasies&lt;/h2&gt;

&lt;p&gt;So you can probably see some weird things going on there so I'll explain:&lt;/p&gt;

&lt;h3&gt;Separate Build Events for Non-Windows&lt;/h3&gt;

&lt;p&gt;I have a separate post build event for debug and release for non windows builds, I like that, seems correct to keep them separate. However, for Windows builds I have an &lt;code&gt;if&lt;/code&gt; statement.  I did get separate build events firing but then when it hit the &lt;code&gt;npm install&lt;/code&gt; line it just wouldn't work for some reason.&lt;/p&gt;

&lt;h3&gt;Export PATH for Non-Windows&lt;/h3&gt;

&lt;p&gt;I spent a long time trying to get gulp running on OSX. I tried it via executable shell scripts and in a post build event but it just wouldn't work but yet in the terminal all was fine.  I'm not sure if &lt;code&gt;xbuild&lt;/code&gt; gets executed as a different user or with limited privielges but I was stumped until &lt;a href="http://twitter.com/yantrio"&gt;@yantrio&lt;/a&gt; pointed me in the direction of exporting the &lt;code&gt;PATH&lt;/code&gt;.  As &lt;code&gt;Gulp&lt;/code&gt; sits in &lt;code&gt;/usr/local/bin&lt;/code&gt; I had to expose it to the post build event&lt;/p&gt;

&lt;h3&gt;Changing directory&lt;/h3&gt;

&lt;p&gt;In the post build events I change directory to the folder where the gulp file is located.  I did play quickly on doing it without changing directory but there were some disadvantages so I just went with this approach.  On Windows you'll see that I change directory after each command is executed.  For some reason when executed on Windows when the commands are run the current working directory is changed so you have to change it back to folder where you want to run gulp from.  On OSX &amp;amp; Linux you can set it once and all is fine.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So there you have it.  You're asking why not use the &lt;a href="https://visualstudiogallery.msdn.microsoft.com/8e1b4368-4afb-467a-bc13-9650572db708"&gt;Visual Studio 2013&lt;/a&gt; extension that enables running gulp from VS?  That would only work for those developers using Visual Studio and we have developers using Xamarin Studio so we wanted an across the board solution for all developers.  I know VS 2015 will have Gulp runners baked into it but I'm not sure of Xamarin Studio plans.  If you use this and get it working without a &lt;code&gt;if&lt;/code&gt; statement for Windows let me know. &lt;/p&gt;

&lt;p&gt;Also check out &lt;a href="http://omnisharp.net"&gt;OmniSharp&lt;/a&gt; a little project I've been working on!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/11/12/csharp-first-class-citizen-sublime-text/</guid><link>http://blog.jonathanchannon.com/2014/11/12/csharp-first-class-citizen-sublime-text/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>community</category><category>oss</category><category>sublimetext</category><title>Microsoft Endorsing C# as a First Class Citizen in Sublime Text</title><description>&lt;p&gt;At the end of my last &lt;a href="http://blog.jonathanchannon.com/2014/08/05/nancy-aspnetvnext-osx-sublime-text/"&gt;post&lt;/a&gt; on using ASP.Net vNext with Sublime Text I briefly mentioned a &lt;a href="https://github.com/OmniSharp/omnisharp-sublime"&gt;plugin&lt;/a&gt; that aimed at giving intellisense for C# within the editor.  Well 2 months later and I'm happy to announce that intellisense works and I've added a slew of other features that will hopefully make you feel at home away from Visual Studio.&lt;/p&gt;

&lt;p&gt;I discovered the plugin thanks to &lt;a href="http://twitter.com/jasonimison"&gt;Jason Imison&lt;/a&gt; but at that point there was some issues getting the intellisense working consistently because at that time I was using it with an ASP.NET vNext application which didn't have a solution file (*.sln) and the plugin was expecting that.  After speaking to Jason I found out I could change the settings so it wouldn't expect a solution file and give me the intellisense I was after in a text editor.  Eureka, it worked!  I was now on a mission to make Sublime be a first class citizen when writing C#.  Some may question why on earth would I want to edit C# in something other than Visual Studio.  I don't really want to get into that debate here but all I'll say is, it's nice to have other editor options and with Microsoft's mission to provide vNext compatibility with Mono and Visual Studio not running on OSX/Linux it makes sense to have an editor with feature rich C# support (yes I know there is Xamarin Studio but "options" people, "options").&lt;/p&gt;

</description><pubDate>Wed, 12 Nov 2014 00:00:00 Z</pubDate><a10:updated>2014-11-12T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;At the end of my last &lt;a href="http://blog.jonathanchannon.com/2014/08/05/nancy-aspnetvnext-osx-sublime-text/"&gt;post&lt;/a&gt; on using ASP.Net vNext with Sublime Text I briefly mentioned a &lt;a href="https://github.com/OmniSharp/omnisharp-sublime"&gt;plugin&lt;/a&gt; that aimed at giving intellisense for C# within the editor.  Well 2 months later and I'm happy to announce that intellisense works and I've added a slew of other features that will hopefully make you feel at home away from Visual Studio.&lt;/p&gt;

&lt;p&gt;I discovered the plugin thanks to &lt;a href="http://twitter.com/jasonimison"&gt;Jason Imison&lt;/a&gt; but at that point there was some issues getting the intellisense working consistently because at that time I was using it with an ASP.NET vNext application which didn't have a solution file (*.sln) and the plugin was expecting that.  After speaking to Jason I found out I could change the settings so it wouldn't expect a solution file and give me the intellisense I was after in a text editor.  Eureka, it worked!  I was now on a mission to make Sublime be a first class citizen when writing C#.  Some may question why on earth would I want to edit C# in something other than Visual Studio.  I don't really want to get into that debate here but all I'll say is, it's nice to have other editor options and with Microsoft's mission to provide vNext compatibility with Mono and Visual Studio not running on OSX/Linux it makes sense to have an editor with feature rich C# support (yes I know there is Xamarin Studio but "options" people, "options").&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h2&gt;How does it work?&lt;/h2&gt;

&lt;p&gt;I should really introduce &lt;a href="http://twitter.com/jasonimison"&gt;Jason Imison&lt;/a&gt;.  Jason is the author of a library called &lt;a href="https://github.com/OmniSharp/omnisharp-server"&gt;OmniSharpServer&lt;/a&gt; which is a -  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;HTTP wrapper around NRefactory allowing C# editor plugins to be written in any language.&lt;/p&gt;
  
  &lt;p&gt;NRefactory is the C# analysis library used in the SharpDevelop and MonoDevelop IDEs. It allows applications to easily analyze both syntax and semantics of C# programs. It is quite similar to Microsoft's Roslyn project; except that it is not a full compiler – NRefactory only analyzes C# code, it does not generate IL code.  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In simple terms OmniSharpServer is a local web server (written in &lt;a href="http://www.nancyfx.org"&gt;Nancy&lt;/a&gt;) that accepts requests to various different endpoints which returns results about the code you sent to it.  For example, in Sublime Text when you have a string variable and you type &lt;code&gt;.&lt;/code&gt; after the variable a request is sent to OmniSharpServer with a specific payload and the response contains all the possible completions for that variable.  &lt;/p&gt;

&lt;p&gt;The editor plugin is responsible for initiating the requests and dealing with the response in order to provide the user a rich user experience for editing C# and this is hopefully what I have done with &lt;a href="https://github.com/OmniSharp/omnisharp-sublime"&gt;OmniSharpSublime&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;For Mac &amp;amp; Linux users you will need Mono installed.&lt;/p&gt;

&lt;p&gt;For Windows users you will need Python installed and in your PATH.&lt;/p&gt;

&lt;p&gt;Well versed users in Sublime know there is a package manager that allows you to easily install plugins.  In the package manager if you search for OmniSharp you will see the plugin avaiable for install.  Install and away you go!  Please note that I would not consider the plugin a stable release, its close but not quite there yet.&lt;/p&gt;

&lt;p&gt;As I mentioned earlier you can use OmniSharpSublime with a traditional C# solution or with an ASP.Net vNext project (that does not require a solution file) however, you will need a &lt;code&gt;sublime-project&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Let's assume you already have a solution.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Go to "File -&amp;gt; Open" and select the folder with your solution in it.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Go to "Project -&amp;gt; Save Project As" and save a YOURPROJECTNAME.sublime-project in the same location as your *.sln&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Open your YOURPROJECTNAME.sublime-project file that should now appear in the sidebar on the left&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Enter the location to the *.sln file like below&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Your &lt;code&gt;.sublime.project&lt;/code&gt; file should look like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "folders":
    [
        {
            "follow_symlinks": true,
            "path": "."
        }
    ],
    "solution_file": "./testconsoleprj.sln"
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the &lt;code&gt;YOURPROJECT.sublime-project&lt;/code&gt; is set up and saved follow the below:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Close Sublime (YMMV but this seems to be the best way to open the YOURPROJECTNAME.sublime-project)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Open Sublime&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Click "Project -&amp;gt; Open Project", and select your YOURPROJECTNAME.sublime-project file&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now with the &lt;code&gt;sublime.project&lt;/code&gt; open you should be ready to edit your &lt;code&gt;*.cs&lt;/code&gt; files.&lt;/p&gt;

&lt;p&gt;There is one last "nice touch" to add to Sublime before you edit your files.  Sublime allows syntax specific settings so for C# it would be great if we could invoke intellisense when we type a full stop.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Click "Sublime Text -&amp;gt; Preferences -&amp;gt; Settings - More -&amp;gt; Syntax Specific - User"&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Paste in the below code and save&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "auto_complete": true,
    "auto_complete_selector": "source - comment",
    "auto_complete_triggers": [ {"selector": "source.cs", "characters": ".&amp;lt;"} ],
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you're ready to rock!&lt;/p&gt;

&lt;h2&gt;Features&lt;/h2&gt;

&lt;p&gt;Below I will highlight some of the cool features of &lt;a href="https://github.com/OmniSharp/omnisharp-sublime"&gt;OmniSharpSublime&lt;/a&gt; by the power of animated gifs (hope you have a decent internet connection, sorry!)&lt;/p&gt;

&lt;h3&gt;Intellisense&lt;/h3&gt;

&lt;p&gt;This is probably one of the most important features that springs to mind when editing a C# file in a text editor. &lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/IkirwAE.gif" alt="Intellisense" /&gt;&lt;/p&gt;

&lt;h3&gt;Go To Definition&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/ZClA8qG.gif" alt="Go to definition" /&gt;&lt;/p&gt;

&lt;h3&gt;Rename&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/6ByBw5R.gif" alt="Rename" /&gt;&lt;/p&gt;

&lt;h3&gt;Find Usages&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/yUGl59t.gif" alt="Find Usages" /&gt;&lt;/p&gt;

&lt;h3&gt;Go To Implementation&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/3Ypv6H8.gif" alt="Go to Implementation" /&gt;&lt;/p&gt;

&lt;h3&gt;Format Document&lt;/h3&gt;

&lt;p&gt;Checkout the key binding!&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/kkkUiRZ.gif" alt="Format Document" /&gt;&lt;/p&gt;

&lt;h3&gt;Override&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/EntuVe1.gif" alt="Override" /&gt;&lt;/p&gt;

&lt;h3&gt;Add Reference&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/nWZndb0.gif" alt="Add Reference" /&gt;&lt;/p&gt;

&lt;h3&gt;Syntax Errors&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/Ka4tHk6.gif" alt="Syntax Errors" /&gt;&lt;/p&gt;

&lt;h3&gt;Semantic Errors&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/ljEfdfv.gif" alt="Semantic Errors" /&gt;&lt;/p&gt;

&lt;h3&gt;Code Issues&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/qRN9ydy.gif" alt="Code Issues" /&gt;&lt;/p&gt;

&lt;h3&gt;Fix Code Issues&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/xWD8qwn.gif" alt="Fix Code Issues" /&gt;&lt;/p&gt;

&lt;h3&gt;Fix Using Statements&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/7fmRFqm.gif" alt="Fix Usings" /&gt;&lt;/p&gt;

&lt;h3&gt;Code Actions&lt;/h3&gt;

&lt;p&gt;Checkout the key binding for Resharper lovers!&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/16UsVBf.gif" alt="Code Actions" /&gt;&lt;/p&gt;

&lt;h3&gt;Add New C# File&lt;/h3&gt;

&lt;p&gt;There is also add new C# interface and you can add &lt;a href="http://omnisharp-sublime.readthedocs.org/en/latest/filetemplates/"&gt;your own templates&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/0S5T48f.gif" alt="Add new c# file" /&gt;&lt;/p&gt;

&lt;h3&gt;Type Lookup&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/5Wu44i5.gif" alt="Type Lookup" /&gt;&lt;/p&gt;

&lt;h3&gt;Build Solution&lt;/h3&gt;

&lt;p&gt;You can press F4 &amp;amp; Shift+F4 to cycle through the errors reported!&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/g6Adivm.gif" alt="Build" /&gt;&lt;/p&gt;

&lt;h3&gt;Unit Tests&lt;/h3&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/gSuTami.gif" alt="Unit Tests" /&gt;&lt;/p&gt;

&lt;h3&gt;Add/Remove from Project&lt;/h3&gt;

&lt;p&gt;If you create an empty chsarp file or paste a file into the folder, if you open it up and simple save the file it will be added to the &lt;code&gt;*csproj&lt;/code&gt; file. You can also right click a file and choose &lt;code&gt;Remove from Project&lt;/code&gt; and this will remove the file from a &lt;code&gt;*.csproj&lt;/code&gt; file.  &lt;/p&gt;

&lt;h2&gt;OSS FTW!&lt;/h2&gt;

&lt;p&gt;Hopefully that's given you a taste of what the plugin can do and I hope you think its as cool as I do.  I believe that this will give users another option when deciding what editor to use when editing C# files.  A big thanks to &lt;a href="http://twitter.com/jasonimison"&gt;Jason Imison&lt;/a&gt; for OmniSharpServer which allows the plugin to provide all the code options and for him putting up with my questions.&lt;/p&gt;

&lt;p&gt;If you're interested please &lt;a href="https://sublime.wbond.net/packages/OmniSharp"&gt;download&lt;/a&gt; the plugin and enjoy it.  Please let me know of any issues or features you think need adding, I already have plenty in my head but I'm sure there are more additions we can add to make the plugin better and better.  Some documentation can be found &lt;a href="http://omnisharp-sublime.readthedocs.org/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also thanks to the guys who originally came up with the idea of the plugin and to the other contributors who have already submitted pull requests.&lt;/p&gt;

&lt;h2&gt;Microsoft &amp;amp; OSS&lt;/h2&gt;

&lt;p&gt;This blog post and project has been sitting on my file system for probably 4-5 weeks ready to be published but during that time I've had chance to make new friends and add new features which I've shown you above! &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Question: Why did I wait 4-5 weeks to publish this post?&lt;br /&gt;
  Answer  : Microsoft!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since my last &lt;a href="http://blog.jonathanchannon.com/2014/08/05/nancy-aspnetvnext-osx-sublime-text/"&gt;blog post&lt;/a&gt; I'd been in touch with the authors of the Kulture plugin (&lt;a href="http://blog.jonathanchannon.com/18"&gt;Sayed Ibrahim Hashimi&lt;/a&gt;) who just happened to work for Microsoft.  They liked my &lt;a href="http://www.nancyfx.org"&gt;Nancy&lt;/a&gt; yeoman generator and I managed to convince them to merge my generator with their ASP.Net generator.  I asked a few questions here and there and &lt;a href="http://twitter.com/jasonimison"&gt;Jason Imison&lt;/a&gt; came onboard, then &lt;a href="http://twitter.com/mpdreamz"&gt;Martijn Laarman&lt;/a&gt; showed some interest in getting intellisense working and soon we started growing something.  &lt;a href="http://twitter.com/davidfowl"&gt;David Fowler&lt;/a&gt; was brought in to answer some questions and we started having Skype chats with &lt;a href="http://twitter.com/shanselman"&gt;Scott Hanselman&lt;/a&gt; and we then decided upon the path we were going to take.  OmniSharp was born!&lt;/p&gt;

&lt;h2&gt;OmniSharp&lt;/h2&gt;

&lt;p&gt;Our path was going to be make C# available on all the popular editors.  Soon we had &lt;a href="http://twitter.com/stephenhjames"&gt;Stephen James&lt;/a&gt; and &lt;a href="http://twitter.com/mpdreamz"&gt;Martijn Laarman&lt;/a&gt; working on making OmniSharp work with &lt;a href="https://github.com/OmniSharp/omnisharp-atom"&gt;Atom&lt;/a&gt;, &lt;a href="http://twitter.com/jasonimison"&gt;Jason Imison&lt;/a&gt; working with &lt;a href="https://github.com/OmniSharp/omnisharp-vim"&gt;Vim&lt;/a&gt;, &lt;a href="http://twitter.com/bbbscarter"&gt;Simon Carter&lt;/a&gt;, &lt;a href="http://twitter.com/sp3ctum"&gt;Mika Vilpas&lt;/a&gt; &amp;amp; &lt;a href="http://twitter.com/jasonimison"&gt;Jason Imison&lt;/a&gt; on &lt;a href="https://github.com/OmniSharp/omnisharp-emacs"&gt;Emacs&lt;/a&gt;, &lt;a href="http://twitter.com/mat_mcloughlin"&gt;Mat McLoughlin&lt;/a&gt; on &lt;a href="https://github.com/OmniSharp/omnisharp-brackets"&gt;Brackets&lt;/a&gt; and me on &lt;a href="https://github.com/OmniSharp/omnisharp-sublime"&gt;Sublime&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We then moved all our repositories to an &lt;a href="https://github.com/OmniSharp/"&gt;OmniSharp Github organization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We built a &lt;a href="http://www.omnisharp.net/"&gt;website&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We created a &lt;a href="http://twitter.com/OmniSharp/"&gt;twitter account&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We created a &lt;a href="https://jabbr.net/#/rooms/omnisharp"&gt;Jabbr room&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lets just make this clear, Microsoft were working with us to publicise and demonstrate C# working on all the major editors where we predominantly used them on OSX so Mono was going to be needed and they were happy about this!  &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hang on, why did you have to wait 4-5 weeks to release the blog post?&lt;/em&gt;  &lt;/p&gt;

&lt;p&gt;The reason was that Microsoft were going to have a &lt;strong&gt;big&lt;/strong&gt; announcement to make on November 12th 2014 and it was going to be that the .NET CLR was going to be built as a cross platform and open source tool and that they were going to demonstrate the work we'd been doing on OmniSharp as part of this announcement so we decided after that we'd go public so to speak!  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;WHAT?!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That's right, .NET is going OSS &amp;amp; cross platform! HUGE NEWS and we were working as part of an OSS team with Microsoft to highlight that you can use any editor you like to build .NET applications on any platform.  It appears Leopards can change their spots. Lets hope this new open Microsoft will continue for many years to come!&lt;/p&gt;

&lt;h3&gt;Taking OmniSharp forward&lt;/h3&gt;

&lt;p&gt;Jason has started working with the &lt;a href="https://github.com/aspnet/KRuntime"&gt;Design Time Host&lt;/a&gt; that is used for ASP.NET vNext projects which provides information about vnext assemblies and this will be integrated into OmniSharpServer and there are areas already in OmniSharpSublime that I have highlighted that need to become more vNext focused.  &lt;a href="https://twitter.com/sayedihashimi"&gt;Sayed&lt;/a&gt; is making changes to Kulture such as intellisense in project.json for NuGet packages, maybe our two projects will merge.  NRefactory are looking to use Roslyn for the code analysis engine underneath which effects OmniSharpServer. I'm hoping that OmniSharp will become an OSS success for all of us to gain from and I hope you will want to be apart of it.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So there you have it, C# as a first class citizen in Sublime Text, .Net as a cross platform tool and .Net going open source.  What a day! &lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;p&gt;Scott Hanselman has posted his views on OmniSharp and a cross platform OSS .NET &lt;a href="http://hnsl.mn/dotnet2015"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Mat McLoughlin has posted information on the Brackets plugin &lt;a href="http://mat-mcloughlin.net/2014/11/12/time-to-cast-away-visual-studio-and-use-a-text-editor/"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Martijn Laarman has posted information on the Brackets plugin &lt;a href="http://localghost.io/articles/getting-your-c-sharp-on-with-atom-2014-11-12/"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Leave them wanting more!&lt;/h2&gt;

&lt;p&gt;I've been spiking NuGet support for Sublime and here's a brief intro&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/MB7EH6x.gif" alt="NuGet" /&gt;&lt;/p&gt;

&lt;!-- 
Notes for tooling blog post:

    Nrefactory looking to move to Roslyn
    MS providing DTH access that feeds OmniSharpServer
--&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/08/05/nancy-aspnetvnext-osx-sublime-text/</guid><link>http://blog.jonathanchannon.com/2014/08/05/nancy-aspnetvnext-osx-sublime-text/</link><a10:author><a10:name /></a10:author><category>.net</category><category>community</category><category>nancyfx</category><category>oss</category><category>osx</category><title>Nancy, ASP.Net vNext, OSX and Sublime Text</title><description>&lt;p&gt;One of the great things that ASP.Net vNext is bringing is the ability to use it cross platform with Microsoft actively testing their libraries against &lt;a href="http://www.mono-project.com/Main_Page"&gt;Mono&lt;/a&gt;.  Along with this MS are developing a web server that is cross platform and goes by the name of &lt;a href="https://github.com/aspnet/KestrelHttpServer"&gt;Kestrel&lt;/a&gt;.  One thing they aren't doing, yet, is making Visual Studio cross platform so we need something to write our code in.  There a few editors out there but one of the most common is &lt;a href="http://www.sublimetext.com/3"&gt;Sublime Text&lt;/a&gt;.  This gives you syntax highlighting and build systems that can all be configured so if you are not aware of it check it out.  Obviously before we can start writing code on OSX with our editor we need Mono installed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE - As of August 13th 2014 there is a Mono 3.6 release which means you no longer need to compile Mono but you will need to install Homebrew for ASP.Net vNext. &lt;a href="http://blog.jonathanchannon.com/#vnext"&gt;Skip to nnext section&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At the time of writing the official binary for Mono is 3.4.0 and this does not include some features needed for ASP.Net vNext to run so we are going to have to manually compile Mono ourselves.  Now I know this sounds scary but its not as bad as it seems and I've gone through the pain of setting it up so hopefully this blog post should make it easier for you&lt;/p&gt;

&lt;p&gt;There is a &lt;a href="http://mono-project.com/Compiling_Mono_on_OSX"&gt;guide&lt;/a&gt; on Mono's website on how to compile but I found some issues with it.  I'm running on OSX Mavericks so I'm not sure if that resulted in issues but here's my guide to get it compiling.&lt;/p&gt;

&lt;p&gt;</description><pubDate>Mon, 04 Aug 2014 23:00:00 Z</pubDate><a10:updated>2014-08-04T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;One of the great things that ASP.Net vNext is bringing is the ability to use it cross platform with Microsoft actively testing their libraries against &lt;a href="http://www.mono-project.com/Main_Page"&gt;Mono&lt;/a&gt;.  Along with this MS are developing a web server that is cross platform and goes by the name of &lt;a href="https://github.com/aspnet/KestrelHttpServer"&gt;Kestrel&lt;/a&gt;.  One thing they aren't doing, yet, is making Visual Studio cross platform so we need something to write our code in.  There a few editors out there but one of the most common is &lt;a href="http://www.sublimetext.com/3"&gt;Sublime Text&lt;/a&gt;.  This gives you syntax highlighting and build systems that can all be configured so if you are not aware of it check it out.  Obviously before we can start writing code on OSX with our editor we need Mono installed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE - As of August 13th 2014 there is a Mono 3.6 release which means you no longer need to compile Mono but you will need to install Homebrew for ASP.Net vNext. &lt;a href="http://blog.jonathanchannon.com/#vnext"&gt;Skip to nnext section&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At the time of writing the official binary for Mono is 3.4.0 and this does not include some features needed for ASP.Net vNext to run so we are going to have to manually compile Mono ourselves.  Now I know this sounds scary but its not as bad as it seems and I've gone through the pain of setting it up so hopefully this blog post should make it easier for you&lt;/p&gt;

&lt;p&gt;There is a &lt;a href="http://mono-project.com/Compiling_Mono_on_OSX"&gt;guide&lt;/a&gt; on Mono's website on how to compile but I found some issues with it.  I'm running on OSX Mavericks so I'm not sure if that resulted in issues but here's my guide to get it compiling.&lt;/p&gt;

&lt;p&gt;&lt;!--excerpt--&gt;
Download 3.4.0 from &lt;a href="http://www.go-mono.com/mono-downloads/download.html"&gt;here&lt;/a&gt; and install with the PKG installer because when compiling the latest Mono source it needs a compiler on the machine.&lt;/p&gt;

&lt;p&gt;Install &lt;a href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; and install the dependencies as shown in Mono's guide. The guide discusses compiling these dependencies but it conflicted with something already on the system.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install autoconf
brew install automake
brew install libtool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; You could install Mono using Homebrew if you wish but I didn't realise this until afterwards&lt;/p&gt;

&lt;p&gt;Copy the below and put it in a &lt;code&gt;mymonoinstall.sh&lt;/code&gt; file. Make &lt;strong&gt;$PREFIX&lt;/strong&gt; to be a folder on your system, I used &lt;code&gt;/Users/jonathanchannon/mono&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PATH=$PREFIX/bin:$PATH
git clone https://github.com/mono/mono.git
cd mono
CC='cc -m32' ./autogen.sh --prefix=$PREFIX --disable-nls --build=i386-apple-darwin11.2.0
sudo make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Execute this by typing &lt;code&gt;mymonoinstall.sh&lt;/code&gt; in a terminal under the folder where the file is. This may take some time based on your internet connection and power of your machine but once done you should be able to run &lt;code&gt;mono --version&lt;/code&gt; and hopefully see something greater than 3.4.0.&lt;/p&gt;

&lt;p&gt;&lt;a name="vnext"&gt;&lt;/a&gt; 
Now we have Mono installed we're ready to start coding so download &lt;a href="http://www.sublimetext.com/3"&gt;Sublime Text 3&lt;/a&gt; and install it.  Once installed we need to install the ASP.Net vNext tools using the following instructions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew tap aspnet/k
brew install kvm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a &lt;code&gt;.profile&lt;/code&gt; file in your home directory and enter &lt;code&gt;source kvm.sh&lt;/code&gt;. This is so the system knows what the kvm command is when its typed into the console.&lt;/p&gt;

&lt;p&gt;We now need to install the &lt;a href="https://github.com/ligershark/Kulture"&gt;Kulture&lt;/a&gt; Sublime plugin that will enable us to run ASP.Net vNext from Sublime.  In Sublime open the console &lt;code&gt;View -&amp;gt; Show Console&lt;/code&gt; and paste in the below&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import urllib.request,os,hashlib; h = '7183a2d3e96f11eeadd761d777e62404' + 'e330c659d4bb41d3bdf022e94cab3cd0'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://sublime.wbond.net/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;NOTE: These instructions are for Sublime Text 3. At the time of writing Kulture is only supported on version 3 so if you have version 2 you're out of luck I'm afraid&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This will install a package manager for Sublime, you can never have too many package managers eh!&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Bring up the Command Palette in Sublime (Cmd + Shift + P)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Select Package Control: Install Package&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Select Kulture when the list appears.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;So now we have the ability to run, restore and build our ASP.Net vNext apps in Sublime we're ready to create a Nancy app.  Now you could create 4 files manually and build and run them but we might as well install another package manager that will create a Nancy app for us!  I've made a &lt;a href="http://yeoman.io/"&gt;Yeoman&lt;/a&gt; plugin which &lt;a href="https://www.npmjs.org/package/generator-nancy"&gt;creates a Hello World app&lt;/a&gt; in Nancy for ASP.Net vNext which can be opened in Sublime.&lt;/p&gt;

&lt;p&gt;Go and install &lt;a href="http://nodejs.org/"&gt;Node.js&lt;/a&gt; and then open up a terminal and type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install -g yo
npm install -g generator-nancy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go to a folder in your terminal and type &lt;code&gt;yo nancy&lt;/code&gt;, it will ask you what you want to call your app, you can type something in or accept the default and it will create the files needed to create the app.  It will also do a NuGet restore for all the dependencies the app has.  In Sublime go to &lt;code&gt;File -&amp;gt; Open Folder&lt;/code&gt; and select the folder with your app in.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Go to Tools -&amp;gt; Build System and select ASP.Net&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Type Cmd + B&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;You should see the build output in the Sublime console&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Bring up the Command Palette in Sublime (Cmd + Shift + P)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Type 'K' and select Run K commands&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Select Kestrel and a terminal window will open and you should see 'Started'&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Open a browser and go to http://localhost:5000 and you should see 'Hello World' from your Nancy app&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;There we have it, a Nancy ASP.Net vNext building and running from Sublime Text.  Here's a video to prove it works!&lt;/p&gt;

&lt;iframe width="560" height="315" src="//www.youtube.com/embed/qZDRhNw_TPI" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt; : Put a syntax error in your code and build your app and see the error reported in the Sublime console.  Double click that error line and watch it open the offending file!&lt;/p&gt;

&lt;p&gt;For bonus points you can have a play with &lt;a href="https://github.com/moonrabbit/OmniSharpSublime"&gt;this plugin&lt;/a&gt; that aims to give C# intellisense in Sublime!&lt;/p&gt;

&lt;p&gt;Have fun!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/06/14/nancy-aspnet-vnext-vs2014-azure/</guid><link>http://blog.jonathanchannon.com/2014/06/14/nancy-aspnet-vnext-vs2014-azure/</link><a10:author><a10:name /></a10:author><category>.net</category><category>community</category><category>nancyfx</category><category>oss</category><category>owin</category><title>Nancy, ASP.Net vNext, VS2014 &amp; Azure</title><description>&lt;p&gt;By now we know of Microsoft's plans for the next version of ASP.Net and they've turned it on its head and from the looks of it, its goooooood!&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.hanselman.com/blog/IntroducingASPNETVNext.aspx"&gt;Here&lt;/a&gt; is a blog post from Scott Hanselman introducing ASP.Net vNext. There are introductory and deep dive videos available for your perusal which are also well worth a watch.&lt;/p&gt;

&lt;p&gt;The TL;DR is ASP.Net vNext will take heavy influence from Node.js by using Owin to wire up all the app dependencies and middleware.  It will also remove *.csproj files and use a project.json file similar to Node's package.json and use NuGet to reference the application's dependencies.  It also takes inspiration from Node and Nancy's approach requiring you to opt-in to dependencies rather that traditionally having everything but the kitchen sink.  It also takes influence from Nancy via built in dependency injection and Mono support.  Microsoft announced they will run all their vNext tests against Mono builds ensuring all their code is compatible for cross platform deployments.&lt;/p&gt;

&lt;p&gt;Here's a tweet direct from the horses mouth albeit with a typo .&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/XMmMDce.png" alt="vNext influenced by Node/Nancy" /&gt;&lt;/p&gt;

</description><pubDate>Fri, 13 Jun 2014 23:00:00 Z</pubDate><a10:updated>2014-06-13T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;By now we know of Microsoft's plans for the next version of ASP.Net and they've turned it on its head and from the looks of it, its goooooood!&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.hanselman.com/blog/IntroducingASPNETVNext.aspx"&gt;Here&lt;/a&gt; is a blog post from Scott Hanselman introducing ASP.Net vNext. There are introductory and deep dive videos available for your perusal which are also well worth a watch.&lt;/p&gt;

&lt;p&gt;The TL;DR is ASP.Net vNext will take heavy influence from Node.js by using Owin to wire up all the app dependencies and middleware.  It will also remove *.csproj files and use a project.json file similar to Node's package.json and use NuGet to reference the application's dependencies.  It also takes inspiration from Node and Nancy's approach requiring you to opt-in to dependencies rather that traditionally having everything but the kitchen sink.  It also takes influence from Nancy via built in dependency injection and Mono support.  Microsoft announced they will run all their vNext tests against Mono builds ensuring all their code is compatible for cross platform deployments.&lt;/p&gt;

&lt;p&gt;Here's a tweet direct from the horses mouth albeit with a typo .&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/XMmMDce.png" alt="vNext influenced by Node/Nancy" /&gt;&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;I also believe the guys at Microsoft are looking at developing an unmanaged code HTTP server to run on Mono to produce ridiculous performance which is great news!&lt;/p&gt;

&lt;h3&gt;Azure&lt;/h3&gt;

&lt;p&gt;As you'll see from the videos vNext runs via the command prompt which is fine but obviously Microsoft would want to bake all the NuGet and other tooling features into Visual Studio.  A few weeks after the initial vNext details were made public Microsoft announced a pre-release of Visual Studio which had support for ASP.Net vNext.  They also &lt;a href="http://blogs.msdn.com/b/visualstudioalm/archive/2014/06/04/visual-studio-14-ctp-now-available-in-the-virtual-machine-azure-gallery.aspx"&gt;announced&lt;/a&gt; that Azure had a virtual machine in its gallery that had this version of Visual Studio installed.&lt;/p&gt;

&lt;p&gt;What this meant was that you could setup a VM in Azure, then use Remote Desktop to connect to the virtual machine and run Visual Studio and have a play with all the new shiny things (they also provided the ISO so you could download it and run it locally if you preferred, pretty cool huh!).&lt;/p&gt;

&lt;p&gt;After you follow the &lt;a href="http://blogs.msdn.com/b/visualstudioalm/archive/2014/06/04/visual-studio-14-ctp-now-available-in-the-virtual-machine-azure-gallery.aspx"&gt;above steps&lt;/a&gt; in the blog post announcing the Visual Studio release you'll see in your Azure dashboard that you can connect to the virtual machine. &lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/R6QFSjY.png" alt="Coonect to VM" /&gt;&lt;/p&gt;

&lt;p&gt;Clicking "connect" will download the information about your virtual machine and Remote Desktop can then use it to connect to it.  As a Mac user I was able to use the OSX Microsoft Remote Desktop app and connect to my Windows virtual machine in the cloud.  This was amazing.  Once connected, on your desktop you'll see a shortcut to Visual Studio.  When you click File - New Project you'll see the vNext project types.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/9hINknn.png" alt="New vNext project" /&gt;&lt;/p&gt;

&lt;p&gt;As you can see there are similar options to what we've had in the past.  Creating a vNext Web Application will setup an application that has MVC and Entity Framework all wired up for you.  This is a blog post about Nancy and vNext so we'll choose &lt;code&gt;ASP.NET vNext Empty Web Application&lt;/code&gt;.  The goood news is that choosing this option will actually give is an "empty" application whereas in the past this option still contained 17 references.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/Npw77Ar.png" alt="Empty application" /&gt;&lt;/p&gt;

&lt;p&gt;Microsoft have also &lt;a href="http://www.asp.net/vnext/overview/aspnet-vnext/getting-started-with-aspnet-vnext-and-visual-studio"&gt;produced&lt;/a&gt; a "Getting Started with ASP.NET vNext and Visual Studio 14" guide if you want some more reading material.&lt;/p&gt;

&lt;h3&gt;Nancy and vNext&lt;/h3&gt;

&lt;p&gt;As Nancy supported Owin from the very beginning unlike MVC there was a Nancy.Owin package that you could opt-in to your application and use Nancy in an Owin based application.  This also gave you the option to use Nancy in a non-Owin application.  Today you still have that option but as of June 10th 2014 the Nancy team decided to merge a pull request moving Owin as a separate package into the core code base.  This pull request also added extensions to be able to run Nancy in a ASP.Net vNext project.  You can still run Nancy in a non-Owin application but the decision was made to embrace Owin as a first class citizen.  These changes are currently in the master branch and not yet part of an official release but to save building the source and referencing the project we can use Nancy's &lt;a href="https://www.myget.org/F/nancyfx/"&gt;nightly builds&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;In our &lt;code&gt;ASP.NET vNext Empty Web Application&lt;/code&gt; we can open up the Package Manager Console and run the following commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Install-Package DiscoverPackageSources
Discover-PackageSources -Url "https://www.myget.org/F/nancyfx/"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now open up &lt;code&gt;project.json&lt;/code&gt; and under the dependency node add the below and save the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;"Microsoft.AspNet.Owin": "0.1-alpha-*",
"Nancy": "0.23-Pre1387"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You'll notice in the Solution Explorer under References that these two references have magically appeared.&lt;/p&gt;

&lt;p&gt;Open &lt;code&gt;Startup.cs&lt;/code&gt; and make it look like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;namespace WebApplication3
{
    using Microsoft.AspNet.Builder;
    using Nancy.Owin;

    public class Startup
    {
        public void Configure(IBuilder app)
        {
            app.UseOwin(x =&amp;gt; x.UseNancy());
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a class called HomeModule and make it look like this:&lt;/p&gt;

&lt;p&gt;namespace WebApplication3
{
    using Nancy;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeModule : NancyModule
{
    public HomeModule()
    {
        Get["/"] = _ =&amp;gt; "Hello World from Nancy in ASP.Net vNext";
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;Hit F5 and &lt;strong&gt;&lt;em&gt;*boom&lt;/em&gt;*&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/dkf2HF0.png" alt="Nancy running on vNext" /&gt;&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;ASP.Net vNext has some great changes in terms of solution architecture and developer productivity (see editing a file, pressing save, hitting F5 and the changes appearing not needing a re-compile).  Microsoft's embracing of Mono I hope will lead to a true cross platform development stack we can all work on.  ASP.Net vNext has been influenced by other major players in the OSS world and Microsoft has realised there are some great features that the community have provided for a long time now so Node, Go and Nancy should be flattered.  Its also great to see vNext on Github where poeple can see clearly the commits, issues and pull requests open for ASP.Net.  My personal belief and not an official Nancy statement is that Nancy still offers many great features that MVC and WebAPI (or I should call that MVC6 now that they've merged) doesn't but I think what Microsoft has produced in terms of vNext is awesome work and the features of MVC and new middleware can only add to the options available for .Net developers to choose from and having this choice is what its all about.  Good work Microsoft!&lt;/p&gt;

&lt;h3&gt;One more thing!&lt;/h3&gt;

&lt;p&gt;The current release of Nancy is 0.23 and Nancy has been stable for a long time and is used in production for many users yet there are numerous requests for when Nancy will release a v1.0.  The response is usually "why will renaming 0.23 to v1.0 make any difference to the functionality within Nancy" mainly because its felt this is some psychological issue people have regarding version numbers.  I could do File - New Project and make it v1.0 and it could be completely unstable but does that give the perception that its more feature rich than something that is 0.23?  I think its an interesting topic how people perceive version  numbers but that discussion is for another day.  As of June 10th 2014 the Nancy team agreed that the next version of Nancy will not be 0.24 but in fact be v1.0.0.  There are more finer details about the v1.0 release which &lt;a href="http://twitter.com/thecodejunkie"&gt;TheCodeJunkie&lt;/a&gt; will be blogging about soon but hopefully this release will attract more people to use Nancy that may have been put off by the version number in the past.&lt;/p&gt;

&lt;p&gt;The future of .Net development is looking good. Go out and code up a storm =)&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/06/06/modifying-bash-adding-git-auto-completion-to-terminal/</guid><link>http://blog.jonathanchannon.com/2014/06/06/modifying-bash-adding-git-auto-completion-to-terminal/</link><a10:author><a10:name /></a10:author><category>bash</category><category>git</category><category>osx</category><category>terminal</category><title>Modifying the bash prompt and adding Git completion to terminal</title><description>&lt;p&gt;At work we use Git and I use &lt;a href="http://sourceforge.net/projects/console/files/"&gt;Console2&lt;/a&gt; to control my terminal envrionments eg/Git Bash, Powershell, Dos and when using Git I can type part type a git command press tab and it will auto complete the command or offer suggestions to commands.  By default on OSX this behaviour is not present and it frustrated me enough to go and find out how to enable that behaviour.&lt;/p&gt;

&lt;p&gt;Fire up your terminal and type in this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl https://raw.githubusercontent.com/git/git/master/contrib/completion/git-completion.bash -o ~/.git-completion.bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will download a git completion file into your home folder to a hidden file called git-completion.bash.&lt;/p&gt;

&lt;p&gt;If the file ~/.bash_profile does not already exist create it with the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now open it and paste this in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if [ -f ~/.git-completion.bash ]; then . ~/.git-completion.bash; fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if you type a git command and press tab, BOOM!, you have auto complete for Git!&lt;/p&gt;

</description><pubDate>Thu, 05 Jun 2014 23:00:00 Z</pubDate><a10:updated>2014-06-05T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;At work we use Git and I use &lt;a href="http://sourceforge.net/projects/console/files/"&gt;Console2&lt;/a&gt; to control my terminal envrionments eg/Git Bash, Powershell, Dos and when using Git I can type part type a git command press tab and it will auto complete the command or offer suggestions to commands.  By default on OSX this behaviour is not present and it frustrated me enough to go and find out how to enable that behaviour.&lt;/p&gt;

&lt;p&gt;Fire up your terminal and type in this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl https://raw.githubusercontent.com/git/git/master/contrib/completion/git-completion.bash -o ~/.git-completion.bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will download a git completion file into your home folder to a hidden file called git-completion.bash.&lt;/p&gt;

&lt;p&gt;If the file ~/.bash_profile does not already exist create it with the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now open it and paste this in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if [ -f ~/.git-completion.bash ]; then . ~/.git-completion.bash; fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if you type a git command and press tab, BOOM!, you have auto complete for Git!&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Also by default if you're in a directory that has a git repository inside it will not show you the branch you're on.  Again in Console2 in Windows it does however we can get the same functionality.&lt;/p&gt;

&lt;p&gt;Open the .bash_profile and paste the below in, save and reopen your terminal and bingo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Git branch in prompt.
parse_git_branch() {
    git branch 2&amp;gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/ (\1)/'
}
export PS1="\u@\h \W\[\033[32m\]\$(parse_git_branch)\[\033[00m\] $ "
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A few days later whilst I was using Console2 I wanted to have the date/time in my prompt so I know when I last used the git command.  If you look at the code above you'll see the PS1 variable which controls what you prompt will look like.  The variables in there eg\ &lt;code&gt;\W&lt;/code&gt; all mean different things. Here's a &lt;a href="http://www.cyberciti.biz/tips/howto-linux-unix-bash-shell-setup-prompt.html"&gt;link&lt;/a&gt; to explain them.&lt;/p&gt;

&lt;p&gt;So in Console to I executed &lt;code&gt;echo $PS1&lt;/code&gt; which showed me my current settings. I used the above link to help me and I added the datetime with a color. &lt;/p&gt;

&lt;p&gt;Before :  &lt;code&gt;\[\033]0;$MSYSTEM:${PWD//[^[:ascii:]]/?}\007\]\n\[\033[32m\]\u@\h\[\033[33m\]\w$(__git_ps1)\[\033[0m\]\n\ \[\033[0m\]$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After :  &lt;code&gt;\[\033]0;$MSYSTEM:${PWD//[^[:ascii:]]/?}\007\]\n\[\033[32m\]\u@\h\[\033[33m\]\w$(__git_ps1)\[\033[0m\]\n\[\033[31m\]\t \[\033[0m\]$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now if this is all a bit much for you I was told about &lt;a href="http://www.iterm2.com/#/section/home"&gt;iTerm2&lt;/a&gt;, a OSX terminal replacement and &lt;a href="https://github.com/robbyrussell/oh-my-zsh"&gt;Oh My Zsh&lt;/a&gt; a way to configure your terminal that provides lots of plugins and themes.&lt;/p&gt;

&lt;p&gt;Hope this is helpful for someone.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/05/07/introducing-owin-statelessauth-with-nancy-angular-demo/</guid><link>http://blog.jonathanchannon.com/2014/05/07/introducing-owin-statelessauth-with-nancy-angular-demo/</link><a10:author><a10:name /></a10:author><category>.net</category><category>angularjs</category><category>nancyfx</category><category>oss</category><category>owin</category><title>Introducing Owin.StatelessAuth with Nancy/Angular demo</title><description>&lt;p&gt;If you're writing an API, current thinking is to provide a token in the &lt;code&gt;Authorization&lt;/code&gt; header for your app to validate when the request comes in.  I have used the &lt;a href="http://www.nuget.org/packages/Nancy.Authentication.Stateless/"&gt;Nancy.Authentication.Stateless&lt;/a&gt; package in the past for my APIs and even have a demo of it &lt;a href="https://github.com/jchannon/Nancy.Demo.StatelessAuth"&gt;here&lt;/a&gt; if you're interested (there are more Nancy demos at &lt;a href="http://samples.nancyfx.org/"&gt;http://samples.nancyfx.org&lt;/a&gt;). This is a great package and does a great job but what if one day you want to use &lt;a href="http://www.asp.net/signalr"&gt;SignalR&lt;/a&gt; v2 that uses &lt;a href="http://owin.org/"&gt;OWIN&lt;/a&gt; and you want to validate not just requests to your Nancy app but also the SignalR requests?  You're going to need to validate requests as they come in before they get to SignalR or Nancy.&lt;/p&gt;

&lt;p&gt;For those of you who are not quite up to date or unsure what OWIN is let me try and give you the tl:dr, no doubt others may say its something slightly different.  Imagine you are asked to create a ASP.Net MVC 3 app (ignore the fact that that person needs a slap) so you fire up Visual Studio and create the app.  So what has it done? Its created an app that runs on IIS and all requests come straight into your app.   &lt;/p&gt;

</description><pubDate>Tue, 06 May 2014 23:00:00 Z</pubDate><a10:updated>2014-05-06T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;If you're writing an API, current thinking is to provide a token in the &lt;code&gt;Authorization&lt;/code&gt; header for your app to validate when the request comes in.  I have used the &lt;a href="http://www.nuget.org/packages/Nancy.Authentication.Stateless/"&gt;Nancy.Authentication.Stateless&lt;/a&gt; package in the past for my APIs and even have a demo of it &lt;a href="https://github.com/jchannon/Nancy.Demo.StatelessAuth"&gt;here&lt;/a&gt; if you're interested (there are more Nancy demos at &lt;a href="http://samples.nancyfx.org/"&gt;http://samples.nancyfx.org&lt;/a&gt;). This is a great package and does a great job but what if one day you want to use &lt;a href="http://www.asp.net/signalr"&gt;SignalR&lt;/a&gt; v2 that uses &lt;a href="http://owin.org/"&gt;OWIN&lt;/a&gt; and you want to validate not just requests to your Nancy app but also the SignalR requests?  You're going to need to validate requests as they come in before they get to SignalR or Nancy.&lt;/p&gt;

&lt;p&gt;For those of you who are not quite up to date or unsure what OWIN is let me try and give you the tl:dr, no doubt others may say its something slightly different.  Imagine you are asked to create a ASP.Net MVC 3 app (ignore the fact that that person needs a slap) so you fire up Visual Studio and create the app.  So what has it done? Its created an app that runs on IIS and all requests come straight into your app.   &lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h3&gt;Enter OWIN&lt;/h3&gt;

&lt;p&gt;What OWIN introduces is an HTTP abstraction from the host to framework and therefore you have access to the whole request at any point. As an example, &lt;a href="https://github.com/bbaia/connect-owin-samples/tree/master/Samples.Nancy"&gt;here&lt;/a&gt; is a node.js web server (replacing IIS) and then calling out to Nancy.  Pretty cool huh!  As HTTP is abstracted you can have two applications, one in Nancy and one in WebAPI in the same project and via OWIN you can tell it which requests go to Nancy and which go to WebAPI.&lt;/p&gt;

&lt;h3&gt;Authentication&lt;/h3&gt;

&lt;p&gt;Due to the HTTP abstraction we can now inspect the requests and then determine whether we should return a 401 or let the request continue. So how does that look?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Startup
{
    public void Configuration(IAppBuilder app)
    {
         app
           .RequiresStatelessAuth(new MySecureTokenValidator())
           .MapSignalR() //This could be WebAPI etc
           .UseNancy();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In an OWIN app we need a Startup class to configure our application and we wire up the requests and how they may be handled in order of processing.  So as I stated earlier we want to use SignalR and Nancy and validate the requests before they hit our application, using &lt;a href="https://www.nuget.org/packages/Owin.StatelessAuth/"&gt;Owin.StatelessAuth&lt;/a&gt; we can do that.  It takes an implementation of &lt;code&gt;ITokenValidator&lt;/code&gt; where a method gets called to determine if the request is valid by passing in a token from the &lt;code&gt;Authorization&lt;/code&gt; header.  How you implement the interface and determine what is a valid request is up to you.  Luckily I have a demo available in the &lt;a href="https://github.com/jchannon/Owin.StatelessAuth"&gt;Github repository&lt;/a&gt; which I'll now explain.&lt;/p&gt;

&lt;h3&gt;Demo Time&lt;/h3&gt;

&lt;p&gt;About 2 days after publishing &lt;a href="https://github.com/jchannon/Owin.StatelessAuth"&gt;Owin.StatelessAuth&lt;/a&gt;, Mike Hadlow published a great &lt;a href="http://mikehadlow.blogspot.co.uk/2014/04/json-web-tokens-owin-and-angularjs.html"&gt;blog post&lt;/a&gt; on using JWT (JavaScript Web Tokens) &amp;amp; OWIN &amp;amp; Angular so I thought I would do a similar post just to throw my 2 cents in.  Its going to be hard not to say the same things as Mike so I may skip some stuff but it just means you should read his post too!  So lets get the code to do the talking...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Startup.cs&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Startup
{
    public void Configuration(IAppBuilder app)
    {
        app.RequiresStatelessAuth(
              new MySecureTokenValidator(new ConfigProvider()), 
              new StatelessAuthOptions() {IgnorePaths = new List&amp;lt;string&amp;gt;(new []{"/login","/content"})})
            .UseNancy();

    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we pass in our implementation of ITokenValidator called MySecureTokenValidator and pass in some options to Owin.StatelessAuth which says if the paths contain the items in the list then Owin.StatelessAuth will not try and authenticate those requests.  In the demo we have javascript and images in the content folder so we don't want to authenticate those requests.  We also don't want to authenticate requests to the login path.  Why not? This is the route that will give us the token for all subsequent requests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nancy Module&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public HomeModule(IConfigProvider configProvider, IJwtWrapper jwtWrapper)
{
    Get["/login"] = _ =&amp;gt; View["Login"];

    Post["/login"] = _ =&amp;gt;
    {
        var user = this.Bind&amp;lt;UserCredentials&amp;gt;();

        //Verify user/pass
        if (user.User != "fred" &amp;amp;&amp;amp; user.Password != "securepwd")
        {
            return 401;
        }

        var jwttoken = new JwtToken()
        {
            Issuer = "http://issuer.com",
            Audience = "http://mycoolwebsite.com",
            Claims =
                new List&amp;lt;Claim&amp;gt;(new[]
                {
                    new Claim(ClaimTypes.Role, "Administrator"),
                    new Claim(ClaimTypes.Name, "Fred")
                }),
            Expiry = DateTime.UtcNow.AddDays(7)
        };

        var token = jwtWrapper.Encode(jwttoken, configProvider.GetAppSetting("securekey"), JwtHashAlgorithm.HS256);
        return Negotiate.WithModel(token);
    };

    Get["/"] = _ =&amp;gt; "Hello Secure World!";
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here on the GET request to login we return a view where Angular wil be used.  On the POST request to login we Bind the posted values to a class called UserCredentials, we then need to validate these credentials (I assume yours will be better than mine) and then create a new instance of JwtToken which is just another class in our application which has properties that relate to the &lt;a href="http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html"&gt;JWT spec&lt;/a&gt; and then we encode the object to return a token for our user using the &lt;a href="https://www.nuget.org/packages/JWT/"&gt;JWT&lt;/a&gt; library (I have created a wrapper for it in the demo as they are static methods out of the box).  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Angular View&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here's the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en" xmlns="http://www.w3.org/1999/xhtml" ng-app="owinstatelessauthexample"&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;meta charset="utf-8" /&amp;gt;
    &amp;lt;title&amp;gt;&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body ng-controller="appCtrl"&amp;gt;
    &amp;lt;h1&amp;gt;Login&amp;lt;/h1&amp;gt;
    &amp;lt;form ng-submit="getToken()"&amp;gt;
        &amp;lt;input type="text" name="user" ng-model="user" /&amp;gt;
        &amp;lt;input type="password" name="password" ng-model="password" /&amp;gt;
        &amp;lt;input type="submit" value="Login" /&amp;gt;
    &amp;lt;/form&amp;gt;
    &amp;lt;label&amp;gt;Status: {{loggedinstatus}}&amp;lt;/label&amp;gt;
    &amp;lt;span&amp;gt;{{secureresponse}}&amp;lt;/span&amp;gt;
    &amp;lt;button ng-click="getsecureresponse()"&amp;gt;Get Secure Response&amp;lt;/button&amp;gt;

    &amp;lt;script src="/content/localforage.js"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.2.16/angular.min.js"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src="/content/angular-localforage.js"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src="/Content/app.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have a form that will POST to our login route, a label to show our logged in status, a button to hit our route that should return "Hello Secure World"&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Angular Code&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(function () {
    'use strict';

    var app = angular.module('owinstatelessauthexample', ['LocalForageModule'])
        .controller('appCtrl', ['$scope', '$localForage', '$http', function ($scope, $localForage, $http) {
            // Start fresh
            $localForage.clearAll();

            $scope.user = 'fred';
            $scope.password = 'securepwd';
            $scope.secureresponse = '';
            $scope.loggedinstatus = 'Not Logged In';

            $scope.getToken = function () {
                $http({
                    method: 'POST',
                    url: '/login',
                    data: {
                        "user": $scope.user,
                        "password": $scope.password,
                    }
                })
                    .success(function (data, status) {
                        console.log('All ok : ' + data);
                        $localForage.setItem('mysecuretoken', JSON.parse(data));
                        $scope.loggedinstatus = 'Logged In';
                    })
                    .error(function (data, status) {
                        console.log('Oops : ' + data);
                    });

            };

            $scope.getsecureresponse = function () {
                $localForage.get('mysecuretoken').then(function (data) {
                    $http({
                        method: 'GET',
                        url: '/',
                        headers: { 'Authorization': data }

                    })
                   .success(function (data, status) {
                       console.log('All secure : ' + data);
                       $scope.secureresponse = data;
                   })
                   .error(function (data, status) {
                       console.log('Oops : ' + data);
                       $scope.secureresponse = "Oops!" + data;
                   });
                });

            };
        }]);

})();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the form from our view is posted to our login route we get take the response data and store it in localStorage.  However, here we are using a library called &lt;a href="https://github.com/mozilla/localForage"&gt;localForage&lt;/a&gt; which has a fallback option if you don't have HTML5 in your browser.  When the user clicks the button to hit our secure route it will retrieve the token from localForage and pass it in the request and hopefully we get the expected response as Owin.StatelessAuth will validate it via MySecureTokenValidator.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MySecureTokenValidator&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MySecureTokenValidator : ITokenValidator
{
    private readonly IConfigProvider configProvider;

    public MySecureTokenValidator(IConfigProvider configProvider)
    {
        this.configProvider = configProvider;
    }

    public ClaimsPrincipal ValidateUser(string token)
    {
        try
        {
            //Claims don't deserialize :(
            //var jwttoken = JsonWebToken.DecodeToObject&amp;lt;JwtToken&amp;gt;(token, configProvider.GetAppSetting("securekey"));

            var decodedtoken = JsonWebToken.DecodeToObject(token, configProvider.GetAppSetting("securekey")) as Dictionary&amp;lt;string, object&amp;gt;;

            var jwttoken = new JwtToken()
            {
                Audience = (string)decodedtoken["Audience"],
                Issuer = (string)decodedtoken["Issuer"],
                Expiry = DateTime.Parse(decodedtoken["Expiry"].ToString()),
            };

            if (decodedtoken.ContainsKey("Claims"))
            {
                var claims = new List&amp;lt;Claim&amp;gt;();

                for (int i = 0; i &amp;lt; ((ArrayList)decodedtoken["Claims"]).Count; i++)
                {
                    var type = ((Dictionary&amp;lt;string, object&amp;gt;)((ArrayList)decodedtoken["Claims"])[i])["Type"].ToString();
                    var value = ((Dictionary&amp;lt;string, object&amp;gt;)((ArrayList)decodedtoken["Claims"])[i])["Value"].ToString();
                    claims.Add(new Claim(type, value));
                }

                jwttoken.Claims = claims;
            }

            if (jwttoken.Expiry &amp;lt; DateTime.UtcNow)
            {
                return null;
            }

            return new ClaimsPrincipal(new ClaimsIdentity(jwttoken.Claims, "Token"));
        }
        catch (SignatureVerificationException)
        {
            return null;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My first comment in the code is that the class &lt;a href="http://msdn.microsoft.com/en-us/library/system.identitymodel.claims.claim(v=vs.110).aspx"&gt;Claim&lt;/a&gt; won't deserialize which would have made our code a one liner but unfortunately not. Possibly if the JWT library used JSON.Net or ServiceStack.Text it may work but for now I had to do some logic to assign the properties of the JwtToken class.  It really is some ugly code so hopefully a PR or so to JWT it may be cleaned up.  So we decode the token to a dictionary and then assign the values to our class, loop over the claims, see if the expiry date is before now and if so return null which will cause Owin.StatelessAuth to return a 401.  If all is well we return a &lt;a href="http://msdn.microsoft.com/en-GB/library/system.security.claims.claimsprincipal.aspx"&gt;ClaimsPrincipal&lt;/a&gt; instance.  Owin.StatelessAuth will add it to the Owin environment which can be read further down the request stack.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nancy Bootstrapper&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Bootstrapper : DefaultNancyBootstrapper
{
    protected override void RequestStartup(TinyIoCContainer container, IPipelines pipelines, NancyContext context)
    {
        base.RequestStartup(container, pipelines, context);
        var owinEnvironment = context.GetOwinEnvironment();
        var user = owinEnvironment["server.User"] as ClaimsPrincipal;
        if (user != null)
        {
            context.CurrentUser = new DemoUserIdentity()
            {
                UserName = user.Identity.Name,
                Claims = user.Claims.Where(x =&amp;gt; x.Type == "http://schemas.microsoft.com/ws/2008/06/identity/claims/role").Select(x =&amp;gt; x.Value)
            };
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nancy has a CurrentUser property on the NancyContext, if this is not null then we know the user is authenticated.  In the introduction of the blog post I mentioned Nancy.Authentication.Stateless (other Nancy.Authentication libraries are available) which does exactly that, it assigns the the CurrentUser to the validated user.  In our Bootstrapper we use the ClaimsPrincipal instance in the Owin environment that Owin.StatelessAuth put in there for us to assign the properties of &lt;code&gt;IUserIdentity&lt;/code&gt; in Nancy to assign the current user.  We can then use &lt;code&gt;RequiresAuthentication&lt;/code&gt; on our Nancy routes to secure routes based on extra security such as claim types.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;What we have now is a way using Owin.StatelessAuth to secure all incoming requests, the option to ignore some requests for authentication, a way for tokens to be issued, a way for them to be validated and the ability to assign Nancy's user to the user we validated using Owin.StatelessAuth.&lt;/p&gt;

&lt;p&gt;I enjoyed writing Owin.StatelessAuth middleware component and the demo with it so please take a look, any constructive feedback welcomed along with pull requests :)&lt;/p&gt;

&lt;p&gt;Finally just to prove this works, here's some pretty pictures:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;POST to generate a token&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/FlI4NAi.png" alt="Post Get Token" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GET with our token&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/GeCP9IJ.png" alt="Get Secure Request" /&gt;&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/03/22/using-node-and-ftp-with-promises/</guid><link>http://blog.jonathanchannon.com/2014/03/22/using-node-and-ftp-with-promises/</link><a10:author><a10:name /></a10:author><category>javascript</category><category>nodejs</category><category>promises</category><title>Using NodeJS and FTP with Promises</title><description>&lt;p&gt;I've played with node in the &lt;a href="http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/"&gt;past&lt;/a&gt; but as of the new year I decided to try and make a more concerted effort to get stuck into node properly.  I decided to go back to the beginning to try and get a better appreciation for the language so read "JavaScript: The Good Parts by Douglas Crockford".  I found that exercise fulfilling and resulted in a few light bulb moments that made some dots join up so I'd recommend reading it if you haven't already.&lt;/p&gt;

&lt;h3&gt;Real World App&lt;/h3&gt;

&lt;p&gt;As I stated earlier I have already played with node in the past using &lt;a href="http://expressjs.com/"&gt;Express&lt;/a&gt; and have read quite a bit on node and read many examples but I wanted to write a non-web app as I felt this would give me a better opportunity to get to grips with the language and Node. Using Express allows you to get up and running very quickly without to much head scratching so I felt a standalone script would give me more exposure to things.&lt;/p&gt;

</description><pubDate>Sat, 22 Mar 2014 00:00:00 Z</pubDate><a10:updated>2014-03-22T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I've played with node in the &lt;a href="http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/"&gt;past&lt;/a&gt; but as of the new year I decided to try and make a more concerted effort to get stuck into node properly.  I decided to go back to the beginning to try and get a better appreciation for the language so read "JavaScript: The Good Parts by Douglas Crockford".  I found that exercise fulfilling and resulted in a few light bulb moments that made some dots join up so I'd recommend reading it if you haven't already.&lt;/p&gt;

&lt;h3&gt;Real World App&lt;/h3&gt;

&lt;p&gt;As I stated earlier I have already played with node in the past using &lt;a href="http://expressjs.com/"&gt;Express&lt;/a&gt; and have read quite a bit on node and read many examples but I wanted to write a non-web app as I felt this would give me a better opportunity to get to grips with the language and Node. Using Express allows you to get up and running very quickly without to much head scratching so I felt a standalone script would give me more exposure to things.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;During the previous couple of weeks at work I wrote a console app that downloaded zip file from a FTP server, extract the contents, read data in a XML file that was in the zip, do some string matching and upload the zip to another FTP server.  I figured this would be a good app to replicate in node so off I went.&lt;/p&gt;

&lt;p&gt;After a bit of &lt;a href="http://npmjs.org"&gt;npm&lt;/a&gt; research I found the modules I needed and managed to get to the point of downloading files pretty easily with the below code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var path = require('path');
var fs = require('fs');
var Promise = require('bluebird');
var Client = require('ftp');

var c = new Client();

var connectionProperties = {
    host: "myhost",
    user: "myuser",
    password: "mypwd"
};

c.on('ready', function () {
    console.log('ready');
    c.list(function (err, list) {
        if (err) throw err;
        list.forEach(function (element, index, array) {
            //Ignore directories
            if (element.type === 'd') {
                console.log('ignoring directory ' + element.name);
                return;
            }
            //Ignore non zips
            if (path.extname(element.name) !== '.zip') {
                console.log('ignoring file ' + element.name);
                return;
            }
            //Download files
            c.get(element.name, function (err, stream) {
                if (err) throw err;
                stream.once('close', function () {
                    c.end();
                });
                stream.pipe(fs.createWriteStream(element.name));
            });
        });
    });
});

c.connect(connectionProperties);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, I originally had that code in a function and wanted to call it and then call another function to read the files that I had downloaded but what I found was callback hell.&lt;/p&gt;

&lt;h3&gt;Enter Promises&lt;/h3&gt;

&lt;p&gt;I needed to know that all the files had downloaded and then I could read the files in a directory ready for zip extraction but I couldn't work out how.  I discovered promises and probably didn't read enough about all the ins and outs of them but I remember &lt;a href="http://twitter.com/gblock"&gt;Glenn Block&lt;/a&gt; giving a talk about &lt;a href="https://github.com/glennblock/codemash-async"&gt;async programming in node&lt;/a&gt; so I pestered him on Twitter and he kindly helped and me out and also pointed me towards his code and slides where I decided to use &lt;a href="https://github.com/petkaantonov/bluebird/"&gt;Bluebird&lt;/a&gt;, the promise library.  Unfortunately I just couldn't get the files downloaded. It would download one file but not the other and closed the streams.&lt;/p&gt;

&lt;p&gt;Here is a snippet of what I had (brace yourself)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var processListing = function (directoryItems) {
    var itemsToDownload = [];
    directoryItems.forEach(function (element, index, array) {
        //Ignore directories
        if (element.type === 'd') {
            console.log('directory ' + element.name);
            return;
        }
        //Ignore non zips
        if (path.extname(element.name) !== '.zip') {
            console.log('ignoring ' + element.name);
            return;
        }
        //Download zip
        itemsToDownload.push({
            source: element.name,
            destination: element.name
        });
    });
    return itemsToDownload;
};

var processItem = function (object) {
    return aFtpClient.getAsync(object.source);
};

var downloadFiles = function () {
    console.log('downloading files');
    aFtpClient.
    listAsync().
    then(processListing).
    map(function (object) {
        return processItem(object).then(function (processResult) {
            return {
                input: object,
                result: processResult
            };
        });
    }).
    map(function (downloadItem) {
        downloadItem.result.pipe(fs.createWriteStream(process.cwd() + "/zips/" + downloadItem.input.destination));
        return new Promise(function (resolve, reject) {
            downloadItem.result.once("close", function () {
                console.log('closed');
                resolve();
            });
        });
    }).done()
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not only is that a tad complicated but I could not for the life of me understand what the hell was happening and why it wasn't downloading all the files.  I reached out to &lt;a href="https://twitter.com/PrabirShrestha"&gt;@PrabirShrestha&lt;/a&gt; who agreed it was a tad over complicated and tried to help but recommended I take a look at Reactive Extensions, maybe I will in the future but at this point my frustration had kicked in and I wanted to give up.  I went through a mixture of emotions from frustration, which led to anger, fuming anger, denial, then apathy.  Although these emotions went by and after a couple of questions on stackoverflow that helped but didn't give the solution I explained the issue to a colleague and we both took a look.  I went through a few iterations with no luck and after a bit more reading I think we were closing in on it individually but I was beaten to it. All hail &lt;a href="http://twitter.com/iamnerdfury"&gt;@iamnerdfury&lt;/a&gt; who produced this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var connect = function() {
    c.connect(connectionProperties);
    return c.onAsync('ready');
};

var getList = function() {
    return c.listAsync();
};

var zipFiles = function(element) {
    return element.type !== 'd' &amp;amp;&amp;amp; path.extname(element.name) === '.zip';
};

var current = Promise.resolve();

var downloadFiles = function(file) {
    current = current.then(function() {
        return c.getAsync(file.name)
    }).then(function(stream) {
        stream.pipe(fs.createWriteStream(file.name));
        console.log(file.name + ' downloaded..');
    });
    return current;
};

connect().then(getList).filter(zipFiles).map(downloadFiles).done();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think the previous issues I had was I was returning &lt;code&gt;resolve()&lt;/code&gt; after the first file downloaded which is not what you want to do when multiple calls to it are executed as a promise can only resolve once.  I needed to find some way of concatenating a promise somehow for each file that is downloaded. I looked at the &lt;code&gt;all()&lt;/code&gt; command but I couldn't get it to fit but @iamnerdfury found that you could do this via creating an instance of a promise by calling resolve and then assign to it on each file that needed to be downloaded.&lt;/p&gt;

&lt;p&gt;Now I know the files are downloaded I can chain more functions to read the file system, extract the zip for each one, read the XML and upload to a new server.&lt;/p&gt;

&lt;p&gt;I hope this helps someone else because it wound me up something chronic and whilst I get pissed off with JavaScript when things like this happen I will keep at it because I think node is now becoming a serious contender and us developers need to keep a finger in many pies.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(If you think there is a way to improve the solution above I'd love to hear it)&lt;/em&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2014/01/02/unit-testing-with-sqlexception/</guid><link>http://blog.jonathanchannon.com/2014/01/02/unit-testing-with-sqlexception/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>unit testing</category><title>Unit Testing with SqlException</title><description>&lt;p&gt;So after a nice Christmas break I get to some code that needs some unit testing around a try/catch. Something similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;try
{
    myService.DoSomethingThatMightTakeALongTime();
}
catch (EntityCommandExecutionException ex)
{
    var exception = ex.InnerException as SqlException;
    if (exception != null)
    {
        if (exception.Number == -2)
        {
            //Do something special
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Thu, 02 Jan 2014 00:00:00 Z</pubDate><a10:updated>2014-01-02T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;So after a nice Christmas break I get to some code that needs some unit testing around a try/catch. Something similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;try
{
    myService.DoSomethingThatMightTakeALongTime();
}
catch (EntityCommandExecutionException ex)
{
    var exception = ex.InnerException as SqlException;
    if (exception != null)
    {
        if (exception.Number == -2)
        {
            //Do something special
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Obviously &lt;code&gt;myService&lt;/code&gt; has a interface that can be mocked and I can tell it to throw a &lt;code&gt;EntityCommandExecutionException&lt;/code&gt; when &lt;code&gt;DoSomethingThatMightTakeALongTime&lt;/code&gt; is called and the constructor for that takes a string and an Exception as an inner exception.  However, you can't create a new instance of SqlException because its a sealed class therefore doing the below is impossible:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var fakeService = A.Fake&amp;lt;IMyService&amp;gt;();
A.CallTo(() =&amp;gt; fakeService.DoSomethingThatMightTakeALongTime()).Throws(new EntityCommandExecutionException("What a mistaka da maka", new SqlException());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can't create your own exception class and inherit off SqlException to get around it that way either.  You could use &lt;code&gt;System.Runtime.Serialization.FormatterServices.GetUninitializedObject&lt;/code&gt; to give you a &lt;code&gt;SqlException&lt;/code&gt; but that won't have the &lt;code&gt;Number&lt;/code&gt; property assigned to -2.  You could also setup a method in your test class that tries to connect to a non existant db that times out after 1 second but again that won't give you the Number property you may want plus its a lot of ugly and unnecessary code in a unit test project.  &lt;/p&gt;

&lt;h2&gt;How did you do it?&lt;/h2&gt;

&lt;p&gt;So after browsing all the stackoverflow answers and comments I came up with a solution that worked which I thought I'd share so here it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private SqlException GetSqlException()
{
    SqlErrorCollection collection = Construct&amp;lt;SqlErrorCollection&amp;gt;();
    SqlError error = Construct&amp;lt;SqlError&amp;gt;(-2, (byte)2, (byte)3, "server name", "error message", "proc", 100, (uint)1);

    typeof(SqlErrorCollection)
        .GetMethod("Add", BindingFlags.NonPublic | BindingFlags.Instance)
        .Invoke(collection, new object[] { error });    

    var e = typeof(SqlException)
        .GetMethod("CreateException", BindingFlags.NonPublic | BindingFlags.Static, null, CallingConventions.ExplicitThis, new[] { typeof(SqlErrorCollection), typeof(string) }, new ParameterModifier[] { })
        .Invoke(null, new object[] { collection, "11.0.0" }) as SqlException;

    return e;
}

private T Construct&amp;lt;T&amp;gt;(params object[] p)
{
    return (T)typeof(T).GetConstructors(BindingFlags.NonPublic | BindingFlags.Instance)[0].Invoke(p);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then amend the previous mocking code to look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var fakeService = A.Fake&amp;lt;IMyService&amp;gt;();
A.CallTo(() =&amp;gt; fakeService.DoSomethingThatMightTakeALongTime()).Throws(new EntityCommandExecutionException("What a mistaka da maka", GetSqlException());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see it uses reflection to create instances of all the sealed classes required and it also calls sealed methods to assign properties ie/adding the error instance to the collection instance.  You'll see that the -2 value is the first argument in the parameters used to construct the SqlError object so if you're interested in using the Number property on the exception thats where to change it.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This approach works and allows me to test my code but all in all its not particualry elegant and the following gif can sum up what we've learnt from sealed methods and classes and thats they're &lt;em&gt;nasty&lt;/em&gt;: &lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/pR3tklc.gif" alt="Nasty" /&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/12/20/using-sql-server-with-nodejs/</guid><link>http://blog.jonathanchannon.com/2013/12/20/using-sql-server-with-nodejs/</link><a10:author><a10:name /></a10:author><category>community</category><category>javascript</category><category>node.js</category><category>OSS</category><category>sql</category><title>Using SQL Server with node.js</title><description>&lt;p&gt;I like to keep eyes and ears open for new technologies and methodologies in order to become a better developer and I'd heard about &lt;a href="http://tjanczuk.github.io/edge/#/"&gt;edge.js&lt;/a&gt; many months ago but made a mental note of it and waved it goodbye.  edge.js lets you have two-way communication between node and C# libraries.  When I first looked at it I thought that sounded a bit hacky, I've spent my time communicating with COM libraries in Delphi and OCX libraries with C# and didn't like it so I felt this was pretty much the same thing.  A long time passed and I was writing a console based Windows app as a service and had wondererd whether I could quickly port it to node.  &lt;/p&gt;

&lt;p&gt;I was discussing with a colleague about using node at work and that we needed something seperate and small just to try it out and see how the whole developement process with it worked.  As the database that this app needed to communicate with was MSSQL I looked into a library on NPM that would communicate with MSSQL and maybe act as an ORM.  There was a Microsoft lib that seemed untouched and reading the comments on the issues list on Github it didnt favour too well.  There were libraries that would communicate with MySQL &amp;amp; PostgresSQL but not MSSQL.  In my search I came across edge.js again.  It had 2 samples, one that used edge-sql and one that used ScriptCS so in laymans terms, one that used a precompiled dll and one that used a C# script that was executed at runtime.&lt;/p&gt;

</description><pubDate>Fri, 20 Dec 2013 00:00:00 Z</pubDate><a10:updated>2013-12-20T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I like to keep eyes and ears open for new technologies and methodologies in order to become a better developer and I'd heard about &lt;a href="http://tjanczuk.github.io/edge/#/"&gt;edge.js&lt;/a&gt; many months ago but made a mental note of it and waved it goodbye.  edge.js lets you have two-way communication between node and C# libraries.  When I first looked at it I thought that sounded a bit hacky, I've spent my time communicating with COM libraries in Delphi and OCX libraries with C# and didn't like it so I felt this was pretty much the same thing.  A long time passed and I was writing a console based Windows app as a service and had wondererd whether I could quickly port it to node.  &lt;/p&gt;

&lt;p&gt;I was discussing with a colleague about using node at work and that we needed something seperate and small just to try it out and see how the whole developement process with it worked.  As the database that this app needed to communicate with was MSSQL I looked into a library on NPM that would communicate with MSSQL and maybe act as an ORM.  There was a Microsoft lib that seemed untouched and reading the comments on the issues list on Github it didnt favour too well.  There were libraries that would communicate with MySQL &amp;amp; PostgresSQL but not MSSQL.  In my search I came across edge.js again.  It had 2 samples, one that used edge-sql and one that used ScriptCS so in laymans terms, one that used a precompiled dll and one that used a C# script that was executed at runtime.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Looking at the samples the Github repo gave you could do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var edge = require('edge');

var getTop10Products = edge.func('sql', function () {/*
    select top 10 * from Products
*/});

getTop10Products(null, function (error, result) {
    if (error) throw error;
    console.log(result);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thats it, you could call &lt;code&gt;node myscript&lt;/code&gt; and it would log out the values of the result variable.  &lt;/p&gt;

&lt;p&gt;What this did was in fact send the SQL string to a compiled dll which had a class and async method in it that was setup to respond to calls from node js.  This method essentially returned a C# &lt;code&gt;List&amp;lt;object&amp;gt;&lt;/code&gt; that was serialized to JSON so the node.js function could interact with it.  The one issue I saw with it was the actual format of the JSON.  It was a 2 dimentional array, with the first array in the parent array containing the column names and the subsequent arrays containing values from the rows in the SQL result.  &lt;/p&gt;

&lt;h2&gt;Time to roll up your sleeves&lt;/h2&gt;

&lt;p&gt;Whilst I liked the fact that I could now return data from MSSQL with node its format wasnt quite right.  I forked the project on Github and then looked at the way it was executing the SQL and storing it in a &lt;code&gt;List&amp;lt;object&amp;gt;&lt;/code&gt;.  Whilst I kept the &lt;code&gt;List&amp;lt;object&amp;gt;&lt;/code&gt; return type the information inside it differed.  I was now using &lt;code&gt;var dataObject = new ExpandoObject() as IDictionary&amp;lt;string, Object&amp;gt;;&lt;/code&gt; and for each field in the resulting SQL dataset I populated it like so &lt;code&gt;dataObject.Add(record.GetName(i), resultRecord[i]);&lt;/code&gt; ie/ the column name and corresponding value.  So this looped over the sql storing objects in a list and then returning it as JSON as it did before.  What this meant was that the API had now changed so I could refer to the column names as object properties on the node object.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;getTop10Products(null, function (error, result) {
    if (error) throw error;
    console.log(result[0].ProductName);
    console.log(result[1].UnitPrice);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bingo!&lt;/p&gt;

&lt;p&gt;So now just out of curisotiy I wanted to right a sample ExpressJS app to see how I could use this to have a JS file that acted as a C# repository to do all the data access.  I'll let you look into setting express up yourself but what I managed to do was this:&lt;/p&gt;

&lt;h4&gt;server.js&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;var express = require('express');
var edge = require('edge');
var index = require('./index.js');
var db = require('./db.js');

var app = express();

app.get('/', index.home(db));

app.listen(999);
console.log('Listening on port 999')
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;db.js&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;var edge = require('edge');

exports.getProducts = edge.func('sql', function() {/*
                    select top 10 * from Products 
                 */});
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;index.js&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;exports.home = function(db) {
    return function(req, res) {
        db.getProducts(null, function(error, result) {
            if (error) throw error;
            var data = {};
            data.all = result;
            data.Item1Name = result[0].ProductName;
            data.Item2ReorderLevel = result[1].ReorderLevel;
            res.send(data);
        });
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I fired up a browser and pointed it at http://localhost:999 and it returned showed me my 10 products, then my first item's product name and second item's re-order level. Consider me pleased!&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I know some people will think using MSSQL for a node app seems odd but if you want to spike something up and/or only have access to a MSSQL db for whatever reason you can now do it very easily and actually quite elegantly.  You execute your SQL and you get back a JSON object that represents your data, same as any other SQL/NOSQL database.  Give it a whirl and see how you get on!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/11/25/octopus-xml-transformation-in-services/</guid><link>http://blog.jonathanchannon.com/2013/11/25/octopus-xml-transformation-in-services/</link><a10:author><a10:name /></a10:author><category>.net</category><category>octopusdeploy</category><title>Octopus XML Transformation in Services</title><description>&lt;p&gt;We use &lt;a href="http://octopusdeploy.com/"&gt;Octopus Deploy&lt;/a&gt; at work and its a superb tool for deploying your applications whether they be websites or *.exes.&lt;/p&gt;

&lt;p&gt;One of the great things it also provides is the ability to use &lt;a href="http://msdn.microsoft.com/en-us/library/dd465326.aspx"&gt;Microsoft's Transformation&lt;/a&gt; process for config files.  However, when deploying a exe application its a bit trickier than a website.  Unfortunately the documentation doesn't mention the steps needed to get this working so read on!  &lt;/p&gt;

&lt;p&gt;Typically a web application will have web.config and a web.Release.config as well as other derivations you may use.  Octopus also supports web.[Environment].config.&lt;/p&gt;

</description><pubDate>Mon, 25 Nov 2013 00:00:00 Z</pubDate><a10:updated>2013-11-25T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;We use &lt;a href="http://octopusdeploy.com/"&gt;Octopus Deploy&lt;/a&gt; at work and its a superb tool for deploying your applications whether they be websites or *.exes.&lt;/p&gt;

&lt;p&gt;One of the great things it also provides is the ability to use &lt;a href="http://msdn.microsoft.com/en-us/library/dd465326.aspx"&gt;Microsoft's Transformation&lt;/a&gt; process for config files.  However, when deploying a exe application its a bit trickier than a website.  Unfortunately the documentation doesn't mention the steps needed to get this working so read on!  &lt;/p&gt;

&lt;p&gt;Typically a web application will have web.config and a web.Release.config as well as other derivations you may use.  Octopus also supports web.[Environment].config.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;In a console application you have an app.config and maybe a app.Release.config if you create one.  Deploying this via Octopus won't invoke the XML transformation.&lt;/p&gt;

&lt;p&gt;The trick is to rename the app.Release.config to be the name of the final config file produced by the build along with the 'exe' extension in it and to make sure in Visual Studio you set the build to Copy Always on the MyApp.exe.Release.config file.&lt;/p&gt;

&lt;p&gt;So for example if your project is called MyApp and you have an app.config and app.Release.config, open Windows Explorer and rename it to MyApp.exe.Release.config.  Visual Studio won't allow you to rename these files that are dependent on another so you now have to open up MyApp.csproj and alter the references from app.Release.config to MyApp.exe.Release.config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Content Include="App.config" /&amp;gt;
&amp;lt;Content Include="MyApp.exe.Release.config" &amp;gt;
    &amp;lt;DependentUpon&amp;gt;App.Config&amp;lt;/DependentUpon&amp;gt;
&amp;lt;/Content&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go into VS, Reload All when it prompts you and set the Copy to Output Directory value to Copy Always&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/E8Kbezh.jpg" alt="VS Property Window" /&gt;&lt;/p&gt;

&lt;p&gt;Now when you deploy Octopus should run the transformation and replace connection strings etc as you would expect.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/11/07/using-git-to-update-youtrack-via-teamcity/</guid><link>http://blog.jonathanchannon.com/2013/11/07/using-git-to-update-youtrack-via-teamcity/</link><a10:author><a10:name /></a10:author><category>git</category><category>team city</category><category>youtrack</category><title>Using Git to update YouTrack via TeamCity</title><description>&lt;p&gt;This post is mainly a reminder for me as I keep forgetting the command in Git to integrate commits to YouTrack items.&lt;/p&gt;

&lt;p&gt;YouTrack uses TeamCity to get the information about the commits and then scans the commit comment for a YouTrack item id and any commands that it can apply such as item status or time spent on said item.&lt;/p&gt;

&lt;p&gt;There is some documentation &lt;a href="http://confluence.jetbrains.com/display/YTD4/Executing+Commands+from+Comment+to+VCS+Commit"&gt;here&lt;/a&gt; but its not the greatest in terms of clarity and I've spoken to &lt;a href="https://twitter.com/hhariri"&gt;Hadi Hariri&lt;/a&gt; from JetBrains about improving this so hopefully they're working on it.&lt;/p&gt;

&lt;p&gt;Anyhow here's some example Git commands to wire it all up&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git commit -am "I fixed a massive bug #PROJ-158 Complete"
git commit -am "I fixed a massive bug #PROJ-158 Complete add work 1h"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first command will update the status of YouTrack item #PROJ-158 to Complete.  The second item will do the same but also add Time Tracking information to the item in YouTrack.&lt;/p&gt;

&lt;p&gt;Hope that helps, Happy Coding!&lt;/p&gt;
</description><pubDate>Thu, 07 Nov 2013 00:00:00 Z</pubDate><a10:updated>2013-11-07T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;This post is mainly a reminder for me as I keep forgetting the command in Git to integrate commits to YouTrack items.&lt;/p&gt;

&lt;p&gt;YouTrack uses TeamCity to get the information about the commits and then scans the commit comment for a YouTrack item id and any commands that it can apply such as item status or time spent on said item.&lt;/p&gt;

&lt;p&gt;There is some documentation &lt;a href="http://confluence.jetbrains.com/display/YTD4/Executing+Commands+from+Comment+to+VCS+Commit"&gt;here&lt;/a&gt; but its not the greatest in terms of clarity and I've spoken to &lt;a href="https://twitter.com/hhariri"&gt;Hadi Hariri&lt;/a&gt; from JetBrains about improving this so hopefully they're working on it.&lt;/p&gt;

&lt;p&gt;Anyhow here's some example Git commands to wire it all up&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git commit -am "I fixed a massive bug #PROJ-158 Complete"
git commit -am "I fixed a massive bug #PROJ-158 Complete add work 1h"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first command will update the status of YouTrack item #PROJ-158 to Complete.  The second item will do the same but also add Time Tracking information to the item in YouTrack.&lt;/p&gt;

&lt;p&gt;Hope that helps, Happy Coding!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/10/21/running-mocha-tests-with-sublime-text/</guid><link>http://blog.jonathanchannon.com/2013/10/21/running-mocha-tests-with-sublime-text/</link><a10:author><a10:name /></a10:author><category>javascript</category><category>mocha</category><category>oss</category><category>sublime text 2</category><category>unit testing</category><title>Running Mocha tests within Sublime Text</title><description>&lt;p&gt;I spend most of my day in Visual Studio with lots of the goodies an IDE can offer.  One of them being able to run your tests from a keystroke.&lt;/p&gt;

&lt;p&gt;In a bid to expand my mind I'm working on a little project that is made up of JS entirely so I've dug out &lt;a href="http://sublimetext.com"&gt;Sublime Text&lt;/a&gt;. It has lots of plugins that are very handy, especially &lt;a href="https://github.com/victorporof/Sublime-HTMLPrettify"&gt;Sublime-HTMLPrettify&lt;/a&gt; which will tidy your HTML, CSS &amp;amp; JS for you.&lt;/p&gt;

&lt;p&gt;When writing tests for JS there are many libraries you can use but I've chosen &lt;a href="http://visionmedia.github.io/mocha/"&gt;Mocha&lt;/a&gt; for now.  The one thing I couldn't work out was to run my tests within Sublime Text until now.&lt;/p&gt;

&lt;h3&gt;Build System&lt;/h3&gt;

&lt;p&gt;Sublime allows you to have build systems a bit like an IDE so you can tell it what to do when you invoke it via &lt;kbd&gt;cmd&lt;/kbd&gt;+&lt;kbd&gt;B&lt;/kbd&gt;.&lt;/p&gt;

&lt;p&gt;To get Mocha to run we need to create a new build system. To do this click Tools - Build System - New Build System and paste in the below:&lt;/p&gt;

</description><pubDate>Sun, 20 Oct 2013 23:00:00 Z</pubDate><a10:updated>2013-10-20T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I spend most of my day in Visual Studio with lots of the goodies an IDE can offer.  One of them being able to run your tests from a keystroke.&lt;/p&gt;

&lt;p&gt;In a bid to expand my mind I'm working on a little project that is made up of JS entirely so I've dug out &lt;a href="http://sublimetext.com"&gt;Sublime Text&lt;/a&gt;. It has lots of plugins that are very handy, especially &lt;a href="https://github.com/victorporof/Sublime-HTMLPrettify"&gt;Sublime-HTMLPrettify&lt;/a&gt; which will tidy your HTML, CSS &amp;amp; JS for you.&lt;/p&gt;

&lt;p&gt;When writing tests for JS there are many libraries you can use but I've chosen &lt;a href="http://visionmedia.github.io/mocha/"&gt;Mocha&lt;/a&gt; for now.  The one thing I couldn't work out was to run my tests within Sublime Text until now.&lt;/p&gt;

&lt;h3&gt;Build System&lt;/h3&gt;

&lt;p&gt;Sublime allows you to have build systems a bit like an IDE so you can tell it what to do when you invoke it via &lt;kbd&gt;cmd&lt;/kbd&gt;+&lt;kbd&gt;B&lt;/kbd&gt;.&lt;/p&gt;

&lt;p&gt;To get Mocha to run we need to create a new build system. To do this click Tools - Build System - New Build System and paste in the below:&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;pre&gt;&lt;code&gt;{
    "cmd": ["make"],
    "file_regex": "^(..[^:]*):([0-9]+):?([0-9]+)?:? (.*)$",
    "working_dir": "${project_path:${folder:${file_path}}}",
    "selector": "source.makefile",
    "shell": true,
    "variants": [{
        "name": "Clean",
        "cmd": ["make", "clean"]
    }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Click Save and call it Mocha&lt;/p&gt;

&lt;p&gt;Now when you have a project go to Tools - Build System and select Mocha&lt;/p&gt;

&lt;h3&gt;Make file&lt;/h3&gt;

&lt;p&gt;A Make file is a script that allows you to execute various commands and its what our build system looks for when we tell Sublime to build our project. We need a file called &lt;code&gt;makefile&lt;/code&gt; in the root of our project.  Inside that &lt;code&gt;makefile&lt;/code&gt; we can invoke Mocha to run our tests.&lt;/p&gt;

&lt;p&gt;Place this in your &lt;code&gt;makefile&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;test:
    mocha --recursive --reporter spec moviebucketlist.tests/*.js
.PHONY: test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now when you invoke the build system via &lt;kbd&gt;cmd&lt;/kbd&gt;+&lt;kbd&gt;B&lt;/kbd&gt; in Sublime it will execute Mocha and give you the results in the console of Sublime.  Mocha by default will look for a folder called &lt;code&gt;test&lt;/code&gt; and execute the tests inside it. If you have a different folder name you can append the folder name and wildcard to js files like I have done above.&lt;/p&gt;

&lt;p&gt;Happy Coding!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/10/01/blogging-with-markdown-and-git/</guid><link>http://blog.jonathanchannon.com/2013/10/01/blogging-with-markdown-and-git/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>community</category><category>git</category><category>nancyfx</category><category>oss</category><category>snow</category><title>Blogging with Markdown &amp; Deploying via Git - Introducing Sandra.Snow</title><description>&lt;p&gt;There are many markdown blogging engines out there such as &lt;a href="http://calepin.co/"&gt;Calepin&lt;/a&gt;, &lt;a href="http://scriptogr.am/"&gt;Scriptogram&lt;/a&gt; and even &lt;a href="http://wordpress.org/"&gt;WordPress&lt;/a&gt; allows you to write blog posts in Markdown but &lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Sandra.Snow&lt;/a&gt; tries to add something different.  Firstly, it is written in .Net and &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt;, secondly its a static blog generator and finally it supports Git deployment.&lt;/p&gt;

&lt;p&gt;Even if you don't want to use Git deployment you can use FTP, its a great tool.  To write your blog post in Markdown you need a custom header in your file so it knows some information about your post.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
layout: post
category: Azure
title: Setting up a ServiceStack Service
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It then parses this information along with your Markdown into its engine, uses a Markdown view engine to convert the file content into HTML, assign model properties based on the header and creates a HTML file using the model via a Razor viewengine.&lt;/p&gt;

&lt;p&gt;The "layout" refers to the Razor file it uses to render the final HTML file.  This allows you to style your pages and blog posts whichever way you'd prefer.  These "layout" files should exist in the "_layouts" folder for your site template.  The site template is a set of files and folders that Sandra.Snow uses to produce the final static website.&lt;/p&gt;

&lt;p&gt;The "category" or "categories" property, you can use both for singular or multiple comma-seperated values that refer to the category/categories of your blog post.&lt;/p&gt;

&lt;p&gt;The "title" should hopefully be self explanatory!&lt;/p&gt;

&lt;p&gt;You can optionally add an author and email properties to override the global config settings for example, if you wanted to allow guest author blog posts.  There is also an optional metadescription property you can use for SEO.
</description><pubDate>Mon, 30 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-30T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;There are many markdown blogging engines out there such as &lt;a href="http://calepin.co/"&gt;Calepin&lt;/a&gt;, &lt;a href="http://scriptogr.am/"&gt;Scriptogram&lt;/a&gt; and even &lt;a href="http://wordpress.org/"&gt;WordPress&lt;/a&gt; allows you to write blog posts in Markdown but &lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Sandra.Snow&lt;/a&gt; tries to add something different.  Firstly, it is written in .Net and &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt;, secondly its a static blog generator and finally it supports Git deployment.&lt;/p&gt;

&lt;p&gt;Even if you don't want to use Git deployment you can use FTP, its a great tool.  To write your blog post in Markdown you need a custom header in your file so it knows some information about your post.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
layout: post
category: Azure
title: Setting up a ServiceStack Service
---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It then parses this information along with your Markdown into its engine, uses a Markdown view engine to convert the file content into HTML, assign model properties based on the header and creates a HTML file using the model via a Razor viewengine.&lt;/p&gt;

&lt;p&gt;The "layout" refers to the Razor file it uses to render the final HTML file.  This allows you to style your pages and blog posts whichever way you'd prefer.  These "layout" files should exist in the "_layouts" folder for your site template.  The site template is a set of files and folders that Sandra.Snow uses to produce the final static website.&lt;/p&gt;

&lt;p&gt;The "category" or "categories" property, you can use both for singular or multiple comma-seperated values that refer to the category/categories of your blog post.&lt;/p&gt;

&lt;p&gt;The "title" should hopefully be self explanatory!&lt;/p&gt;

&lt;p&gt;You can optionally add an author and email properties to override the global config settings for example, if you wanted to allow guest author blog posts.  There is also an optional metadescription property you can use for SEO.
&lt;!--excerpt--&gt;&lt;/p&gt;

&lt;h3&gt;Global Config&lt;/h3&gt;

&lt;p&gt;In the root of the site template is a snow.config file which is what Sandra.Snow uses to determine url format, where to look for posts and layouts and other related information. It is JSON formatted and looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  "blogTitle" : "Joe Bloggs Blog",
  "author" : "Mr.Guest",
  "email" : "guest@gmail.com",
  "siteUrl": "http://blog.joebloggs.com",
  "posts": "Snow/_posts",
  "layouts": "Snow/_layouts",
  "output": "../MYRelativeWebsiteFolder",
  "urlFormat": "yyyy/MM/dd/slug",
  "copyDirectories": [
    "Snow/images =&amp;gt; images",
    "Snow/js =&amp;gt; js",
    "Snow/css =&amp;gt; css"
  ],
  "processFiles": [{
    "file": "Snow/index.cshtml",
    "loop": "Posts"
  },{
    "file": "Snow/category.cshtml",
    "loop": "Categories"
  },{
    "file": "Snow/categories.cshtml =&amp;gt; categories"
  },{
    "file": "Snow/archive.cshtml =&amp;gt; archive"
  },{
    "file": "Snow/about.cshtml =&amp;gt; about"
  },{
    "file": "Snow/contact.cshtml =&amp;gt; contact"
  },{
    "file": "feed.xml",
    "loop": "RSS"
  },{
    "file": "sitemap.xml",
    "loop": "sitemap"
  }]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is an explanation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;"blogTitle" : The title of the blog you want to appear on your RSS feed&lt;/li&gt;
&lt;li&gt;"author" : The author's name&lt;/li&gt;
&lt;li&gt;"email" : The author's email.(You can use an &lt;a href="https://github.com/Sandra/Sandra.Snow/wiki/Gravatar-Support"&gt;HTMLHelper&lt;/a&gt; in the view that gets the author's Gravatar from the global/post settings)&lt;/li&gt;
&lt;li&gt;"siteUrl": "This is used to enable Disqus support. Simply use an &lt;a href="https://github.com/Sandra/Sandra.Snow/wiki/Disqus-Support"&gt;HTMLHelper&lt;/a&gt; to render Disqus comments"&lt;/li&gt;
&lt;li&gt;"posts" : The location of the markdown files&lt;/li&gt;
&lt;li&gt;"layouts" : The location of the layout files&lt;/li&gt;
&lt;li&gt;"output" : The location where Sandra.Snow will put the static HTML. This is relative&lt;/li&gt;
&lt;li&gt;"urlFormat" : The format of the URL to your blog post&lt;/li&gt;
&lt;li&gt;"copyDirectories" : The directories in your template that it will copy to the output&lt;/li&gt;
&lt;li&gt;&lt;p&gt;"processFiles" : This takes an object of the filename and property information on how to render the file.  Each file/view will be called and rendered with model information availble.  The model information available to these views are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public List&amp;lt;Post&amp;gt; PostsInCategory { get; set; }
public Dictionary&amp;lt;int, Dictionary&amp;lt;int, List&amp;lt;Post&amp;gt;&amp;gt;&amp;gt; PostsGroupedByYearThenMonth { get; set; }
public List&amp;lt;Post&amp;gt; Posts { get; set; }
public List&amp;lt;Post&amp;gt; PostsPaged { get; set; }


public bool HasPreviousPage { get; set; }
public bool HasNextPage { get; set; }
public int NextPage { get; set; }
public int PreviousPage { get; set; }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the standard website template there are category, categories, about and index *.cshtml pages which accept this model information and render the relevant information. Based on the file name you can guess what each file outputs.  The "loop" in the settings is used internally by Sandra.Snow to process the relevant data. For example "RSS" creates a RSS file based on the list of posts while Posts/Categories create sub-directories for the relevant model type eg/categories/posts.  "sitemap" will use the &lt;code&gt;List&amp;lt;Post&amp;gt;&lt;/code&gt; to create a sitemap.xml in the root of your blog. &lt;/p&gt;

&lt;p&gt;As Sandra.Snow is a static HTML generator it will create folders with the relevant name for the post or file named in the config file eg/&lt;code&gt;http://mydomain.com/2013/08/18/this-is-a-great-article&lt;/code&gt; or &lt;code&gt;http://mydomain.com/categories&lt;/code&gt; and create a &lt;code&gt;index.html&lt;/code&gt; file for each folder.  In the root of the website it will create a &lt;code&gt;index.html&lt;/code&gt; with 10 blog posts inside it.  If you have 100 markdown posts it will it will page this for you and create links to &lt;code&gt;http://mydomain.com/page2&lt;/code&gt; etc.  If you use &lt;code&gt;&amp;lt;!--excerpt--&amp;gt;&lt;/code&gt; in your Markdown it will read up to that point so you can click a "read more" link otherwise it will use the whole Markdown content.    If you look at the model properties you'll see your index layout view can tell if there is a next/previous page and therefore create the relevant links in the HTML.&lt;/p&gt;

&lt;p&gt;Once you run the Sandra.Snow exe it will output the HTML and you can then FTP your files to your website.&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href="https://github.com/Sandra/Sandra.Snow/wiki"&gt;wiki&lt;/a&gt; for more details about other HTMLHelpers such as Google Analytics.&lt;/p&gt;

&lt;h3&gt;Git Integration&lt;/h3&gt;

&lt;p&gt;FTP is so 2001 so Sandra.Snow has a website called Sandra.Snow.Barbato which allows you to access it (final URL to be confirmed) and log in with your Github credentials.  It will then give you a list of your repositories, the idea being one of them is your blog with the markdown posts and snow.config etc.  A &lt;a href="https://github.com/Sandra/Sandra.Snow.BarbatoTemplate"&gt;base template&lt;/a&gt; is available in the Sandra repository for you to fork.  You can then choose whether you'd like to deploy to another Git repository that supports Git deployment eg/Azure, AppHarbor, Heroku or you can select FTP.  In either scenario, enter your details and off you go. The website will use Sandra.Snow to create the output and it will then wire it over to your chosen destination.  &lt;/p&gt;

&lt;p&gt;Sandra.Snow.Barbato is also setup to handle Github hooks so in Github if you tell your repository to do a post commit hook to the website, after you write a new blog post and push to Github it will post to the website and know if you've previously logged in and if so generate the HTML and re-deploy your blog.  It will also push the generated content back to your Github repository on the master branch. Very nice!&lt;/p&gt;

&lt;h3&gt;Setting up Sandra.Snow.Barbato&lt;/h3&gt;

&lt;p&gt;One you have forked the Barbato template you can begin to style your blog pages.  Obviously every time you want to make a style change you want to see the results.  There are 2 ways to do this.  i) Run Sandra.Snow locally, setup a webserver eg/IISExpress to point to the Snow output directory and open up your browser to see the changes. Keep making changes to the *.cshtml and *.css files until happy ii) Make the changes in your repo, sign up with Sandra.Snow.Barbato and go to your domain to check the changes that were deployed.&lt;/p&gt;

&lt;h4&gt;Azure&lt;/h4&gt;

&lt;p&gt;If deploying to Azure you must have a .deployment file in the root of your repository that contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[config]
project = Website
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is needed because when your template repository is pushed to Azure it needs to know what to deploy. This simply tells it to use the Website folder ie.the output folder from Sandra.Snow.&lt;/p&gt;

&lt;h4&gt;Github Pages&lt;/h4&gt;

&lt;p&gt;If deploying to Github you need a few things. Your repository needs to be called &lt;code&gt;username.github.io&lt;/code&gt;. You then need to create a CNAME file in your repo that has the domain you will be using inside it. Finally you need to setup the DNS on your domain to point to &lt;code&gt;username.github.io&lt;/code&gt; by creating a CNAME record in your DNS Manager. What you'll have to probably do is clone your &lt;code&gt;username.github.io&lt;/code&gt; repo and run Snow and set the output setting in snow.config to the path of your &lt;code&gt;username.github.io&lt;/code&gt; folder. You can then push the changes to Github. &lt;/p&gt;

&lt;h3&gt;Wordpress Migration&lt;/h3&gt;

&lt;p&gt;My blog was previously using Wordpress so I needed to get my data out.  The most common referred to tool was &lt;a href="https://github.com/dreikanter/wp2md"&gt;wp2md&lt;/a&gt; which uses Python to go through the exported Wordpress content and then convert to Markdown.  For some reason I didn't go with that choice and went with &lt;a href="http://heckyesmarkdown.com/"&gt;http://heckyesmarkdown.com/&lt;/a&gt;.  Its a bit more work because you have to give it your previous URLs to your blog posts and it reads the source of the page and converts it to Markdown.  It worked brilliantly for me.  I had to make a few changes on the output it provided by generally it was very good.&lt;/p&gt;

&lt;p&gt;As I didn't want to worry about HTTP 302, I made sure I saved my markdown files as the urls are on my live site so &lt;a href="http://blog.jonathanchannon.com/2012/12/19/why-use-nancyfx/"&gt;http://blog.jonathanchannon.com/2012/12/19/why-use-nancyfx/&lt;/a&gt; was saved in a file called &lt;code&gt;2012-12-19-why-use-nancyfx.md&lt;/code&gt;. This file naming format is currently enforced so Snow can gather date and slug information(unsafe characters in the slug/title for URLs will be removed).&lt;/p&gt;

&lt;p&gt;I then went through addind the meta headers to tell Sandra.Snow a bit more about the posts and also added in the &lt;code&gt;&amp;lt;!--excerpt--&amp;gt;&lt;/code&gt; information so not to render the whole blog content on the home pages.&lt;/p&gt;

&lt;p&gt;I then went through and styled the master page &lt;code&gt;default.cshtml&lt;/code&gt; in the _layouts folder as well as the &lt;code&gt;post.cshtml&lt;/code&gt; and the other files in the root of the site template folder.&lt;/p&gt;

&lt;p&gt;Once done I ran the .exe file to generate my content.  One of the great things about Sandra.Snow is its speed. It takes less than a second to do 100 blog posts, luckily I only have 25 so its really fast.  I opened up a browser and checked my files and if some styling needed tweaking I could do so and re-run.  Once all ok I can deploy or push the template folder to Github, setup the post commit hook and then use Sandra.Snow.Barbato to handle deployment from now on.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;If you're a Git and Markdown user and want to create a blog with complete simplicity this is a great tool.  No more messy Wordpress, no more running exe's on your machine (unless you want to), its completely automated apart from writing the blog posts!  In fact I'm so happy with this project, this blog is using it!  Give it a try and if you like the look of it get involved with its development.  &lt;a href="https://github.com/Sandra/Sandra.Snow"&gt;Sandra.Snow&lt;/a&gt; the new modern, simplistic and effective tool for blogging.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/09/20/returning-multiple-fake-objects-with-fakeiteasy/</guid><link>http://blog.jonathanchannon.com/2013/09/20/returning-multiple-fake-objects-with-fakeiteasy/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>fake it easy</category><category>integration testing</category><category>oss</category><category>unit testing</category><title>Returning multiple fake objects with FakeItEasy</title><description>&lt;p&gt;I was recently writing some unit tests where I needed to test that multiple calls to an interface returned different objects.  &lt;/p&gt;

&lt;p&gt;With &lt;a href="https://github.com/FakeItEasy/FakeItEasy"&gt;FakeItEasy&lt;/a&gt; this is easy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All very nice, but now if I have multiple calls to &lt;code&gt;myInterface&lt;/code&gt; I have to execute the above statement 'x' amount of times:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void Should_Do_Something()
{
  var myInterface = A.Fake&amp;lt;IApplication&amp;gt;();
  A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(2)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(3)).Returns(new Something());

  var result = sut.DoSomething(myInterface);

  Assert.Equal("Super Duper", result);
}
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Thu, 19 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-19T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I was recently writing some unit tests where I needed to test that multiple calls to an interface returned different objects.  &lt;/p&gt;

&lt;p&gt;With &lt;a href="https://github.com/FakeItEasy/FakeItEasy"&gt;FakeItEasy&lt;/a&gt; this is easy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All very nice, but now if I have multiple calls to &lt;code&gt;myInterface&lt;/code&gt; I have to execute the above statement 'x' amount of times:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void Should_Do_Something()
{
  var myInterface = A.Fake&amp;lt;IApplication&amp;gt;();
  A.CallTo(() =&amp;gt; myInterface.GetSomething(1)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(2)).Returns(new Something());
  A.CallTo(() =&amp;gt; myInterface.GetSomething(3)).Returns(new Something());

  var result = sut.DoSomething(myInterface);

  Assert.Equal("Super Duper", result);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;There is a tidier way to do the above where you can return specific objects and its called &lt;code&gt;ReturnsLazily&lt;/code&gt;.  Lets take a look at this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Employee
{
    public string Name { get; set; }
}

public interface IEmployeeRepository
{
    Employee GetEmployeeById(int id);
}

public class App
{
    private readonly IEmployeeRepository employeeRepository;

    public App(IEmployeeRepository employeeRepository)
    {
        this.employeeRepository = employeeRepository;
    }

    public string GetNamesAsCsv(int[] ids)
    {
        var employees = ids.Select(id =&amp;gt; employeeRepository.GetEmployeeById(id).Name);
        return string.Join(",", employees);
    }
}

public class AppTests
{
    [Fact]
    public void AppReturnsNamesAsCsv()
    {
        //Given
        var employees = new Dictionary&amp;lt;int, Employee&amp;gt;
        {
            { 1, new Employee { Name = "Moss"} },
            { 2, new Employee { Name = "Roy"} },
        };

        var fakeRepository = A.Fake&amp;lt;IEmployeeRepository&amp;gt;();
        A.CallTo(() =&amp;gt; fakeRepository.GetEmployeeById(A&amp;lt;int&amp;gt;.Ignored))
            .ReturnsLazily&amp;lt;Employee, int&amp;gt;(id =&amp;gt; employees[id]);

        var app = new App(fakeRepository);

        //When
        var result = app.GetNamesAsCsv(employees.Keys.ToArray());

        //Then
        Assert.Equal("Moss,Roy", result);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have an &lt;code&gt;Employee&lt;/code&gt; object, a &lt;code&gt;IEmployeeRepository&lt;/code&gt; which returns an &lt;code&gt;Employee&lt;/code&gt; object and an App that returns a CSV.  We then want to test this and make sure we get back a CSV from multiple objects.&lt;/p&gt;

&lt;p&gt;So we set our fake setup and say that when &lt;code&gt;GetEmployeeById&lt;/code&gt; is called we want to return a specific object.  Our App class will call &lt;code&gt;GetEmployeeById&lt;/code&gt; twice with the id of 1 and 2.  This is done by passing in &lt;code&gt;employees.Keys.ToArray()&lt;/code&gt; to our GetNamesAsCsv method under test. &lt;/p&gt;

&lt;p&gt;When this is called with the id we want to return specific objects &lt;code&gt;.ReturnsLazily&amp;lt;Employee, int&amp;gt;(id =&amp;gt; employees[id]);&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This says we want to return an Employee and the argument in the repository call is an int.  We can then use that to return a specific object based on that id which is where the &lt;code&gt;Dictionary&amp;lt;int, Employee&amp;gt;&lt;/code&gt; comes in handy.  Based on the key it will return either an Employee called Moss or Roy.  Our &lt;code&gt;GetNamesAsCsv&lt;/code&gt; will then join Moss &amp;amp; Roy together as a CSV and we can assert that our method works.&lt;/p&gt;

&lt;p&gt;Hope that helps someone!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/09/16/enabling-cors-in-iisexpress/</guid><link>http://blog.jonathanchannon.com/2013/09/16/enabling-cors-in-iisexpress/</link><a10:author><a10:name /></a10:author><category>.net</category><category>iis express</category><category>integration testing</category><title>Enabling CORS in IISExpress</title><description>&lt;p&gt;I was playing around with &lt;a href="https://github.com/wordnik/swagger-ui"&gt;swagger-ui&lt;/a&gt; and was trying to point it to a local endpoint that I started with IIS Express.  I was getting an error saying that it needed the endpoint to accept Access-Control-Allow-Origin requests.&lt;/p&gt;

&lt;p&gt;I went Googling and it couldn't find anything specific to IIS Express but managed to use some guidance for full blown IIS.&lt;/p&gt;

&lt;p&gt;The solution is to go to &lt;code&gt;C:\Program Files (x86)\IIS Express\AppServer&lt;/code&gt; and open the &lt;code&gt;applicationhost.config&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Search for &lt;code&gt;httpProtocol&lt;/code&gt; and you should see this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;httpProtocol&amp;gt;
    &amp;lt;customHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
        &amp;lt;add name="X-Powered-By" value="ASP.NET" /&amp;gt;
    &amp;lt;/customHeaders&amp;gt;
    &amp;lt;redirectHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
    &amp;lt;/redirectHeaders&amp;gt;
&amp;lt;/httpProtocol&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now add this to the &lt;code&gt;customHeaders&lt;/code&gt; node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;add name="Access-Control-Allow-Origin" value="*" /&amp;gt;
&amp;lt;add name="Access-Control-Allow-Headers" value="Content-Type" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just bear in mind this opens up your webserver so you may need to find something alternative for a live production environment.&lt;/p&gt;

&lt;p&gt;Anyway you should now be able to start accepting requests via CORS when you fire up IISExpress&lt;/p&gt;
</description><pubDate>Sun, 15 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-15T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I was playing around with &lt;a href="https://github.com/wordnik/swagger-ui"&gt;swagger-ui&lt;/a&gt; and was trying to point it to a local endpoint that I started with IIS Express.  I was getting an error saying that it needed the endpoint to accept Access-Control-Allow-Origin requests.&lt;/p&gt;

&lt;p&gt;I went Googling and it couldn't find anything specific to IIS Express but managed to use some guidance for full blown IIS.&lt;/p&gt;

&lt;p&gt;The solution is to go to &lt;code&gt;C:\Program Files (x86)\IIS Express\AppServer&lt;/code&gt; and open the &lt;code&gt;applicationhost.config&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Search for &lt;code&gt;httpProtocol&lt;/code&gt; and you should see this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;httpProtocol&amp;gt;
    &amp;lt;customHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
        &amp;lt;add name="X-Powered-By" value="ASP.NET" /&amp;gt;
    &amp;lt;/customHeaders&amp;gt;
    &amp;lt;redirectHeaders&amp;gt;
        &amp;lt;clear /&amp;gt;
    &amp;lt;/redirectHeaders&amp;gt;
&amp;lt;/httpProtocol&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now add this to the &lt;code&gt;customHeaders&lt;/code&gt; node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;add name="Access-Control-Allow-Origin" value="*" /&amp;gt;
&amp;lt;add name="Access-Control-Allow-Headers" value="Content-Type" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just bear in mind this opens up your webserver so you may need to find something alternative for a live production environment.&lt;/p&gt;

&lt;p&gt;Anyway you should now be able to start accepting requests via CORS when you fire up IISExpress&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/09/12/keeping-sql-data-organised-in-integration-tests/</guid><link>http://blog.jonathanchannon.com/2013/09/12/keeping-sql-data-organised-in-integration-tests/</link><a10:author><a10:name /></a10:author><category>.net</category><category>community</category><category>integration testing</category><category>oss</category><category>xunit</category><title>Keeping SQL Data Organised in Integration Tests</title><description>&lt;p&gt;In my latest project I had kept my solution tidy with my main app project, my unit test project and integration test project. I tend to stick with a naming convention such as MainApp, MainApp.Tests.Unit &amp;amp; MainApp.Tests.Integration.&lt;/p&gt;

&lt;p&gt;I had begun writing my integration tests for a repository that hits the database and returns data. Currently it was one method being called in the repository.  &lt;a href="http://xunit.codeplex.com/"&gt;xUnit&lt;/a&gt; allows you to setup any test dependencies in the constructor of your test class.  It also allows you to do any tidying up in a Dispose method if you implement IDisposable although this is &lt;a href="http://xunit.codeplex.com/wikipage?title=Comparisons&amp;amp;referringTitle=Home#note2"&gt;frowned upon&lt;/a&gt;.  However I felt for my needs I would implement this.&lt;/p&gt;

&lt;p&gt;I  was creating data in the database in the constructor which will get called before the test runs, retrieving data in the test, asserting and then deleting all data and resetting the auto-incrementing from the tables in the Dispose method.&lt;/p&gt;

&lt;p&gt;This was working perfectly until I wanted to test another method on my repository.&lt;/p&gt;

&lt;p&gt;I now needed to add data for my new method but realised if I added different data to the database in the constructor, I would be creating unnecessary data unrelated to the test.&lt;/p&gt;

&lt;p&gt;My options were to move the constructor logic into separate methods and then call the methods in the test or have separate test classes per method in the repo.  Both were a not an ideal solution and quite frankly verbose, ugly and not best practice.
</description><pubDate>Wed, 11 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-11T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;In my latest project I had kept my solution tidy with my main app project, my unit test project and integration test project. I tend to stick with a naming convention such as MainApp, MainApp.Tests.Unit &amp;amp; MainApp.Tests.Integration.&lt;/p&gt;

&lt;p&gt;I had begun writing my integration tests for a repository that hits the database and returns data. Currently it was one method being called in the repository.  &lt;a href="http://xunit.codeplex.com/"&gt;xUnit&lt;/a&gt; allows you to setup any test dependencies in the constructor of your test class.  It also allows you to do any tidying up in a Dispose method if you implement IDisposable although this is &lt;a href="http://xunit.codeplex.com/wikipage?title=Comparisons&amp;amp;referringTitle=Home#note2"&gt;frowned upon&lt;/a&gt;.  However I felt for my needs I would implement this.&lt;/p&gt;

&lt;p&gt;I  was creating data in the database in the constructor which will get called before the test runs, retrieving data in the test, asserting and then deleting all data and resetting the auto-incrementing from the tables in the Dispose method.&lt;/p&gt;

&lt;p&gt;This was working perfectly until I wanted to test another method on my repository.&lt;/p&gt;

&lt;p&gt;I now needed to add data for my new method but realised if I added different data to the database in the constructor, I would be creating unnecessary data unrelated to the test.&lt;/p&gt;

&lt;p&gt;My options were to move the constructor logic into separate methods and then call the methods in the test or have separate test classes per method in the repo.  Both were a not an ideal solution and quite frankly verbose, ugly and not best practice.
&lt;!--excerpt--&gt;&lt;/p&gt;

&lt;h2&gt;The Solution&lt;/h2&gt;

&lt;p&gt;I started playing with the attributes on my tests to see if xUnit offered me something and was chuffed to find the &lt;code&gt;BeforeAfterTestAttribute&lt;/code&gt;.  This does exactly what it says on the tin.  Its an abstract class that you inherit from for your own implementation and overide the &lt;code&gt;Before&lt;/code&gt; and &lt;code&gt;After&lt;/code&gt; methods;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class RepoMethod1BeforeAfter : BeforeAfterTestAttribute
{
    public override void After(MethodInfo methodUnderTest)
    {
      //Insert data into tables
    }

    public override void Before(MethodInfo methodUnderTest)
    {
      //Drop data from tables
    }
}

[Fact]
[RepoMethod1BeforeAfter]
public void RepoMethod1_Should_be_Awesome()
{
  //Check it's awesome
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then put an attribute on the relevant tests that need to have specific data inserted/deleted and it keeps the design of your test class follow best practices as well as not implementing IDisposable.&lt;/p&gt;

&lt;p&gt;The only thing I can spot as a slight issue is remembering to put the attribute on your tests but I think that'll be quite obvious when your tests start failing!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/09/11/comparing-object-instances-with-fakeiteasy/</guid><link>http://blog.jonathanchannon.com/2013/09/11/comparing-object-instances-with-fakeiteasy/</link><a10:author><a10:name /></a10:author><category>.net</category><category>community</category><category>fake it easy</category><category>oss</category><category>unit testing</category><title>Comparing object instances with FakeItEasy</title><description>&lt;p&gt;I had the task of writing a new application recently and of course I chose &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt;.  One of the many great reasons is the testing capabilites it offers (For more on that see &lt;a href="http://www.marcusoft.net/2013/01/NancyTesting1.html"&gt;this&lt;/a&gt; great series of articles).&lt;/p&gt;

&lt;p&gt;The basics of a test with Nancy looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void Should_return_status_ok_when_route_exists()
{
    // Given
    var bootstrapper = new DefaultNancyBootstrapper();
    var browser = new Browser(bootstrapper);

    // When
    var result = browser.Get("/", with =&amp;gt; {
        with.HttpRequest();
    });

    // Then
    Assert.Equal(HttpStatusCode.OK, result.StatusCode);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You set up a bootstrapper, this can be your live one or an inherited version of your live one with dependencies changed to mocks for example or use the &lt;code&gt;ConfigurableBootstrapper&lt;/code&gt;.&lt;/p&gt;

</description><pubDate>Tue, 10 Sep 2013 23:00:00 Z</pubDate><a10:updated>2013-09-10T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I had the task of writing a new application recently and of course I chose &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt;.  One of the many great reasons is the testing capabilites it offers (For more on that see &lt;a href="http://www.marcusoft.net/2013/01/NancyTesting1.html"&gt;this&lt;/a&gt; great series of articles).&lt;/p&gt;

&lt;p&gt;The basics of a test with Nancy looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void Should_return_status_ok_when_route_exists()
{
    // Given
    var bootstrapper = new DefaultNancyBootstrapper();
    var browser = new Browser(bootstrapper);

    // When
    var result = browser.Get("/", with =&amp;gt; {
        with.HttpRequest();
    });

    // Then
    Assert.Equal(HttpStatusCode.OK, result.StatusCode);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You set up a bootstrapper, this can be your live one or an inherited version of your live one with dependencies changed to mocks for example or use the &lt;code&gt;ConfigurableBootstrapper&lt;/code&gt;.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;In my scenario I was testing that when a route got called something on an interface was called with an instance of an object.&lt;/p&gt;

&lt;p&gt;I had the object available in the test, I passed it to my fake interface and asserted that the call happened.&lt;/p&gt;

&lt;p&gt;Here's an example of what the route and test might look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class ApiModule : NancyModule
{
    public ApiModule(IScheduleRepository scheduleRepository)
        : base("/api/schedules")
    {
        Post["/"] = parameters =&amp;gt;
        {
            var result = this.BindAndValidate&amp;lt;Schedule&amp;gt;();

            if (!ModelValidationResult.IsValid)
            {
                return HttpStatusCode.UnprocessableEntity;
            }

            var conflict = scheduleRepository.CheckForConflict(result);

            return HttpStatusCode.Created;
        };
    }
}

[Fact]
public void Creating_Schedule_Entry_Should_Check_For_Conflicts()
{
    //Given
    var fakeScheduleRepository = A.Fake&amp;lt;IScheduleRepository&amp;gt;();
    var model = GetModel();

    var browser = new Browser(GetBootstrapper(scheduleRepository:fakeScheduleRepository));

    //When
    var result = browser.Post("/api/schedules", with =&amp;gt;
    {
        with.HttpRequest();
        with.JsonBody(model);
    });

    //Then
    A.CallTo(() =&amp;gt; fakeScheduleRepository.CheckForConflict(model)).MustHaveHappened();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The test is using &lt;a href="http://xunit.codeplex.com/"&gt;xUnit&lt;/a&gt; and &lt;a href="https://github.com/FakeItEasy/FakeItEasy"&gt;FakeItEasy&lt;/a&gt; for creating fakes/mocks or whatever you choose to call them and the test will pass if the call to &lt;code&gt;fakeScheduleRepository.CheckForConflict&lt;/code&gt; was called with the model object.&lt;/p&gt;

&lt;h2&gt;The test fails!&lt;/h2&gt;

&lt;p&gt;The reason for the test failing is because...? That's right, the object that is passed into the call on IScheduleRepository in the route is different to the one in the test.&lt;/p&gt;

&lt;p&gt;We could override Equals on our model object but that's not a great approach, we could hope that if we create an &lt;code&gt;IEqualityComparer&amp;lt;Schedule&amp;gt;&lt;/code&gt; we could pass that in somewhere but from what I've seen that's not possible so how do we get our test to pass?&lt;/p&gt;

&lt;p&gt;FakeItEasy has a nice fluent API that allows you to do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void Creating_Schedule_Entry_Should_Check_For_Conflicts()
{
    //Given
    var fakeScheduleRepository = A.Fake&amp;lt;IScheduleRepository&amp;gt;();
    var model = GetModel();

    var browser = new Browser(GetBootstrapper(scheduleRepository:fakeScheduleRepository));

    //When
    var result = browser.Post("/api/schedules", with =&amp;gt;
    {
        with.HttpRequest();
        with.JsonBody(model);
    });

    //Then
    A.CallTo(() =&amp;gt; fakeScheduleRepository.CheckForConflict(A&amp;lt;Schedule&amp;gt;.That.Matches(x =&amp;gt; BodyModel(x)))).MustHaveHappened();
}

private bool BodyModel(CreateKeywordSchedule match)
{
    return match.DateFrom == new DateTime(2013, 01, 01, 12, 00, 00) &amp;amp;&amp;amp;
           match.DateTo == new DateTime(2013, 01, 02, 12, 00, 00);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spot the difference? So instead of passing our original model object in we told FakeItEasy to expect a type of Schedule that matches an an object that is of type Schedule.  We wrote a &lt;code&gt;Func&amp;lt;Schedule,bool&amp;gt;&lt;/code&gt; to determine what a match is when comparing objects.&lt;/p&gt;

&lt;p&gt;So when the test runs and &lt;code&gt;fakeScheduleRepository.CheckForConflict(model)&lt;/code&gt; is executed FakeItEasy will assert that the argument passed into &lt;code&gt;fakeScheduleRepository.CheckForConflict()&lt;/code&gt; matches the property values we decided to match on in our BodyModel method.  &lt;/p&gt;

&lt;p&gt;This way if the model we send into the route has the property values that match those we have defined in BodyModel we can pass our test.&lt;/p&gt;

&lt;p&gt;Its a much neater way without having to write &lt;code&gt;IEqualityComparer&lt;/code&gt; or anything over the top and hope you found this useful.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/08/24/async-route-handling-with-nancy/</guid><link>http://blog.jonathanchannon.com/2013/08/24/async-route-handling-with-nancy/</link><a10:author><a10:name /></a10:author><category>.net</category><category>async</category><category>community</category><category>nancyfx</category><category>oss</category><title>Async Route Handling with Nancy</title><description>&lt;p&gt;I don't know about you but all I hear is "ASYNC ALL THE THINGS!", I think this is partly down to its new and shiny and us developers love "the shiny" and partly a lot of the things we do in our applications are I/O based whether that be database or file system. &lt;/p&gt;

&lt;p&gt;The problem that comes with the new and shiny bandwagon is you need to understand what you're evangelising. Making asynchronous methods and executing them with no actual reason will not give your codebase any gains and could actually effect your application's performance.  There is more depth to that argument but for simplicity just remember this, only use asynchronous methods if you are doing some sort of I/O. &lt;/p&gt;

&lt;p&gt;It could also be argued that only "use asynchronicity in a web framework if you expect high traffic in your web application". If you only have 10 requests on a small site you're not going to benefit from asynchronous execution as there are plenty of threads available to handle your application.  If you start hitting 1000 concurrent requests (the default IIS limit) then requests will start getting queued up.  If you make your routes asynchronous then any code that is being waited on, the thread that is being used there can be released to process another request thus speeding up the performance of your app and prevent the likely hood of large queues.  I will show how simple it is to make your routes asynchronous with Nancy below.&lt;/p&gt;

</description><pubDate>Fri, 23 Aug 2013 23:00:00 Z</pubDate><a10:updated>2013-08-23T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I don't know about you but all I hear is "ASYNC ALL THE THINGS!", I think this is partly down to its new and shiny and us developers love "the shiny" and partly a lot of the things we do in our applications are I/O based whether that be database or file system. &lt;/p&gt;

&lt;p&gt;The problem that comes with the new and shiny bandwagon is you need to understand what you're evangelising. Making asynchronous methods and executing them with no actual reason will not give your codebase any gains and could actually effect your application's performance.  There is more depth to that argument but for simplicity just remember this, only use asynchronous methods if you are doing some sort of I/O. &lt;/p&gt;

&lt;p&gt;It could also be argued that only "use asynchronicity in a web framework if you expect high traffic in your web application". If you only have 10 requests on a small site you're not going to benefit from asynchronous execution as there are plenty of threads available to handle your application.  If you start hitting 1000 concurrent requests (the default IIS limit) then requests will start getting queued up.  If you make your routes asynchronous then any code that is being waited on, the thread that is being used there can be released to process another request thus speeding up the performance of your app and prevent the likely hood of large queues.  I will show how simple it is to make your routes asynchronous with Nancy below.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h2&gt;Using Async with Nancy&lt;/h2&gt;

&lt;p&gt;With the introduction of &lt;code&gt;async/await&lt;/code&gt; in .NET 4.5 the way to do asynchronous execution simplified the previous approaches in .NET.  Having asynchronous execution within a web framework these days seems to be a "must have" so the Nancy team got their freak on (mainly &lt;a href="http://twitter.com/grumpydev"&gt;@grumpydev&lt;/a&gt;) and enabled async/await within Nancy.  Its codebase has been kept backward compatible with .NET 4.0 but has been enabled to use the .NET 4.5 &lt;code&gt;async/await&lt;/code&gt;, pretty impressive! In fact it uses its own version of &lt;code&gt;ContinueWith&lt;/code&gt; as the default one was considered not quick enough along with other &lt;a href="http://msdn.microsoft.com/en-us/library/dd460717.aspx"&gt;TPL&lt;/a&gt; optimizations.&lt;/p&gt;

&lt;p&gt;Below is the synchronous version of returning "Hello World":&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SampleModule : Nancy.NancyModule
{
    public SampleModule()
    {
        Get["/"] = parameters =&amp;gt; "Hello World!";
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we wanted to make this &lt;code&gt;async&lt;/code&gt; (although we wouldn't as there is no I/O and we wouldn't see any benefit) we would change it to look this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SampleModule : Nancy.NancyModule
{
    public SampleModule()
    {
        Get["/", true] = async (parameters, ct) =&amp;gt; "Hello World!";
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Simple and elegant hey?!&lt;/p&gt;

&lt;p&gt;So what's going on you say?  Well the boolean of "true" on the request path tells Nancy that the request is marked as asynchronous.  We can then mark the route as &lt;code&gt;async&lt;/code&gt; as you would any &lt;code&gt;async&lt;/code&gt; method and the delegate of the route now takes and additional &lt;code&gt;CancellationToken&lt;/code&gt; along with the captured parameters.  If you wanted you could use named parameters and define your route like so: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SampleModule : Nancy.NancyModule
{
    public SampleModule()
    {
        Get["/", runAsync:true] = async (parameters, ct) =&amp;gt; "Hello World!";
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;CancellationToken&lt;/code&gt; is passed in so you can check the &lt;code&gt;ct.IsCancellationRequested&lt;/code&gt; property to determine if you want to cooperatively cancel processing in your route handler.  This property may be set for example if there is an internal error or if a piece of middleware decides to cancel the request, or the host is shutting down. If you didn't know Nancy is OWIN compliant and has been pretty much since the OWIN specification came out.&lt;/p&gt;

&lt;h2&gt;Demo Time&lt;/h2&gt;

&lt;p&gt;As I stated above, returning "Hello World" from an asynchronous route is pointless so we need something I/O bound to demonstrate a bit better how we would use async/await in an application.&lt;/p&gt;

&lt;p&gt;Lets imagine we are one of those types that love QR codes and we need to generate one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class IndexModule : NancyModule
{
    public IndexModule()
    {
        Get["/"] = parameters =&amp;gt; View["Index"];

        Post["/", true] = async (x, ct) =&amp;gt;
        {
            var link = await GetQrCode(ct);
            var model = new { QrPath = link };
            return View["Index", model];
        };
    }

    private async Task&amp;lt;string&amp;gt; GetQrCode(CancellationToken ct)
    {
        var client = new HttpClient();
        client.DefaultRequestHeaders.Add("X-Mashape-Authorization", "oEzDRdFudTpsuLtmgewrIGcuj08tK7PI");
        var response = await client.GetAsync(
                "https://mutationevent-qr-code-generator.p.mashape.com/generate.php?content=http://www.nancyfx.org&amp;amp;type=url"
                , ct);

        var stringContent = await response.Content.ReadAsStringAsync();
        ct.ThrowIfCancellationRequested();
        dynamic model = JsonObject.Parse(stringContent);

        return model["image_url"];
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we have a GET that returns a view and then an async POST that &lt;code&gt;await&lt;/code&gt;'s a &lt;code&gt;GetQrCode&lt;/code&gt; method that returns a &lt;code&gt;Task&amp;lt;string&amp;gt;&lt;/code&gt; or &lt;code&gt;string&lt;/code&gt; depending on how you interpret that specific .NET 4.5 behaviour.  At this point the thread can be used to process another request whilst it waits to be notified that &lt;code&gt;GetQrCode&lt;/code&gt; has finished.  &lt;/p&gt;

&lt;p&gt;The &lt;code&gt;GetQrCode&lt;/code&gt; method uses a &lt;code&gt;HttpClient&lt;/code&gt; to execute an API call to get a QR code which will link to http://www.nancyfx.org.  Our method will then return the location of the QR code image. &lt;/p&gt;

&lt;p&gt;Anything marked with &lt;code&gt;async&lt;/code&gt; needs an &lt;code&gt;await&lt;/code&gt; otherwise it will just execute synchronously.  In our method we execute an asynchronous call (just like our async route) to the API so we &lt;code&gt;await&lt;/code&gt; it and once we do we &lt;code&gt;await&lt;/code&gt; reading the response as &lt;code&gt;string&lt;/code&gt; and then parse the JSON content to a dynamic type.  &lt;/p&gt;

&lt;p&gt;We return a string from the method but the compiler will actually convert that to a &lt;code&gt;Task&amp;lt;string&amp;gt;&lt;/code&gt; for us.  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;"An await expression does not block the thread on which it is executing. Instead, it causes the compiler to sign up the rest of the async method as a continuation on the awaited task. Control then returns to the caller of the async method. When the task completes, it invokes its continuation, and execution of the async method resumes where it left off."&lt;a href="http://msdn.microsoft.com/en-us/library/vstudio/hh156528.aspx"&gt;MSDN&lt;/a&gt;  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once the &lt;code&gt;GetQrCode&lt;/code&gt; returns we set up a simple anonymous type with a QrPath property that is set to the result of &lt;code&gt;GetQrCode&lt;/code&gt; and we return our view.  &lt;/p&gt;

&lt;p&gt;In the view we then have some code that determines when to show the QR image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@if (Model != null)
{
    &amp;lt;img alt="QR Code" src="Model.QrPath"/&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can view this code as a running application in my Github repository &lt;a href="https://github.com/jchannon/Nancy.Demo.Async"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;The Guts of it&lt;/h2&gt;

&lt;p&gt;If you want a bit more of an understanding how &lt;code&gt;async/await&lt;/code&gt; works in Nancy then lets take a look at the code below that is located in the &lt;a href="https://github.com/NancyFx/Nancy/blob/master/src/Nancy/Routing/DefaultRouteInvoker.cs"&gt;DefaultRouteInvoker&lt;/a&gt; class:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public Task&amp;lt;Response&amp;gt; Invoke(Route route, CancellationToken cancellationToken, DynamicDictionary parameters, NancyContext context)
{
    var tcs = new TaskCompletionSource&amp;lt;Response&amp;gt;();

    var result = route.Invoke(parameters, cancellationToken);

    result.WhenCompleted(
       completedTask =&amp;gt;
        {
            var returnResult = completedTask.Result;
            ...
        }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our route that we are executing is invoked and as we know from above the captured parameters on the route eg/customer/{id} and a CancellationToken is passed in.  We can then see the customized &lt;code&gt;ContinueWith&lt;/code&gt; known as &lt;code&gt;WhenCompleted&lt;/code&gt; is setup to resolve what our route returns be that a view or data.  So as we know when using async we need to return a &lt;code&gt;Task&amp;lt;T&amp;gt;&lt;/code&gt; (you can return void and have a method marked as &lt;code&gt;async&lt;/code&gt; but those should only be used for fire-and-forget methods like event handlers) and in our routes case it returns a Task&lt;Nancy.Responses.Negotiation.Negotiator&gt;.  The DefaultRouteInvoker then carries on to do its thing getting ready to render a view or serialize our data.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So there's the scope of async/await in Nancy, all the goodies of Nancy still apply now with the addition of asynchronous routes.  If you have read this blog post and not used Nancy before please read &lt;a href="http://blog.jonathanchannon.com/2012/12/19/why-use-nancyfx/"&gt;this blog post&lt;/a&gt; which reminds me I need to add "ASYNC ALL THE THINGS" to the list of reasons to use Nancy!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/08/22/a-quick-look-at-visual-node/</guid><link>http://blog.jonathanchannon.com/2013/08/22/a-quick-look-at-visual-node/</link><a10:author><a10:name /></a10:author><category>javascript</category><category>visual node</category><category>visual studio</category><title>A quick look at Visual Node</title><description>&lt;p&gt;I came across &lt;a href="http://www.visualnode.info/"&gt;Visual Node&lt;/a&gt; a few months ago and was excited by the looks of it.  For those that didn't click that link, it basically brings the power of Visual Studio debugging to a node.js app.  You can write your node.js app in Visual Studio, fire up the debugger by hitting F5 and use breakpoints and watches to see what's going on.  &lt;/p&gt;

&lt;p&gt;The hipster in me is screaming saying "You should be using Sublime Text and &lt;a href="https://github.com/dannycoates/node-inspector"&gt;node-inspector&lt;/a&gt; for debugging" but to be honest I found it a bit hackety-hack and it seemed a bit odd debugging my server app in Chrome but maybe that's just something I need to get over.  JavaScript is getting a huge surge in popularity recently so its your duty as a developer to investigate this.  I want to learn and understand JS better but I always get frustrated with it after 10mins and swear that I'm never going to touch a dynamic language again, "give me a statically typed language every time with a compiler".  I have a bit of a Jekyll and Hyde situation going on that I need to overcome.&lt;/p&gt;

</description><pubDate>Wed, 21 Aug 2013 23:00:00 Z</pubDate><a10:updated>2013-08-21T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I came across &lt;a href="http://www.visualnode.info/"&gt;Visual Node&lt;/a&gt; a few months ago and was excited by the looks of it.  For those that didn't click that link, it basically brings the power of Visual Studio debugging to a node.js app.  You can write your node.js app in Visual Studio, fire up the debugger by hitting F5 and use breakpoints and watches to see what's going on.  &lt;/p&gt;

&lt;p&gt;The hipster in me is screaming saying "You should be using Sublime Text and &lt;a href="https://github.com/dannycoates/node-inspector"&gt;node-inspector&lt;/a&gt; for debugging" but to be honest I found it a bit hackety-hack and it seemed a bit odd debugging my server app in Chrome but maybe that's just something I need to get over.  JavaScript is getting a huge surge in popularity recently so its your duty as a developer to investigate this.  I want to learn and understand JS better but I always get frustrated with it after 10mins and swear that I'm never going to touch a dynamic language again, "give me a statically typed language every time with a compiler".  I have a bit of a Jekyll and Hyde situation going on that I need to overcome.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;When I saw Visual Node it appeared to my statically typed side. "Ooooh, proper debugging, this looks interesting".  So I signed up to be kept up to date when they were ready for beta testers and yesterday I got my email saying I could download a private beta version and give it a whirl. So that's what I did.&lt;/p&gt;

&lt;p&gt;I went over to their &lt;a href="http://www.visualnode.info/readme"&gt;README&lt;/a&gt; page which explained how to install it and some of the features.  I installed a VSIX which gives me a project template to choose from when creating a project in Visual Studio(VS).&lt;/p&gt;

&lt;p&gt;&lt;img src="http://www.visualnode.info/images/readme/new-project.png" alt="Project Template" /&gt;&lt;/p&gt;

&lt;p&gt;Selecting this gives you a basic http server app template.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/projectlayout.PNG" alt="Project Layout" /&gt;&lt;/p&gt;

&lt;p&gt;I like that they have tried to bring a &lt;a href="http://www.nuget.org/"&gt;NuGet&lt;/a&gt; style dialog for searching packages in &lt;a href="https://npmjs.org/"&gt;NPM&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://www.visualnode.info/images/readme/package.png" alt="NPM Picture" /&gt;&lt;/p&gt;

&lt;p&gt;This brings a sense of familiarity to Visual Studio users which is great. It orders its results alphabetically. I found the searching a bit slow but I'm not sure if that's down to my internet connection or how NPM handles searching.&lt;/p&gt;

&lt;p&gt;The code in the template is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Load the http module to create an http server.
var http = require('http');

// Configure our HTTP server to respond with Hello World to all requests.
var server = http.createServer(function (request, response) {
    response.writeHead(200, { "Content-Type": "text/plain" });
    response.end("Hello World\n");
});

// Listen on port 8000, IP defaults to 127.0.0.1
server.listen(8000);

// Put a friendly message on the terminal
console.log("Server running at http://127.0.0.1:8000/");
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then pressed F5 to see what happened.&lt;/p&gt;

&lt;p&gt;It fires up a console app and a browser:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/console.png" alt="Console" /&gt;
&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/helloworldbrowser.PNG" alt="Browser" /&gt;&lt;/p&gt;

&lt;p&gt;I then went back to Visual Studio and put a breakpoint on the &lt;code&gt;response.end("Hello World\n");"&lt;/code&gt; to see what would happen when I refreshed my browser.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/debugging.png" alt="Debugging" /&gt;&lt;/p&gt;

&lt;p&gt;It stopped on the breakpoint and gives me information about the objects in scope etc and let me step into the current line.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/f11.png" alt="F11" /&gt;&lt;/p&gt;

&lt;p&gt;That's a pretty basic hello world style of seeing what Visual Node can do but that static language side of me really likes the look of this and the potential it can bring. I believe &lt;a href="http://www.jetbrains.com/webstorm"&gt;WebStorm&lt;/a&gt; also provides node.js debugging so check that out but for pure familiarity reasons I like the look of this.&lt;/p&gt;

&lt;p&gt;For a better understanding here's a video:&lt;/p&gt;

&lt;iframe width="560" height="315" src="//www.youtube.com/embed/gXGLGVWWwKI" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/07/03/modifying-files-within-git-history/</guid><link>http://blog.jonathanchannon.com/2013/07/03/modifying-files-within-git-history/</link><a10:author><a10:name /></a10:author><category>git</category><category>github</category><category>st2</category><category>sublime text 2</category><title>Modifying files within Git history</title><description>&lt;p&gt;If you have been doing code changes and committing as you go and then look back at the changes you may see something you don’t like the look of. Assuming no-one has a copy of your code changes you can go back and modify the files at a certain point in time within your commit history.&lt;/p&gt;

&lt;p&gt;I use Git Bash by default but the editor sucks compared to Sublime Text so first things first lets setup the Git editor.&lt;/p&gt;

&lt;p&gt;For Sublime Text run this in the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git config --global core.editor "'C:/Program Files/Sublime Text 2/sublime_text.exe'"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next up is finding the commit id you want to go back to, to edit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make a note of the parent commit id of the commit you want to edit.&lt;/p&gt;

</description><pubDate>Tue, 02 Jul 2013 23:00:00 Z</pubDate><a10:updated>2013-07-02T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;If you have been doing code changes and committing as you go and then look back at the changes you may see something you don’t like the look of. Assuming no-one has a copy of your code changes you can go back and modify the files at a certain point in time within your commit history.&lt;/p&gt;

&lt;p&gt;I use Git Bash by default but the editor sucks compared to Sublime Text so first things first lets setup the Git editor.&lt;/p&gt;

&lt;p&gt;For Sublime Text run this in the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git config --global core.editor "'C:/Program Files/Sublime Text 2/sublime_text.exe'"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next up is finding the commit id you want to go back to, to edit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make a note of the parent commit id of the commit you want to edit.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Run this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git rebase --interactive 1185cb0de5d5e5b4c79f83a0c51ed06a5d22d7c4^
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will open Sublime Text and you’ll see lines similar to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pick d3adb33 My Commit message
pick d5bdb67 Other Commit message
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change “pick” to “edit” on the commit you want to modify, save and exit&lt;/p&gt;

&lt;p&gt;You can then make the changes to your files and when ready run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git commit --all --amend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sublime Text will open again so you can edit the commit message, just save, exit and run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git rebase --continue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We’re done!&lt;/p&gt;

&lt;p&gt;Your previous commit has been amended and you’re back to where you were with the latest commits ready to be pushed.  &lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/05/29/why-net-should-become-independent/</guid><link>http://blog.jonathanchannon.com/2013/05/29/why-net-should-become-independent/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>career</category><category>community</category><category>oss</category><title>Why .Net should become independent!</title><description>&lt;p&gt;I recently changed jobs and as usual was at the mercy of recruitment agents. The advert for my job contained things like ASP.Net MVC, Entity Framework &amp;amp; TFS (luckily there were other cool pieces of technology on that list and what the role entailed interested me and once I had joined the company I saw they were open to other tech/approaches that made people’s workflow and output more beneficial to developers as well as the company. In fact I implemented an API written in &lt;a href="http://nancyfx.org/"&gt;Nancy&lt;/a&gt; on my first day and paved the way for Git in the first week).&lt;/p&gt;

&lt;p&gt;My point being that whenever I hear from recruiters or look for jobs all the adverts basically list the full Microsoft stack. I recently heard from a friend who runs his own company that he gave his CV to a recruitment agent and was basically rejected because his .Net experience was not MS based enough. I know his .Net skills are very good but because those .Net skills were put to good use using OSS projects he is unlikely to get a job in the mainstream .Net market.&lt;/p&gt;

&lt;p&gt;These adverts usually contain a list of tech/experience similar to: MVC, Webforms, Visual Studio, SQL Server, Entity Framework, WCF, LINQ.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What’s the common denominator here?&lt;br /&gt;
&lt;strong&gt;A:&lt;/strong&gt; They are all owned by Microsoft.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What operating system do these all run on?&lt;br /&gt;
&lt;strong&gt;A:&lt;/strong&gt; Microsoft Windows&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What framework and programming language do they run on?&lt;br /&gt;
&lt;strong&gt;A:&lt;/strong&gt; Microsoft .Net and C#&lt;/p&gt;

&lt;p&gt;Spot a pattern?&lt;/p&gt;

&lt;p&gt;So lets point out the obvious, the operating system, the frameworks, the language, the tooling and the data storage are all owned and implemented by one company (and they say Apple tries to lock its users in).&lt;/p&gt;

</description><pubDate>Tue, 28 May 2013 23:00:00 Z</pubDate><a10:updated>2013-05-28T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I recently changed jobs and as usual was at the mercy of recruitment agents. The advert for my job contained things like ASP.Net MVC, Entity Framework &amp;amp; TFS (luckily there were other cool pieces of technology on that list and what the role entailed interested me and once I had joined the company I saw they were open to other tech/approaches that made people’s workflow and output more beneficial to developers as well as the company. In fact I implemented an API written in &lt;a href="http://nancyfx.org/"&gt;Nancy&lt;/a&gt; on my first day and paved the way for Git in the first week).&lt;/p&gt;

&lt;p&gt;My point being that whenever I hear from recruiters or look for jobs all the adverts basically list the full Microsoft stack. I recently heard from a friend who runs his own company that he gave his CV to a recruitment agent and was basically rejected because his .Net experience was not MS based enough. I know his .Net skills are very good but because those .Net skills were put to good use using OSS projects he is unlikely to get a job in the mainstream .Net market.&lt;/p&gt;

&lt;p&gt;These adverts usually contain a list of tech/experience similar to: MVC, Webforms, Visual Studio, SQL Server, Entity Framework, WCF, LINQ.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What’s the common denominator here?&lt;br /&gt;
&lt;strong&gt;A:&lt;/strong&gt; They are all owned by Microsoft.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What operating system do these all run on?&lt;br /&gt;
&lt;strong&gt;A:&lt;/strong&gt; Microsoft Windows&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What framework and programming language do they run on?&lt;br /&gt;
&lt;strong&gt;A:&lt;/strong&gt; Microsoft .Net and C#&lt;/p&gt;

&lt;p&gt;Spot a pattern?&lt;/p&gt;

&lt;p&gt;So lets point out the obvious, the operating system, the frameworks, the language, the tooling and the data storage are all owned and implemented by one company (and they say Apple tries to lock its users in).&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;There is a vicious circle here, MS provide the full stack and companies feel comfortable with this as MS are a large company and therefore must be reliable. They also believe they have someone to shout at or sue if things go wrong and someone to call for support if they get stuck. Remind me what is the direct phone number to speak to someone about the issue I’m having with my LINQ statement or asynchronous method? You don’t have one you use Stackoverflow? Intersting. The circles continues as developers learn this stack to get jobs and recruiters are instructed to find people that list the MS items on their CV.&lt;/p&gt;

&lt;p&gt;This circle only happens in .Net. Lets look at other frameworks and tools for another language that implement similar things in the tech list that companies are looking for.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Operating System&lt;/strong&gt; : Linux&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: node.js&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web Framework&lt;/strong&gt; : express.js&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language&lt;/strong&gt; : JavaScript&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tooling&lt;/strong&gt; : Sublime Text or WebStorm or Notepad++or [insert many others]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Storage&lt;/strong&gt; : PostgreSQL&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ORM&lt;/strong&gt;: node-postgres&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LINQ&lt;/strong&gt; : linq.js&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web Services&lt;/strong&gt; : restify&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spot a pattern?&lt;/p&gt;

&lt;p&gt;They are all completely independent from each other and there are more than one option for each that are more than acceptable. Acceptable being the keyword. If I write the same list for alternative frameworks/libraries/tooling for .Net this is what you end up with but the question is are they acceptable to the majority of companies?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Operating System&lt;/strong&gt; : Linux&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web Framework&lt;/strong&gt; : Nancy&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tooling&lt;/strong&gt;: Xamarin&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Storage&lt;/strong&gt;: PostgreSQL&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ORM&lt;/strong&gt;: Simple.Data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web Services&lt;/strong&gt;: Service Stack&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m not saying companies have to chuck out Windows and Visual Studio (from my own experience Visual Studio is better than Xamarin) but at least be aware of them, the same goes for developers. These alternative frameworks and libraries may offer something a lot better than what MS give you but if you don’t look you won’t know.&lt;/p&gt;

&lt;p&gt;One of the issues is getting companies/developers to look. People have said to me in the past that why should I bother learning ‘X’ if I can’t get a job in it or whatever the majority of developers use I will stick with that. I can understand where they are coming from because if companies only want to employ people that use libraries that are MS owned there is no incentive for them even if they offer better performance or solutions. These kind of attitudes of are in turn effecting community events in that the talks are often MS based and those in the community that put forward talks on alternative approaches are not getting enough votes to make the cut because people want to learn the MS stuff as they feel it will keep them secure in their jobs.&lt;/p&gt;

&lt;p&gt;The only solution of breaking the vicious circle is for companies to change their attitudes in terms of tooling, frameworks and libraries that they use. Microsoft recently made a change to their &lt;a href="http://www.asp.net/mvc/open-source"&gt;OSS page&lt;/a&gt; which now lists alternative web frameworks on the .Net framework which is great and thanks to Scott Hanselman for doing this. This helps educate those developers/companies that are MS facing because now they have a bit more exposure to alternative/better libraries. In turn this may hopefully help companies realise that there are alternative libraries that are not one man bands and likely to disappear overnight and do offer better ways of providing solutions to their problems.&lt;/p&gt;

&lt;p&gt;This is the answer to why .Net should become independent. You are not tied to one company for all your resources. You can adopt many other libraries and tools better than MS provided ones and allow the frameworks, tooling, libraries and community to become more feature rich with alternatives. The more ideas/projects out there which are used will inspire more projects which may provide better solutions. I can already here people saying “that’s crazy, at least if we follow MS we only have one thing to learn”. My response “Are you sheep, can you not make your own mind up?”. Take a look at some of the alternatives and see if you like it and see if it solves a problem, if it doesn’t fair enough but at least you looked and tried. I believe its a developer’s responsibility to push for “change”, to open eyes and widen opinions on the non-MS stuff although there is a balancing act to use all the new shiny stuff as you don’t want to get burnt but need to use new libraries otherwise you will never know. This is part of continual learning and progress and writing spikes to see if things fit. I’m not saying that if you go off and write your own library it will automatically get main stream adoption because there can only be a handful of great libraries trying to solve the same problem but if they exist and there is no restriction for you as a developer/company you may actually gain by looking elsewhere.&lt;/p&gt;

&lt;p&gt;As a open minded developer and not a MS facing developer I have found myself having to learn the technologies on the list of what recruiters want as well as learn other libraries that interest me and provide better solutions. This is the way of the alternative .Net developer but its a pain the ass. If recruiters started advertising experience for Nancy, Simple.Data and ServiceStack it would make our lives easier! Will the attitude change I speak of happen overnight? Probably not, its been going on for years. Should I wait for the culture change or move languages? You would think that I should change language if I’m not happy with the culture of the MS stack but I enjoy C# and .Net and I enjoy using the OSS projects out there, its just when looking for jobs I need to find a company that is willing to look at new libraries and not just Microsoft’s approach. This is very hard to do unless you are in London. Companies there have changed their attitudes and are more open to using alternative libraries and do advertise for people with experience in the alternative libraries but for the rest of the UK they mostly remain MS orientated.&lt;/p&gt;

&lt;p&gt;Here comes the spanner in the works, the available jobs for Python, JVM languages or node.js are virtually non-existent so whilst as much as I bitch and moan that companies should look at MS alternatives I pretty much have to deal with it because the majority of jobs are for .Net with a MS stack. I could go the way of my friend mentioned earlier and work for myself writing software in using the tech/tooling I want but what if I run out of contracts and need to get back into the job market? I will face the same issue as he did.&lt;/p&gt;

&lt;p&gt;There is no perfect job out there regardless of tech/tools used (if you say you have it then you are either a liar or very very lucky) which is something I have recently accepted but if the majority of jobs are on the MS stack and there are great alternative .Net projects waiting to be discovered can we please ask our companies to open their eyes and take a look at what else might be out there.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/04/30/mocking-httpcontext-with-fake-it-easy/</guid><link>http://blog.jonathanchannon.com/2013/04/30/mocking-httpcontext-with-fake-it-easy/</link><a10:author><a10:name /></a10:author><category>.net</category><category>asp.net mvc</category><category>fake it easy</category><category>nancyfx</category><category>oss</category><category>unit testing</category><category>xunit</category><title>Mocking HttpContext with Fake It Easy</title><description>&lt;p&gt;Lets start with the conclusion first and say “use &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt; for your web applications and APIs” as its brilliant!&lt;/p&gt;

&lt;p&gt;If you want to continue reading lets crack on.&lt;/p&gt;

&lt;p&gt;I’m currently working on a ASP.Net MVC project and one of the controller methods writes directly to the Response, &lt;em&gt;eg. Response.Write(“How will I mock thee?”);&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Having moved over to &lt;a href="http://xunit.codeplex.com/"&gt;xUnit&lt;/a&gt; and &lt;a href="https://github.com/FakeItEasy/FakeItEasy"&gt;FakeItEasy&lt;/a&gt; recently I wanted to write a unit or integration test depending how you see it to assert against the Http Response.&lt;/p&gt;

&lt;p&gt;Doing this is no easy feat with MVC (with Nancy its all &lt;a href="https://github.com/NancyFx/Nancy/wiki/Testing-your-application"&gt;done for you&lt;/a&gt;) and you have to mock a lot of things. I’m hoping that in later releases this will be fixed because I know that ASP.Net Web API has made things a bit easier for testing (and wrote a testing library for it) so I assume the two projects will use bits of each other or their roadmap will merge.&lt;/p&gt;

</description><pubDate>Mon, 29 Apr 2013 23:00:00 Z</pubDate><a10:updated>2013-04-29T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Lets start with the conclusion first and say “use &lt;a href="http://nancyfx.org"&gt;Nancy&lt;/a&gt; for your web applications and APIs” as its brilliant!&lt;/p&gt;

&lt;p&gt;If you want to continue reading lets crack on.&lt;/p&gt;

&lt;p&gt;I’m currently working on a ASP.Net MVC project and one of the controller methods writes directly to the Response, &lt;em&gt;eg. Response.Write(“How will I mock thee?”);&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Having moved over to &lt;a href="http://xunit.codeplex.com/"&gt;xUnit&lt;/a&gt; and &lt;a href="https://github.com/FakeItEasy/FakeItEasy"&gt;FakeItEasy&lt;/a&gt; recently I wanted to write a unit or integration test depending how you see it to assert against the Http Response.&lt;/p&gt;

&lt;p&gt;Doing this is no easy feat with MVC (with Nancy its all &lt;a href="https://github.com/NancyFx/Nancy/wiki/Testing-your-application"&gt;done for you&lt;/a&gt;) and you have to mock a lot of things. I’m hoping that in later releases this will be fixed because I know that ASP.Net Web API has made things a bit easier for testing (and wrote a testing library for it) so I assume the two projects will use bits of each other or their roadmap will merge.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;I found that there a quite a lot of samples with Moq but nothing for Fake It Easy(FIE) so I checked in at the &lt;a href="https://jabbr.net/#/rooms/fakeiteasy"&gt;FIE Jabbr room&lt;/a&gt; and got some help and worked through some ideas and below is the result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Controller
public class MyController : Controller
{
    [HttpPost]
    public void Index()
    {
        Response.Write("This is fiddly");
        Response.Flush();
    }
}

//Unit Test
[Fact]
public void Should_contain_fiddly_in_response()
{
    var sb = new StringBuilder();

    var formCollection = new NameValueCollection();
    formCollection.Add("MyPostedData", "Boo");

    var request = A.Fake&amp;lt;HttpRequestBase&amp;gt;();
    A.CallTo(() =&amp;gt; request.HttpMethod).Returns("POST");
    A.CallTo(() =&amp;gt; request.Headers).Returns(new NameValueCollection());
    A.CallTo(() =&amp;gt; request.Form).Returns(formCollection);
    A.CallTo(() =&amp;gt; request.QueryString).Returns(new NameValueCollection());

    var response = A.Fake&amp;lt;HttpResponseBase&amp;gt;();
    A.CallTo(() =&amp;gt; response.Write(A&amp;lt;string&amp;gt;.Ignored)).Invokes((string x) =&amp;gt; sb.Append(x));

    var mockHttpContext = A.Fake&amp;lt;HttpContextBase&amp;gt;();
    A.CallTo(() =&amp;gt; mockHttpContext.Request).Returns(request);
    A.CallTo(() =&amp;gt; mockHttpContext.Response).Returns(response);

    var controllerContext = new ControllerContext(mockHttpContext, new RouteData(), A.Fake&amp;lt;ControllerBase&amp;gt;());

    var myController = GetController();
    myController.ControllerContext = controllerContext;


    myController.Index();

    Assert.Contains("fiddly", sb.ToString());
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we are not running this against a live web server we need to mock everything from base controller types to requests, responses and everything in between. My example initiates a bit more than is required ie. querystring and headers but hopefully it demonstrates what’s needed.&lt;/p&gt;

&lt;p&gt;Firstly we create an instance of a StringBuilderthat will store the response information that we can assert against. We setup a NameValueCollection to add keys/values for posted data, we could do the same for headers etc if we wanted.&lt;/p&gt;

&lt;p&gt;We then create an instance of a fake HttpRequestBase using FIE and setup all the relevant request properties.&lt;/p&gt;

&lt;p&gt;We then create an instance of a fake HttpResponseBase and configure a callback that is invoked when our controller calls Response.Write. We also configure it to watch any calls to Response.Write with any string using the FIE syntax of A.Ignored, you could change it so it only looks for specific argument contents if you wanted. When the method is called it then takes the argument and adds it to the StringBuilder.&lt;/p&gt;

&lt;p&gt;We then create a fake instance of HttpContextBase and assign the properties of Request and Response to the previously setup fakes.&lt;/p&gt;

&lt;p&gt;We then have to create a ControllerContext and pass the fake Http context, a route collection and a fake ControllerBase which the controller under test inherits off. We then assign the controller context to an instance of the controller class we are testing.&lt;/p&gt;

&lt;p&gt;We can now finally call the method under test and assert the results.&lt;/p&gt;

&lt;p&gt;I would obviously recommend you put the fake setup in a factory method if you have multiple tests you class to prevent duplication. You can obviously then add header, querystring, form method arguments if you want the context populated with that kind of information.&lt;/p&gt;

&lt;p&gt;Hope this helps anyone in a similar situation and provides a point of reference for the Fake It Easy project.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/04/17/up-running-with-typescript-and-webstorm/</guid><link>http://blog.jonathanchannon.com/2013/04/17/up-running-with-typescript-and-webstorm/</link><a10:author><a10:name /></a10:author><category>javascript</category><category>oss</category><category>osx</category><category>typescript</category><category>webstorm</category><title>Up &amp; Running with TypeScript and WebStorm</title><description>&lt;p&gt;I love my iMac and I’m on a mission to find a language I enjoy that I can use my Mac for (no Windows fan boy jokes please). There’s something in my mind I associate with work and my Windows laptop. Therefore, I don’t feel to excited about getting my laptop out of my bag in the evenings/weekends to play with other stuff.&lt;/p&gt;

&lt;p&gt;As I want to broaden my knowledge I wanted to find something ideally statically typed (although I’m currently looking into Python) that would work on OS X. I’ve said previously that JavaScript seems the way to go in my current situation so I thought I’d take a look at TypeScript and also use &lt;a href="http://www.jetbrains.com/webstorm/"&gt;WebStorm&lt;/a&gt; from Jetbrains as my IDE seeing as I’ve heard so many great things about them and their products (don’t worry I use ReSharper).&lt;/p&gt;

&lt;h2&gt;TypeScript&lt;/h2&gt;

&lt;p&gt;So I went over to TypeScript’s &lt;a href="http://www.typescriptlang.org/"&gt;website&lt;/a&gt; and followed the Hello World type code examples under the “Learn” tab.&lt;/p&gt;

</description><pubDate>Tue, 16 Apr 2013 23:00:00 Z</pubDate><a10:updated>2013-04-16T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I love my iMac and I’m on a mission to find a language I enjoy that I can use my Mac for (no Windows fan boy jokes please). There’s something in my mind I associate with work and my Windows laptop. Therefore, I don’t feel to excited about getting my laptop out of my bag in the evenings/weekends to play with other stuff.&lt;/p&gt;

&lt;p&gt;As I want to broaden my knowledge I wanted to find something ideally statically typed (although I’m currently looking into Python) that would work on OS X. I’ve said previously that JavaScript seems the way to go in my current situation so I thought I’d take a look at TypeScript and also use &lt;a href="http://www.jetbrains.com/webstorm/"&gt;WebStorm&lt;/a&gt; from Jetbrains as my IDE seeing as I’ve heard so many great things about them and their products (don’t worry I use ReSharper).&lt;/p&gt;

&lt;h2&gt;TypeScript&lt;/h2&gt;

&lt;p&gt;So I went over to TypeScript’s &lt;a href="http://www.typescriptlang.org/"&gt;website&lt;/a&gt; and followed the Hello World type code examples under the “Learn” tab.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;After understanding the basics of it and wanting to learn more I spotted this demo code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Student {
    fullname : string;
    constructor(public firstname, public middleinitial, public lastname) {
        this.fullname = firstname %2B " " %2B middleinitial %2B " " %2B lastname;
    }
}

interface Person {
    firstname: string;
    lastname: string;
}

function greeter(person : Person) {
    return "Hello, " %2B person.firstname %2B " " %2B person.lastname;
}

var user = new Student("Jane", "M.", "User");

document.body.innerHTML = greeter(user);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is an interface that expects public properties of firstname and lastname. There is a Student class that has a constructor with arguments for firstname, middleinitial and lastname. TypeScript constructor arguments are shorthand for making the arguments properties on the object itself without having to code all that yourself. We then have a function called greeter that takes a method argument of our Person interface and uses the properties of it to return a string. We then create a new instance of the Student class and then call the greeter function with our instance. Woooaaahhh! I want static typing, WTF is going on here? Essentially TypeScript allows for &lt;a href="http://en.wikipedia.org/wiki/Duck_typing"&gt;Duck Typing&lt;/a&gt; where any object that makes calls to or uses properties that match another type, it will allow. Now technically you can do this in C# by using the “dynamic” keyword but I would still keep and one to one mapping even if it was dynamic/duck typed so not to confuse future users.&lt;/p&gt;

&lt;p&gt;It may be this example that I don’t like where properties are being un-used or the fact that the keyword “interface” is being used and the class is not statically implementing it but I guess this is how TypeScript uses interfaces ie/duck typing. I can’t think of a reason why you’d have an object that exposes properties that are not used when they have been shoe horned/duck typed into another with fewer properties etc, however, this might just be my statically typed mind not liking it and it should chill and get with the dynamic nature of things!&lt;/p&gt;

&lt;h2&gt;Webstorm&lt;/h2&gt;

&lt;p&gt;So in my quest to develop something on my Mac I chose Webstorm as my IDE. I haven’t heard one bad thing against Jetbrains so it must be good right? Must be simple and intuitive? Ummm, no. Webstorm has support for TypeScript and uses the computer’s version of TypeScript to compile *.ts files into *.js files. So I installed Node.js and TypeScript (&lt;em&gt;npm install -g typescript&lt;/em&gt;) and fired up Webstorm. Webstorm offers various project types to create new projects from but unfortunately no TypeScript one so I created an empty application. I added a *.ts file and Webstorm spots that this is a TypeScript file and wants to add a File Watcher. This means every time a change is made it recompiles in the background to produce *.js, that seems a bit keen for my liking and would prefer it on every time the file is saved but each to their own. I copied the above code into my my *.ts file. I then created a html file and referenced the JavaScript file that was created from the TypeScript compilation in the head section. (I’m using the term compilation but I guess it should be transpiling but you know what I’m getting at).&lt;/p&gt;

&lt;p&gt;Ok, we’re ready to go and debug and here comes the confusion!&lt;/p&gt;

&lt;p&gt;Being used to Visual Studio I thought it would start a web server instance up, open my system browser and away we go but unfortunately not. I was baffled but that doesn’t take much! I found the Run menu item and it had a Debug option and it then popped up window with a Edit Configuration option so I clicked that and it came up with the below:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/WebstormDebug-620x390.png" alt="WebStorm Debug" title="Webstorm Debug" /&gt;&lt;/p&gt;

&lt;p&gt;As there was no TypeScript option I assumed JavaScript was the option to go with. Under this option it has Local and Remote menu items to choose from. If you look at Local it autofills a path to the index.html on the filesystem, under the Remote option it shows the project structure and asks for a URL. I tried localhost:1234 but nothing so I went back to the filesystem option and clicked Apply but that doesn’t actually do anything.&lt;/p&gt;

&lt;p&gt;So went through it all again and saw the + icon in the screenshot and it gives you an option to add a JavaScript configuration and is a complete mirror of the above screenshot. Very odd. Clicked Apply, then tried to Debug again and it gave me the configuration I chose and then opened Chrome. Nothing. WTF!&lt;/p&gt;

&lt;p&gt;Had a rummage around WebStorm and saw a note about needing a Jetbrains Chrome extension to debug. So off to the Chrome Store I went and installed the plugin. Tried to Debug and it now opened a browser with my page so things are looking good. Went to the TypeScript file and put a breakpoint on a line but it wasn’t getting hit. I then put a breakpoint on a line in the JavaScript file and it got hit. Yay! But no content was in my page. Baffled again. WebStorm was saying that it couldn’t set the innerHTML property of null. Eh? How can a body be null? After a while of head scratching I sussed it. Those of you that are eagle eyed will have spotted that I put the script declaration in my head and therefore when it got executed there was no such thing as a body tag in the DOM yet. So I moved the script declaration to the body and bingo we have content. I should know better and should have put the script tag at the bottom of the body anyway but this was just a “Hello World” app so wasn’t too worried.&lt;/p&gt;

&lt;p&gt;Anyway it now works and either I’m too entrenched in Visual Studio and should learn to adapt to new IDE’s or I expect things to just work without having to &lt;a href="http://en.wikipedia.org/wiki/RTFM"&gt;RTFM&lt;/a&gt;. I must say if I have to read the manual on how to use pieces of software you have an instant fail however, as I’ve heard such great things about Jetbrains and Webstorm I’m willing to give it another go but it has me on the back foot already. I’m also still unsure how to set WebStorm up to use a local web server because debugging by pointing to a file system seems wrong, if anyone knows please let me know.&lt;/p&gt;

&lt;p&gt;I hope this helps someone else to get up and running with TypeScript and WebStorm, if not I’m sure there are plenty who’ve had a laugh from this article asking how one person can be so dumb :)&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/04/08/using-a-markdown-viewengine-with-nancy/</guid><link>http://blog.jonathanchannon.com/2013/04/08/using-a-markdown-viewengine-with-nancy/</link><a10:author><a10:name /></a10:author><category>.net</category><category>community</category><category>markdown</category><category>nancyfx</category><category>oss</category><title>Using a Markdown ViewEngine with Nancy</title><description>&lt;p&gt;Whilst using &lt;a href="http://stackoverflow.com"&gt;stackoverflow.com&lt;/a&gt; and &lt;a href="https://gist.github.com/"&gt;Github gists&lt;/a&gt; I’ve become a frequent user of &lt;a href="http://daringfireball.net/projects/markdown/"&gt;Markdown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For those of you that don’t know what Markdown is, its essentially a shorter/cleaner syntax that can be parsed to produce HTML. Below are a few examples:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Hello World!
##You're awesome!
The quick brown fox jumped over the lazy coder
What the **hell** is this?
This is an [example link](http://example.com/)

&amp;lt;h1&amp;gt;Hello World!&amp;lt;/h1&amp;gt;
&amp;lt;h2&amp;gt;You're awesome!&amp;lt;/h2&amp;gt;
&amp;lt;p&amp;gt;The quick brown fox jumped over the lazy coder&amp;lt;/p&amp;gt;
What the &amp;lt;strong&amp;gt;hell&amp;lt;/strong&amp;gt; is this?
This is an &amp;lt;a href="http://example.com/"&amp;gt; example link&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see more examples in the earlier link.&lt;/p&gt;

&lt;p&gt;When you’re writing a blog post or a lengthy page in your web app with lots of HTML it maybe easier to use Markdown as your preferred syntax. I currently use WordPress for my blog, it’s ok but its quite bloated for probably what I need. I looked into &lt;a href="http://calepin.co/"&gt;Calepin&lt;/a&gt; and &lt;a href="http://scriptogr.am/"&gt;Scriptogr.am&lt;/a&gt; as alternative blogging platforms but I felt it didn’t quite offer what I wanted but the approach was a good idea. It meant you could write a blog post and simply put the file in dropbox and it would appear on your blog.&lt;/p&gt;

</description><pubDate>Sun, 07 Apr 2013 23:00:00 Z</pubDate><a10:updated>2013-04-07T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Whilst using &lt;a href="http://stackoverflow.com"&gt;stackoverflow.com&lt;/a&gt; and &lt;a href="https://gist.github.com/"&gt;Github gists&lt;/a&gt; I’ve become a frequent user of &lt;a href="http://daringfireball.net/projects/markdown/"&gt;Markdown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For those of you that don’t know what Markdown is, its essentially a shorter/cleaner syntax that can be parsed to produce HTML. Below are a few examples:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Hello World!
##You're awesome!
The quick brown fox jumped over the lazy coder
What the **hell** is this?
This is an [example link](http://example.com/)

&amp;lt;h1&amp;gt;Hello World!&amp;lt;/h1&amp;gt;
&amp;lt;h2&amp;gt;You're awesome!&amp;lt;/h2&amp;gt;
&amp;lt;p&amp;gt;The quick brown fox jumped over the lazy coder&amp;lt;/p&amp;gt;
What the &amp;lt;strong&amp;gt;hell&amp;lt;/strong&amp;gt; is this?
This is an &amp;lt;a href="http://example.com/"&amp;gt; example link&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see more examples in the earlier link.&lt;/p&gt;

&lt;p&gt;When you’re writing a blog post or a lengthy page in your web app with lots of HTML it maybe easier to use Markdown as your preferred syntax. I currently use WordPress for my blog, it’s ok but its quite bloated for probably what I need. I looked into &lt;a href="http://calepin.co/"&gt;Calepin&lt;/a&gt; and &lt;a href="http://scriptogr.am/"&gt;Scriptogr.am&lt;/a&gt; as alternative blogging platforms but I felt it didn’t quite offer what I wanted but the approach was a good idea. It meant you could write a blog post and simply put the file in dropbox and it would appear on your blog.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;As you may be aware, I’m a great fan of &lt;a href="http://nancyfx.org/"&gt;Nancy&lt;/a&gt; and the team/contributors were discussing having something similar to Calepin for Nancy so users could upload a Markdown file as a pull request or community article or have something that uses Markdown that can be plugged into Jekyll.&lt;/p&gt;

&lt;p&gt;I initially looked into how hard it would be to convert Markdown into HTML with Nancy and as ever, very easy! I found some libraries that already provided the conversion process and decided upon &lt;a href="http://nuget.org/packages/MarkdownSharp/"&gt;MarkdownSharp&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I then thought we could use Markdown as a proper view engine like Razor. I was unsure of how to provide the ability of model binding, master pages etc but after speaking to &lt;a href="http://twitter.com/grumpydev"&gt;@grumpydev&lt;/a&gt; he suggested using the built in view engine with Nancy to provide that functionality.&lt;/p&gt;

&lt;p&gt;What this means is that you can have a Nancy route such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SampleModule : NancyModule
{
    public SampleModule()
    {
        Get["/"] = _ =&amp;gt;
        {
          var model = new UserModel{FirstName = "John", LastName = "Smith"};
          return View["Home", model];
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will find a Markdown file called Home.md or Home.markdown, translate the model and convert the markdown into a HTML view. You could define you markdown view like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Hi there!

My first name is @Model.FirstName and my last name is @Model.LastName
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This would then translate to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;h1&amp;gt;Hi there!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;My first name is John and my last name is Smith&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a simple demonstration but what it means is that you can now use the Markdown view engine instead of Razor if you so wished. Things like model binding, master pages, partials, model iteration etc etc can all be handled by the view engine and your views can be written in Markdown. To use features like partials and master pages you use the Super Simple View Engine syntax that as it sounds is super simple.&lt;/p&gt;

&lt;p&gt;When I wrote the Markdown view engine I also wrote a demo with the easy approach of Calepin in mind.&lt;/p&gt;

&lt;p&gt;I wrote a HTML view to act as my master page and then a route in Nancy that would take anything after the URL and find the view for it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Get["/{viewname}"] = parameters =&amp;gt;
{
    var popularposts = GetModel();

    dynamic postModel = new ExpandoObject();
    postModel.PopularPosts = popularposts;
    postModel.MetaData = popularposts.FirstOrDefault(x =&amp;gt; x.Slug == parameters.viewname);

    return View["Posts/" + parameters.viewname, postModel];
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the Markdown file I would reference the master page, some partials, the model and a custom section for meta data about the blog post:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Master['master']

@Tags
Date: 15/03/2013
Title: Readme
Tags: Nancy,Runtime
@EndTags

@Section['Content']

@Partial['blogheader', Model.MetaData];

# Readme!

## Markdown Viewengine

This Markdown Viewengine allows views to be written in Markdown.

* Full Model support
* Master page support
* Supports HTML within any MD content
* Simple call `return View["Home"]` for Nancy to render your MD file

## Markdown Blog Demo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This would then render a good looking website with all the features expected in a view engine. It would use the meta data provided to write out things like page titles and tags for a blog post. It would also mean you could use the approach to write your own blog using Nancy and Markdown.&lt;/p&gt;

&lt;p&gt;The GetModel() method used above checks a “Posts” directory for Markdown files and reads the metadata for them. What this means is that you can easily write a blog post using Nancy, Markdown and SuperSimpleViewEngine by just uploading the markdown file to a specific directory.&lt;/p&gt;

&lt;p&gt;Ideally I would like to get rid of the master page and partial references so you could just have Markdown content in your file but I haven’t figured that out yet. Hopefully someone else can use the Markdown view engine or take the demo app and find an alternative way to provide the simplicity that Calepin offers.&lt;/p&gt;

&lt;p&gt;Overall it’s another nice feature that Nancy can offer its users and I have enjoyed contributing to the OSS project and tinkering with code!&lt;/p&gt;

&lt;p&gt;The Markdown View Engine and Demo will be released in 0.17 of Nancy due very soon!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/04/08/using-datetime-in-c-and-sql/</guid><link>http://blog.jonathanchannon.com/2013/04/08/using-datetime-in-c-and-sql/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>datetime</category><category>sql</category><title>Using DateTime in C# and SQL</title><description>&lt;p&gt;I’m sure there are millions of blog posts out there that already discuss this but I think its worth noting down even if its just something for me to remember.&lt;/p&gt;

&lt;p&gt;Store your datetimes in UTC format into the database. Unfortunately this mean executing something like:
&lt;code&gt;myObject.ExpiryDate = TimeZoneInfo.ConvertTimeToUtc(dateTime, TimeZoneInfo.FindSystemTimeZoneById(“timezoneid of users”)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In every central place where you update/insert DateTime values on your objects you will need the above.&lt;/p&gt;

&lt;p&gt;When you display any DateTime information it must display as a local datetime value. You can do this by using     &lt;code&gt;myObject.ExpiryDate.ToLocalTime()&lt;/code&gt;&lt;/p&gt;

</description><pubDate>Sun, 07 Apr 2013 23:00:00 Z</pubDate><a10:updated>2013-04-07T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I’m sure there are millions of blog posts out there that already discuss this but I think its worth noting down even if its just something for me to remember.&lt;/p&gt;

&lt;p&gt;Store your datetimes in UTC format into the database. Unfortunately this mean executing something like:
&lt;code&gt;myObject.ExpiryDate = TimeZoneInfo.ConvertTimeToUtc(dateTime, TimeZoneInfo.FindSystemTimeZoneById(“timezoneid of users”)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In every central place where you update/insert DateTime values on your objects you will need the above.&lt;/p&gt;

&lt;p&gt;When you display any DateTime information it must display as a local datetime value. You can do this by using     &lt;code&gt;myObject.ExpiryDate.ToLocalTime()&lt;/code&gt;&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;An example of this is if you stored a DateTime that was converted to UTC and it stored April 8th 14:30 2013, when a user in USA was given the date in their web application they would see it as April 8th 10:30 but UK users would read it as April 8th 15:30&lt;/p&gt;

&lt;p&gt;If you have an existing application you need to make sure the server side logic regarding date storage converts to UTC and that your view layer converts to local time.&lt;/p&gt;

&lt;p&gt;Its risky but if your users are all on the same time zone and the database server and all operating systems are the same you don’t need to worry about this and can use DateTime.Now for everything.  &lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/04/02/the-old-development-language-switcheroo/</guid><link>http://blog.jonathanchannon.com/2013/04/02/the-old-development-language-switcheroo/</link><a10:author><a10:name /></a10:author><category>c#</category><category>career</category><category>Dart</category><category>F#</category><category>javascript</category><category>Kotlin</category><category>node.js</category><category>typescript</category><title>The Old Development Language Switcheroo</title><description>&lt;p&gt;As a C# developer I think I’m pretty safe in saying that its not going away any time soon however, its my opinion that to not become irrelevant you need to have options. Some may argue that its better to be master of one than a Jack of all trades but lets just say you can’t find your next job in the primary language you want to work in. What do you do?&lt;/p&gt;

&lt;p&gt;I have looked more into JavaScript recently and with that comes server and client scope for the use of the language so with some understanding of JS that would be probably the best bet for me however, as much as this may annoy some I think I actually prefer statically typed languages.&lt;/p&gt;

&lt;p&gt;Now I know there is &lt;a href="http://www.typescriptlang.org/"&gt;TypeScript&lt;/a&gt; that I could use for my &lt;a href="http://nodejs.org/"&gt;node.js&lt;/a&gt; apps but I see that as more of a workaround and not a core feature of JavaScript. I’m not saying anything bad against JS and the libraries/frameworks that I have used I like but I just find it easier to learn and know what I can do when I have decent intellisense showing me what I can do with my code.&lt;/p&gt;

</description><pubDate>Mon, 01 Apr 2013 23:00:00 Z</pubDate><a10:updated>2013-04-01T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;As a C# developer I think I’m pretty safe in saying that its not going away any time soon however, its my opinion that to not become irrelevant you need to have options. Some may argue that its better to be master of one than a Jack of all trades but lets just say you can’t find your next job in the primary language you want to work in. What do you do?&lt;/p&gt;

&lt;p&gt;I have looked more into JavaScript recently and with that comes server and client scope for the use of the language so with some understanding of JS that would be probably the best bet for me however, as much as this may annoy some I think I actually prefer statically typed languages.&lt;/p&gt;

&lt;p&gt;Now I know there is &lt;a href="http://www.typescriptlang.org/"&gt;TypeScript&lt;/a&gt; that I could use for my &lt;a href="http://nodejs.org/"&gt;node.js&lt;/a&gt; apps but I see that as more of a workaround and not a core feature of JavaScript. I’m not saying anything bad against JS and the libraries/frameworks that I have used I like but I just find it easier to learn and know what I can do when I have decent intellisense showing me what I can do with my code.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;You could argue that I could go back to my first language of Delphi or I could look into C++, Java or PHP but my counter argument might just be laughter. In my opinion these are old languages. They may be reliable for some needs but I feel its looking back to the past somewhat and not looking forward.&lt;/p&gt;

&lt;p&gt;I have recently come across &lt;a href="http://www.dartlang.org/"&gt;Dart&lt;/a&gt; and want to spend some time looking into it to see if we suit each other.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dart is a class-based, object-oriented language with lexical scoping, closures, and optional static typing. Dart helps you build structured modern web apps and is easy to learn for a wide range of developers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is also &lt;a href="http://www.tryfsharp.org/"&gt;F#&lt;/a&gt; which I need to look at as my Twitter timeline has more and more references to this language.&lt;/p&gt;

&lt;p&gt;There is also &lt;a href="http://kotlin.jetbrains.org/"&gt;Kotlin&lt;/a&gt; but that’s as much as I know about it, the name. I think its based on Java and that rings alarm bells for me but I’m sure &lt;a href="http://twitter.com/Cranialstrain"&gt;@Cranialstrain&lt;/a&gt; will put me right.&lt;/p&gt;

&lt;p&gt;There is also &lt;a href="http://golang.org/"&gt;Go&lt;/a&gt; and I’m sure there are many others that I could find but rightly or wrongly when deciding upon a language which lets be honest is a big investment you need to know there are the available jobs out there in that language. Its nice to learn new stuff and I highly recommend it but for me a whole new investment in a language has to warrant some pay offs such as employment.&lt;/p&gt;

&lt;p&gt;I can’t say for sure what language I’d use if I had to change, at present most likely JavaScript but probably that is based out of limited options on my part, maybe I need to become a better polyglot programmer and learn some more languages to have better “options” and not become irrelevant.&lt;/p&gt;

&lt;p&gt;Let me know what your primary language is and what you’d jump to if you had to.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/03/04/the-many-approaches-to-entity-framework/</guid><link>http://blog.jonathanchannon.com/2013/03/04/the-many-approaches-to-entity-framework/</link><a10:author><a10:name /></a10:author><category>.net</category><category>asp.net mvc</category><category>c#</category><category>entity framework</category><category>github</category><category>SRP</category><title>The many approaches to Entity Framework</title><description>&lt;p&gt;I recently had a need to look into using &lt;a href="http://www.asp.net/entity-framework"&gt;Entity Framework (EF)&lt;/a&gt; for a &lt;a href="http://www.asp.net/mvc"&gt;ASP.NET MVC&lt;/a&gt; project. In the past I have always used &lt;a href="http://www.toptensoftware.com/petapoco/"&gt;PetaPoco&lt;/a&gt; as my ORM of choice and with hearing nothing but bad things about EF I was a little sceptical. There are various ways to use EF, Code First being one of them and the easiest from what I can gather and luckily the approach I needed to get up to speed on. This means you can define your model in code and EF will turn that into tables in your database.&lt;/p&gt;

&lt;p&gt;The way I was going to see how EF could be architected in an application was to create a MVC application that provided CRUD capabilities for Customers, Orders and Products. Nothing complicated but something enough to see how EF could be fitted in with a MVC application. I would also like to use a unit of work pattern such as instantiate a model class, set some properties and call a save method. I would also like to keep the architecture well enough abstracted so that another ORM could take its place easily enough if needs be.&lt;/p&gt;

&lt;p&gt;I will list the various approaches I took investigating the how EF could be integrated. They are not in any chronological order.&lt;/p&gt;

</description><pubDate>Mon, 04 Mar 2013 00:00:00 Z</pubDate><a10:updated>2013-03-04T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I recently had a need to look into using &lt;a href="http://www.asp.net/entity-framework"&gt;Entity Framework (EF)&lt;/a&gt; for a &lt;a href="http://www.asp.net/mvc"&gt;ASP.NET MVC&lt;/a&gt; project. In the past I have always used &lt;a href="http://www.toptensoftware.com/petapoco/"&gt;PetaPoco&lt;/a&gt; as my ORM of choice and with hearing nothing but bad things about EF I was a little sceptical. There are various ways to use EF, Code First being one of them and the easiest from what I can gather and luckily the approach I needed to get up to speed on. This means you can define your model in code and EF will turn that into tables in your database.&lt;/p&gt;

&lt;p&gt;The way I was going to see how EF could be architected in an application was to create a MVC application that provided CRUD capabilities for Customers, Orders and Products. Nothing complicated but something enough to see how EF could be fitted in with a MVC application. I would also like to use a unit of work pattern such as instantiate a model class, set some properties and call a save method. I would also like to keep the architecture well enough abstracted so that another ORM could take its place easily enough if needs be.&lt;/p&gt;

&lt;p&gt;I will list the various approaches I took investigating the how EF could be integrated. They are not in any chronological order.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h2&gt;Approach One&lt;/h2&gt;

&lt;p&gt;My first approach was to create a generic repository, IRepository, to handle the the various crud methods needed and also a Save method. The implementation, Repository would take a IUnitOfWork constructor dependency which would expose the model type ie.Customer,Order, Product and allow EF to perform the CRUD actions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Repository : IRepository where T : class
{
    private readonly IUnitOfWork unitOfWork;
    private readonly DbSet entitySet;

    public Repository(IUnitOfWork unitOfWork)
    {
        this.unitOfWork = unitOfWork;
        entitySet = unitOfWork.GetEntitySet();
    }

    public void Add(T entity)
    {
        entitySet.Add(entity);        
    }
}

public interface IUnitOfWork
{
    void SaveChanges();
    DbSet GetEntitySet() where TEntity : class;
}

public OrderController(IRepository orderRepository, IRepository customerRepository )
{
    this.orderRepository = orderRepository;
    this.customerRepository = customerRepository;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two issues with this approach. Firstly, the IUnitOfWork exposes EF specific members which I don’t really want and secondly when a repository calls Save it will in turn call the SaveChanges method on EF which will save any changes across all repositories. This is because we have one EF database context per request as handled by our IOC container. To get around the saving issue we could have a new context per instance of the repository but I don’t think that is a good approach.&lt;/p&gt;

&lt;p&gt;The full code can be seen on the master branch at Github – &lt;a href="https://github.com/jchannon/EntityFrameworkMVC/tree/master/EntityFrameworkMVC"&gt;https://github.com/jchannon/EntityFrameworkMVC/tree/master/EntityFrameworkMVC
&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Approach Two&lt;/h2&gt;

&lt;p&gt;Another approach we could use is to remove IUnitOfWork altogether, keep the Save method on the repository but inject the context into the repository and still inject the repositories into the controllers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Repository : IRepository where T : class
{
    private readonly EntityFrameworkMvcDbContext context;
    private readonly DbSet entitySet;

    public Repository(EntityFrameworkMvcDbContext context)
    {
        this.context = context;
        entitySet = context.Set();
    }

    public void Add(T entity)
    {
        entitySet.Add(entity);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This approach is pretty close to what I think we would want and reasonably straight forward but I still don’t like the fact that the SaveChanges would effect every modified/added model across all repositories. If you’re happy to live with that then I think you’re good to go.&lt;/p&gt;

&lt;p&gt;The full code can be seen on the LessAbstraction branch at Github – &lt;a href="https://github.com/jchannon/EntityFrameworkMVC/tree/LessAbstraction/EntityFrameworkMVC"&gt;https://github.com/jchannon/EntityFrameworkMVC/tree/LessAbstraction/EntityFrameworkMVC
&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Approach Three&lt;/h2&gt;

&lt;p&gt;Another approach for using EF and building on from approach one was to remove the GetEntitySet from the IUnitOfWork interface but keep the Save method, inject the context into the repositories to allow them to add/update and inject the IUnitOfWork and repositories into the controllers so the repositories could use Add and then IUnitOfWork could be used to call SaveChanges.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public interface IUnitOfWork
{
    void SaveChanges();
}

public class Repository : IRepository where T : class
{
    private readonly DbSet entitySet;

    public Repository(EntityFrameworkMvcDbContext context)
    {
        entitySet = context.Set();
    }

    public void Add(T entity)
    {
        entitySet.Add(entity);  
    }
}

public OrderController(IUnitOfWork unitOfWork, IRepository orderRepository, IRepository customerRepository)
{
    this.unitOfWork = unitOfWork;
    this.orderRepository = orderRepository;
    this.customerRepository = customerRepository;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think that’s a decent approach however, I think it could be tidied up so that the controllers don’t get cluttered with too many constructor dependencies.&lt;/p&gt;

&lt;p&gt;The full code can be seen on the GenericContextPerRequest branch at Github – &lt;a href="https://github.com/jchannon/EntityFrameworkMVC/tree/GenericContextPerRequest/EntityFrameworkMVC"&gt;https://github.com/jchannon/EntityFrameworkMVC/tree/GenericContextPerRequest/EntityFrameworkMVC
&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Approach Four&lt;/h2&gt;

&lt;p&gt;This below was another approach and the approach I decided I would use. It uses all the understandings gained from the above samples and hopes to provide a decent way of using EF.&lt;/p&gt;

&lt;p&gt;The repositories are now exposed on the IUnitOfWork interface, the implementation takes the DBContext as a constructor dependency, the repository implementation also takes the DBContext to provide the CRUD features with EF and the controllers only have the IUnitOfWork injected into them.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public interface IUnitOfWork
{
    void SaveChanges();
    IRepository CustomerRepository { get; }
    IRepository ProductRepository { get; }
    IRepository OrderRepository { get; }
}

public OrderController(IUnitOfWork unitOfWork )
{
    this.unitOfWork = unitOfWork;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means we can keep our one database context per request, the controllers are tidied up so we no longer have numerous constructor dependencies and the repositories can be used to do CRUD and then SaveChanges can be called by IUnitOfWork when we have finished with the repositories.&lt;/p&gt;

&lt;p&gt;The only possible downside could be in time that we have numerous constructor dependencies on the IUnitOfWork implementation as well as numerous exposed repository properties on the IUnitOfWork interface. I don’t see this as a big issue as its only one class in your solution. If you didn’t want this to get too polluted you could create a repository factory and inject that into the controller and just have the SaveChanges on the IUnitOfWork interface. You wouldn’t want the IUnitOfWork to expose the repositories because as &lt;a href="https://twitter.com/daanleduc"&gt;@daanleduc&lt;/a&gt; pointed out to me that would violate the SRP.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I only touched the basics of implementing EF into a MVC app but these approaches did take quite a while to discover and after hearing all the negativity towards EF and drawing from my experience I think I could see why people have issues with it in a much larger application as I’m sure there are other caveats to using EF. The biggest issue I saw with it was not exposing a Save method for the model type you were using, instead you have to commit all changes across the database context. I feel this made it quite difficult to create a decent architecture. I can see in a large enterprise application why EF could be helpful in allowing it to query the database for you and create/modify the tables for you based on your model changes but I think for now in smaller applications I think I still prefer micro-ORMs like PetaPoco, Dapper or Simple.Data due to the control it gives you and also the ease in which you can abstract the database communication. As &lt;a href="https://twitter.com/Cranialstrain"&gt;@Cranialstrain&lt;/a&gt; says “the database is an implementation detail”. I’d also like to thank all those on Jabbr that helped me with the above approaches and long discussions had about Entity Framework!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/02/05/evaluating-knockoutjs-and-angularjs-part-1/</guid><link>http://blog.jonathanchannon.com/2013/02/05/evaluating-knockoutjs-and-angularjs-part-1/</link><a10:author><a10:name /></a10:author><category>angularjs</category><category>handlebars</category><category>javascript</category><category>knockoutjs</category><category>learning</category><category>oss</category><category>sammyjs</category><title>Evaluating KnockoutJS and AngularJS – Part 1</title><description>&lt;p&gt;As I stated in my earlier post &lt;a href="http://blog.jonathanchannon.com/2013/01/09/javascript-is-the-future-maybe/" title="JavaScript is the future…maybe!"&gt;“JavaScript is the future…maybe”&lt;/a&gt; so with that in mind I had to brush up my JS skills and get more involved with the language’s core concepts so after watching some videos and reading some articles I was ready to look at &lt;a href="http://knockoutjs.com"&gt;KnockoutJS&lt;/a&gt; and &lt;a href="http://angularjs.org/"&gt;AngularJS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Before I actually looked at these two I spent some time with &lt;a href="http://sammyjs.org/"&gt;SammyJS&lt;/a&gt; but realised afterwards it was mainly MVC based and not around 2-way binding that Knockout and Angular offer. However, I really liked it and it seemed very familiar and easy to use, the reason being it was inspired by &lt;a href="http://www.sinatrarb.com/"&gt;Sinatra&lt;/a&gt; which we all know &lt;a href="http://nancyfx.org/"&gt;Nancy&lt;/a&gt; was also inspired by and we also know how much &lt;a href="http://nancyfx.org/mvm.html"&gt;I like Nancy&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Getting to grips with any language or frameworks is tricky and the best way to do it is to write an application using it. The next difficult thing to overcome is an idea for writing an application. ToDo list’s are very common with JavaScript frameworks and there is a whole website for you to peruse but after looking at SammyJS and their docs they walk through writing a simple shopping basket so I thought I’d use that.&lt;/p&gt;

</description><pubDate>Tue, 05 Feb 2013 00:00:00 Z</pubDate><a10:updated>2013-02-05T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;As I stated in my earlier post &lt;a href="http://blog.jonathanchannon.com/2013/01/09/javascript-is-the-future-maybe/" title="JavaScript is the future…maybe!"&gt;“JavaScript is the future…maybe”&lt;/a&gt; so with that in mind I had to brush up my JS skills and get more involved with the language’s core concepts so after watching some videos and reading some articles I was ready to look at &lt;a href="http://knockoutjs.com"&gt;KnockoutJS&lt;/a&gt; and &lt;a href="http://angularjs.org/"&gt;AngularJS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Before I actually looked at these two I spent some time with &lt;a href="http://sammyjs.org/"&gt;SammyJS&lt;/a&gt; but realised afterwards it was mainly MVC based and not around 2-way binding that Knockout and Angular offer. However, I really liked it and it seemed very familiar and easy to use, the reason being it was inspired by &lt;a href="http://www.sinatrarb.com/"&gt;Sinatra&lt;/a&gt; which we all know &lt;a href="http://nancyfx.org/"&gt;Nancy&lt;/a&gt; was also inspired by and we also know how much &lt;a href="http://nancyfx.org/mvm.html"&gt;I like Nancy&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Getting to grips with any language or frameworks is tricky and the best way to do it is to write an application using it. The next difficult thing to overcome is an idea for writing an application. ToDo list’s are very common with JavaScript frameworks and there is a whole website for you to peruse but after looking at SammyJS and their docs they walk through writing a simple shopping basket so I thought I’d use that.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;If you just want to get to the code then feel free to take a look here for &lt;a href="https://github.com/jchannon/AngularShopping"&gt;Angular&lt;/a&gt; and here for &lt;a href="https://github.com/jchannon/KnockoutShopping"&gt;Knockout&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This blog post will concentrate on AngularJS.&lt;/p&gt;

&lt;h2&gt;Angular&lt;/h2&gt;

&lt;p&gt;After looking at the demos on the the home page and reading the docs I was ready to start. (I also got recommended &lt;a href="http://egghead.io/"&gt;egghead.io&lt;/a&gt; for lots of free AngularJS videos)&lt;/p&gt;

&lt;p&gt;I liked the look of Angular’s data bindings, it was very similar to &lt;a href="http://handlebarsjs.com/"&gt;HandlebarsJS&lt;/a&gt; which is a templating engine for JS and their specific attributes for doing certain things in the DOM seemed clean.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;input type="text" ng-model="yourName" placeholder="Enter a name here"&amp;gt;
&amp;lt;hr&amp;gt;
&amp;lt;h1&amp;gt;Hello {{yourName}}!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Angular uses “Controllers” and coming from experience with MVC it turned out they’re not really controllers but more ViewModels. Each Controller handles its own specific task for example, in CRUD you’d probably have a controller for each create, read, update and delete. I believe its possible to make it have one controller but that’s a bit more advanced!&lt;/p&gt;

&lt;p&gt;The data in the SammyJS demo comes via a data.json file so I duly created my file and followed the examples of wiring up a backend. This is when things start getting a bit more in depth with Angular opposed to a simple Hello World app. Angular provides its own internal IOC container which is recommended you use to keep things nicely separated and of course it then injects your controller’s dependencies for you. See &lt;a href="http://docs.angularjs.org/guide/di"&gt;here&lt;/a&gt; for more about the subject. There is a bit of configuration needed in various places and one missing item or spelling mistake and you’ll be stuck due to the nature of dynamic nature of JavaScript.&lt;/p&gt;

&lt;p&gt;I finally managed to get my wirings all sorted and my products displayed on screen but when I wanted to retrieve a single item this is when I started scratching my head. The recommended way of doing it was to use an abstracted class called $resource which allows you to query REST based services and obviously with just having one file this wasn’t going to work. $resource sits on top of $http but after playing with that I discarded my file and stuck it in memory for ease. If you’d like to see a $http demo with Nancy as a REST service see &lt;a href="http://blogs.lessthandot.com/index.php/WebDev/UIDevelopment/Javascript/angularjs"&gt;this blog post&lt;/a&gt; from &lt;a href="http://blogs.lessthandot.com/index.php/All/?disp=authdir&amp;amp;author=7"&gt;Christiaan Baes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So now with a bit of filtering I could find my product and display it on screen. I had a controller for listing my products and another controller for displaying one item and a dependency injected into both to allow me to retrieve the data I wanted to display and I also had my routing setup.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Routing
var app = angular.module('myCart', []).
        config(function ($routeProvider) {
            $routeProvider.
                when('/', {
                    controller: 'IndexController',
                    templateUrl: '/templates/list.html'
                }).
                when('/:id', {
                    controller: 'DetailController',
                    templateUrl: '/templates/detail.html'
                }).
                otherwise({ redirectTo: '/' });
        });

//Repository
app.factory('shoppingItemsService', function () {
        var data = [
            //My data is here!
        ];

        return {
            getItems: function () { return data; },
            getItem: function (itemId) {
                return data.filter(function (x) { return x.id === parseInt(itemId, 10); })[0];
            }
        };
    });

//List Controller
app.controller(
        'IndexController',
        function ($scope, shoppingItemsService) {
            $scope.items = shoppingItemsService.getItems();
        }
    );

//Detail Controller
app.controller(
        'DetailController',
        function ($scope, $routeParams, basketService, shoppingItemsService) {

            $scope.item = shoppingItemsService.getItem($routeParams.id);
            $scope.item.quantity = 1;

            $scope.item.basketCount = basketService.getCount();

            $scope.addItem = function () {
                basketService.addItem($scope.item);
            };
        }
    );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the app is based around a single page application (SPA) the main area on the screen would show the products or product selected however I had an area, the cart info, that showed the quantity of items in my basket outside this main area. After a lot of head scratching I realised I could create a new controller that was responsible for setting a viewmodel property of the quantity and only update that specific area on screen. This meant when I was viewing a product and added a certain quantity to the basket this area would update automatically. Finally in the demo for a bit of added pizazz the cart info area animates.&lt;/p&gt;

&lt;p&gt;This is the one bit that took me a long time to get my head around. Angular recommends that no DOM manipulation occurs in controllers but in separate classes/functions called Directives. I understand why, to keep your viewmodel not dependent on your view but man it was hard work! Directives allow you to determine your own HTML elements that Angular understands, again you can pass in dependencies and they will get resolved for you. Put whatever logic you want in your directive and then manipulate the DOM.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app.directive('quantity', function () {
        var linker = function (scope, element, attrs, basketService) {

            scope.$watch(attrs.quantity, function (value, oldValue) {
                if (value &amp;gt; oldValue) {
                    element
                    .animate({ paddingTop: '30px' })
                    .animate({ paddingTop: '10px' });
                }
            }, true);
        };

        return {
            restrict: 'A',
            link: linker
        };
    });

//Usage
&amp;lt;div class="cart-info" ng-controller="CartController" quantity="basketCount()"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One of the biggest things I found with Angular was that it was quite fiddly to setup however the documentation was very good and I had help from &lt;a href="http://twitter.com/philjones88"&gt;Phil Jones&lt;/a&gt;. Once I had solved whatever issue I was having at the time it was glaringly obvious what was needed and I why I was having problems but that’s all part of the learning curve I guess but mighty frustrating. Angular is certainly all encompassing by handling routing, model binding, separation of concerns and much more and so for a large app I can certainly see its potential but for small apps it may be a bit over the top but overall I did like it. There are also library extensions such as &lt;a href="http://angular-ui.github.com/"&gt;AngularUI&lt;/a&gt; which provide custom directives for DOM manipulation, ranked &lt;a href="http://ngmodules.org/"&gt;popular modules&lt;/a&gt; and specific &lt;a href="https://github.com/angular/angularjs-batarang"&gt;Angular debuggers&lt;/a&gt; so this framework certainly has a lot behind it and I definitely think it will be sticking around for a while.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/02/05/evaluating-knockoutjs-and-angularjs-part-2/</guid><link>http://blog.jonathanchannon.com/2013/02/05/evaluating-knockoutjs-and-angularjs-part-2/</link><a10:author><a10:name /></a10:author><category>angularjs</category><category>handlebars</category><category>javascript</category><category>knockoutjs</category><category>learning</category><category>oss</category><category>sammyjs</category><title>Evaluating KnockoutJS and AngularJS – Part 2</title><description>&lt;p&gt;In &lt;a href="http://blog.jonathanchannon.com/2013/02/05/evaluating-knockoutjs-and-angularjs-part-1/" title="Evaluating KnockoutJS and AngularJS – Part 1"&gt;Part 1&lt;/a&gt;, I described how I was using the demo tutorial from &lt;a href="http://sammyjs.org/"&gt;SammyJS&lt;/a&gt; to get a better understanding of &lt;a href="http://angularjs.org/"&gt;AngularJS&lt;/a&gt; and &lt;a href="http://knockoutjs.com/"&gt;KnockoutJS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post I will focus on what I found when using KnockoutJS.&lt;/p&gt;

&lt;p&gt;Again if you just want to get to the code then feel free to take a look here for &lt;a href="https://github.com/jchannon/AngularShopping"&gt;Angular&lt;/a&gt; and here for &lt;a href="https://github.com/jchannon/KnockoutShopping"&gt;Knockout&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Knockout&lt;/h2&gt;

&lt;p&gt;Firstly Knockout should be commended on their documentation and online tutorials. Their website tutorials follows a step by step approach and once you have completed each step you can click through to the next section. You can also leave the tutorial and come back to it and it will remember where you left. There is great support in the KnockoutJS room in Jabbr and I’d like to thank &lt;a href="https://twitter.com/davepermen"&gt;David Spörri&lt;/a&gt; for answering my newbie JS and Knockout questions.&lt;/p&gt;

</description><pubDate>Tue, 05 Feb 2013 00:00:00 Z</pubDate><a10:updated>2013-02-05T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;In &lt;a href="http://blog.jonathanchannon.com/2013/02/05/evaluating-knockoutjs-and-angularjs-part-1/" title="Evaluating KnockoutJS and AngularJS – Part 1"&gt;Part 1&lt;/a&gt;, I described how I was using the demo tutorial from &lt;a href="http://sammyjs.org/"&gt;SammyJS&lt;/a&gt; to get a better understanding of &lt;a href="http://angularjs.org/"&gt;AngularJS&lt;/a&gt; and &lt;a href="http://knockoutjs.com/"&gt;KnockoutJS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post I will focus on what I found when using KnockoutJS.&lt;/p&gt;

&lt;p&gt;Again if you just want to get to the code then feel free to take a look here for &lt;a href="https://github.com/jchannon/AngularShopping"&gt;Angular&lt;/a&gt; and here for &lt;a href="https://github.com/jchannon/KnockoutShopping"&gt;Knockout&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Knockout&lt;/h2&gt;

&lt;p&gt;Firstly Knockout should be commended on their documentation and online tutorials. Their website tutorials follows a step by step approach and once you have completed each step you can click through to the next section. You can also leave the tutorial and come back to it and it will remember where you left. There is great support in the KnockoutJS room in Jabbr and I’d like to thank &lt;a href="https://twitter.com/davepermen"&gt;David Spörri&lt;/a&gt; for answering my newbie JS and Knockout questions.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Knockout also uses HTML elements and attributes like Angular to determine behaviour but Knockout’s approach is more HTML5 in that the most common usage is using the data-bind attribute. For example, the below will use the artist property on the viewmodel to render the div contents.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;div data-bind="text: artist"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As this was going to be a SPA (Single Page Application), the main screen would have an area that would display either a list of products or more information about a single product. The way Knockout handles this is via a ‘with’ keyword.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;div id="main"&amp;gt;
  &amp;lt;!-- Main Page --&amp;gt;
  &amp;lt;div data-bind="with: items"&amp;gt;&amp;lt;/div&amp;gt;
  &amp;lt;!-- Detail Page --&amp;gt;
  &amp;lt;div data-bind="with: chosenProduct"&amp;gt;&amp;lt;div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So when the viewmodel’s ‘items’ property is populated it would show the first div and when the ‘chosenProduct’ property was populated it would show the second div. Nice and simple. You can then populate your markup with properties from the items and chosenProduct within those div’s.&lt;/p&gt;

&lt;p&gt;As we have 2 pages/sections in our app we need to handle routing so that when we’re going to the root it populates our view model with all our products and when the user clicks a specific product it finds that product and populates our view model. KnockoutJS does not have this built into it like Angular does and uses SammyJS to handle this.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var app = Sammy(function () {
    this.get('#/:id', function (context) {
        self.items(null);
        var product = data.filter(function (x) {
            return x.id === parseInt(context.params.id, 10);
        })[0];
        product.quantity = 1;
        self.chosenProduct(product);
    });

    this.get('/', function () {
        self.items(data);
        self.chosenProduct(null);
    });
});

jQuery(function () {
    console.log('rn');
    app.run();
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compared to Angular’s separation approach, here we have most things all in one class/viewmodel i.e/ the routing to assign the items property for all products and single item for the selected product as well as other viewmodel properties. We also have functions to add items and determine quantity as well as saving the information to &lt;a href="https://developer.mozilla.org/en-US/docs/DOM/Storage"&gt;localStorage&lt;/a&gt;. I guess it keeps it altogether in one place but this may get tricky once the app becomes quite large however, I’ve been told things can get separated by using &lt;a href="http://requirejs.org/"&gt;RequireJS&lt;/a&gt; although I think in this demo that would require a large refactor.&lt;/p&gt;

&lt;p&gt;Again in this demo outside of the main area that gets changed with markup we have a shopping cart that needs to update when we add something. I already had a function that calculated my quantities but it did not update the div with the new value. Even though the viewmodel property was a computed function it was not re-run every time something was added to the basket, for that to happen the computed function needs to have an observable property within it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;self.basketItemCount = ko.computed(function () {
       var count = 0;
       var items = self.basketItems();
       for (var i = 0; i &amp;lt; items.length; i%2B%2B) {
           count %2B= items[i].quantity(); //Given that the quantity is an observable property
       }
       return count;
   });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We also wanted to do some animation when the item was added to the basket and because SammyJS has a dependency on jQuery it was very easy to add this to the viewmodel’s addItem function however like Angular this is not recommended as it ties the view to the viewmodel. What Knockout recommend is custom bindings.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ko.bindingHandlers.animateCart = {
    update: function (element, valueAccessor, allBindingsAccessor) {
        $(element)
            .animate({ paddingTop: '30px' })
            .animate({ paddingTop: '10px' });
    }
};

//Usage
&amp;lt;div class="cart-info" data-bind="animateCart: basketAsJson()"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now this is where the data binding gets confusing/interesting. My HTML could not use the basketItems viewmodel property as this is an observable array and so would only react when the array changed NOT when properties of objects within the array changed. I was a bit disappointed with that so I had to create a new function that obviously used an observeable within it ie/the basketItems but that was not enough to handle the properties changing within that array. So the recommendation from &lt;a href="https://twitter.com/robwesterlund"&gt;Robert Westerlund&lt;/a&gt; (a Javascript, Knockout and Regex wizard) was to use ko.toJSON(basketItems()). (The reason we use ko.toJSON and not JSON.stringify is because that would not pick up the observeable property values) This would mean that it would have to traverse all the objects to find the values and because they are observeable this function would fire every time, genius! This also meant when it changed I could also store the changes in localStorage for when the user came back to the website!&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I found Knockout easier to comprehend and get going with compared to Angular which is always a winner when starting something new but Knockout is pretty much just about binding so for things like routing and separation of concerns you need to use other libraries whereas Angular has all that built in. I think for smaller projects without too much logic required Knockout is ideal and I really like the simplicity of SammyJS however, for larger applications that requires dependency injection, a clear separation of concerns and the easy ability to unit test logic Angular is a winner. In this case the term “pick the right tool for the job” certainly applies.&lt;/p&gt;

&lt;p&gt;I guess I should go ahead and investigate Backbone now just to fully educate myself and I’ve been recommend this link – &lt;a href="https://github.com/kjbekkelund/writings/blob/master/published/understanding-backbone.md"&gt;“Step by step from jQuery to Backbone”&lt;/a&gt; so here goes…&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/01/24/using-angularjsbackbonejs-in-windows-8-javascript-app/</guid><link>http://blog.jonathanchannon.com/2013/01/24/using-angularjsbackbonejs-in-windows-8-javascript-app/</link><a10:author><a10:name /></a10:author><category>angularjs</category><category>backbonejs</category><category>github</category><category>javascript</category><category>knockoutjs</category><category>oss</category><category>windows 8</category><category>winjs</category><title>Using AngularJS/BackboneJS in Windows 8 JavaScript app</title><description>&lt;p&gt;To help me expand my JavaScript knowledge as I said I would in my &lt;a href="http://blog.jonathanchannon.com/2013/01/09/javascript-is-the-future-maybe/" title="JavaScript is the future…maybe!"&gt;previous post&lt;/a&gt; I thought I’d write a Windows 8 application using JavaScript.&lt;/p&gt;

&lt;p&gt;After following a few “Hello World” tutorials from Microsoft I thought I’d take a look at the ToDo list demos shown at &lt;a href="http://TodoMVC.com"&gt;TodoMVC.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This website/Github repository takes the ToDo demo and implements it in all the various JS frameworks and libraries out there. As I said previously its a minefield.&lt;/p&gt;

&lt;p&gt;Anyhow, I thought I’d start with Backbone, copy the files, add the references to WinJS and hit F5 and bingo. However, I got the below error:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.stack.imgur.com/DOQl1.png" alt="Unhandled Exception" title="Unhandled Exception" /&gt;&lt;/p&gt;

</description><pubDate>Thu, 24 Jan 2013 00:00:00 Z</pubDate><a10:updated>2013-01-24T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;To help me expand my JavaScript knowledge as I said I would in my &lt;a href="http://blog.jonathanchannon.com/2013/01/09/javascript-is-the-future-maybe/" title="JavaScript is the future…maybe!"&gt;previous post&lt;/a&gt; I thought I’d write a Windows 8 application using JavaScript.&lt;/p&gt;

&lt;p&gt;After following a few “Hello World” tutorials from Microsoft I thought I’d take a look at the ToDo list demos shown at &lt;a href="http://TodoMVC.com"&gt;TodoMVC.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This website/Github repository takes the ToDo demo and implements it in all the various JS frameworks and libraries out there. As I said previously its a minefield.&lt;/p&gt;

&lt;p&gt;Anyhow, I thought I’d start with Backbone, copy the files, add the references to WinJS and hit F5 and bingo. However, I got the below error:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.stack.imgur.com/DOQl1.png" alt="Unhandled Exception" title="Unhandled Exception" /&gt;&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;A bit confused, I googled the issue and found that Microsoft have implemented security principles in WinJS applications to prevent un-sanitized markup to your page. So any time you might dynamically add some HTML to your page your application will throw an exception.&lt;/p&gt;

&lt;p&gt;To get around this issue you can wrap your dynamic content calls with &lt;em&gt;execUnsafeLocalFunction&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You can read more about that from Microsoft’s documentation &lt;a href="http://msdn.microsoft.com/en-gb/library/windows/apps/hh767331.aspx"&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is how you would execute the function around dynamically adding HTML:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MSApp.execUnsafeLocalFunction(function() {
  var body = document.getElementsByTagName('body')[0];
  body.innerHTML = 'example';
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I moved onto Angular to see if that would help but again I hit the same bug. I did find in the Angular code that if you have a reference to jQuery in your page it would use that, so I downloaded the latest version, referenced it and still the same issue.&lt;/p&gt;

&lt;p&gt;What I didn’t realise is that Backbone and Angular both ship jQuery or a subset of it within them and I didn’t fancy modifying those libraries.&lt;/p&gt;

&lt;p&gt;I found &lt;a href="https://github.com/appendto/jquery-win8"&gt;jquery-win8&lt;/a&gt; from appendTo which I thought was the answer so referenced that and still the same issue.&lt;/p&gt;

&lt;p&gt;I then gave KnockoutJS a go and it worked perfectly, the reason being no dependency on jQuery.&lt;/p&gt;

&lt;p&gt;I put a question on StackOverflow and after a couple of days &lt;a href="https://twitter.com/elijahmanor"&gt;Elijah Manor&lt;/a&gt; of AppendTo answered me.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;appendTo’s version removes errors that occur when running jQuery at load time. You still may have code that violates the security model Microsoft put in place. Microsoft is trying to make you aware that there is a risk adding un-sanitized markup to your page.&lt;/p&gt;
  
  &lt;p&gt;If you are confident that is not the case you can try setting jQuery.isUnsafe to true after the appendTo library is included. That should wrap all possible unsafe calls with MSApp.execUnsafeLocalFunction so that Microsoft doesn’t complain.&lt;/p&gt;
  
  &lt;p&gt;Note: This flag is turned off by default&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So I setup the Angular version of the TodoMVC demo in my Win8 application, referenced the WinJS libs, jquery-win8 and set &lt;code&gt;jQuery.isUnsafe&lt;/code&gt; to true and the app started perfectly and all functionality was present. Yay!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;script src="//Microsoft.WinJS.1.0/js/base.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src="/js/default.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script src="js/jquery-1.8.2-win8-1.0.min.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script type="text/javascript"&amp;gt;
    jQuery.isUnsafe = true;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One thing to note is that jquery-win8 runs off of jQuery 1.8.2 but appendTo are working on a 1.9 version.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2013/01/09/javascript-is-the-future-maybe/</guid><link>http://blog.jonathanchannon.com/2013/01/09/javascript-is-the-future-maybe/</link><a10:author><a10:name /></a10:author><category>.net</category><category>angularjs</category><category>backbonejs</category><category>c#</category><category>javascript</category><category>knockoutjs</category><category>learning</category><category>node.js</category><category>oss</category><category>typescript</category><title>JavaScript is the future…maybe!</title><description>&lt;p&gt;I’m not one for New Years resolutions but I thought it was time I looked at JavaScript more in depth.&lt;/p&gt;

&lt;p&gt;I looked at &lt;a href="http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/" title="Node.js, Express, Hello World Formula 1 Style"&gt;Node.js a while back&lt;/a&gt; and found it very interesting and I probably need to go back to it. Over the last month or so there has been a large discussion about async in .Net frameworks and there appears to be a lot of misunderstanding about it (and lets leave it at that, I don’t want to start another flame war) but the thing we can definitely say with &lt;a href="http://nodejs.org/"&gt;Node.js&lt;/a&gt;, well JavaScript to be fair is that it is perfectly asynchronous and non-blocking.&lt;/p&gt;

&lt;p&gt;As a web developer I have used JavaScript from the early days of Response.Write moving onto frameworks such as &lt;a href="http://script.aculo.us/"&gt;script.aculo.us&lt;/a&gt; and &lt;a href="http://mootools.net/"&gt;MooTools&lt;/a&gt; and finally ending up with &lt;a href="http://jquery.com/"&gt;jQuery&lt;/a&gt; which has come pretty much a standard these days so my JavaScript skills are not completely new.&lt;/p&gt;

&lt;p&gt;However, there has been a large push to use JS more and more for rich user friendly applications with things like &lt;a href="http://knockoutjs.com/"&gt;KnockoutJS&lt;/a&gt;, &lt;a href="http://angularjs.org/"&gt;AngularJS&lt;/a&gt; and &lt;a href="http://backbonejs.org/"&gt;BackboneJS&lt;/a&gt; on the client and Node.js on the server. Microsoft has even taken a prominent role in helping bring Node.js to a Windows environment as it started out on *nix based platforms. They have also started contributing to and including scripts in their Visual Studio project templates for jQuery.&lt;/p&gt;

</description><pubDate>Wed, 09 Jan 2013 00:00:00 Z</pubDate><a10:updated>2013-01-09T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I’m not one for New Years resolutions but I thought it was time I looked at JavaScript more in depth.&lt;/p&gt;

&lt;p&gt;I looked at &lt;a href="http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/" title="Node.js, Express, Hello World Formula 1 Style"&gt;Node.js a while back&lt;/a&gt; and found it very interesting and I probably need to go back to it. Over the last month or so there has been a large discussion about async in .Net frameworks and there appears to be a lot of misunderstanding about it (and lets leave it at that, I don’t want to start another flame war) but the thing we can definitely say with &lt;a href="http://nodejs.org/"&gt;Node.js&lt;/a&gt;, well JavaScript to be fair is that it is perfectly asynchronous and non-blocking.&lt;/p&gt;

&lt;p&gt;As a web developer I have used JavaScript from the early days of Response.Write moving onto frameworks such as &lt;a href="http://script.aculo.us/"&gt;script.aculo.us&lt;/a&gt; and &lt;a href="http://mootools.net/"&gt;MooTools&lt;/a&gt; and finally ending up with &lt;a href="http://jquery.com/"&gt;jQuery&lt;/a&gt; which has come pretty much a standard these days so my JavaScript skills are not completely new.&lt;/p&gt;

&lt;p&gt;However, there has been a large push to use JS more and more for rich user friendly applications with things like &lt;a href="http://knockoutjs.com/"&gt;KnockoutJS&lt;/a&gt;, &lt;a href="http://angularjs.org/"&gt;AngularJS&lt;/a&gt; and &lt;a href="http://backbonejs.org/"&gt;BackboneJS&lt;/a&gt; on the client and Node.js on the server. Microsoft has even taken a prominent role in helping bring Node.js to a Windows environment as it started out on *nix based platforms. They have also started contributing to and including scripts in their Visual Studio project templates for jQuery.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;JavaScript can now promote one language to develop on the server and the client which makes things easier and that is one argument I understand and I’m sure there are many others which either I have forgotten or I’m unaware of but the truth is JavaScript seems to be picking up pace in the development world.&lt;/p&gt;

&lt;p&gt;We now have things like &lt;a href="http://coffeescript.org/"&gt;CoffeeScript&lt;/a&gt; that abstracts JS and allows users to type arguably neater code which then compiles into JS.&lt;/p&gt;

&lt;p&gt;The big news is that Microsoft have developed &lt;a href="http://www.typescriptlang.org/"&gt;Typescript&lt;/a&gt; which is an attempt to coerce or make the transition easier for C# developers to use JavaScript. Their mantra for TypeScript is&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;TypeScript is a language for application-scale JavaScript development. TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. Any browser. Any host. Any OS. Open Source.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With this development along with the fact that Windows 8 applications can be written in C#/XAML &lt;strong&gt;or&lt;/strong&gt; JavaScript/HTML proves that MS and therefore probably the mainstream of developers are taking JS as a serious platform to start writing large production systems.&lt;/p&gt;

&lt;p&gt;I am off to a &lt;a href="http://www.dotnetdevnet.com/Meetings/tabid/54/EntryID/73/Default.aspx"&gt;talk&lt;/a&gt; about TypeScript with &lt;a href="http://twitter.com/markrendle"&gt;@markrendle&lt;/a&gt; on Jan 22nd so it’ll be interesting to see the language and the points put forward about it and JS as a whole.&lt;/p&gt;

&lt;p&gt;As I mentioned earlier there is a large focus on rich content applications which use JS to make the application quick and easy to use and frameworks like KnockoutJS, AngularJS and Backbone have popped up allowing you to create these types of applications.&lt;/p&gt;

&lt;p&gt;My biggest concern is which one do you use? I’ve had recommendations for each one which hasn’t helped. KnockoutJS is developed by &lt;a href="http://twitter.com/stevensanderson"&gt;@stevensanderson&lt;/a&gt; who works for MS so you could say it might be best to use that as it maybe more main stream and a more common requirement for employers to see you know it. I’ve heard arguments that AngularJS &amp;amp; BackBone provide the ability to write a more larger scale JS application where Knockout only provides JS type data binding and validation so again this is beneficial. I also discussed with someone that they and their company evaluated all of them and decided that the learning curve was too high for each and they went with jQuery and various plugins and &lt;a href="http://mustache.github.com/"&gt;Mustache&lt;/a&gt; for binding scenarios.&lt;/p&gt;

&lt;p&gt;Obviously there are arguments that you should use a library/framework that supplies the solution to your requirements but generally developers want to learn something that will have long lasting benefits to them, yes they learn things for the sake of it but mostly it will benefit them in the long run, for example, you don’t hear of much use for the &lt;a href="http://compsoc.dur.ac.uk/whitespace/"&gt;Whitespace&lt;/a&gt; programming language do you? So whilst there is an obvious push for more intensive use of JavaScript there still seems to be some sort of barrier to entry in deciding upon the correct path to take. Maybe over time the strongest will survive but whilst you’re waiting 1-2 years for something to mature you’re possibly losing business.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/12/19/why-use-nancyfx/</guid><link>http://blog.jonathanchannon.com/2012/12/19/why-use-nancyfx/</link><a10:author><a10:name /></a10:author><category>.net</category><category>asp.net mvc</category><category>asp.net web api</category><category>community</category><category>nancyfx</category><category>oss</category><title>Why use NancyFX?</title><description>&lt;p&gt;When a new project comes along why should you automatically choose ASP.NET MVC? Yes, its Microsoft based so you may have more of your peers fluent already in that architecture but is there an alternative, a better alternative?&lt;/p&gt;

&lt;p&gt;I believe so and its called &lt;a href="http://nancyfx.org/"&gt;NancyFX&lt;/a&gt;. Your first reaction, what is so special about Nancy? I also believe you’ll ask what is wrong with ASP.NET MVC but maybe you should look at it differently and ask what is right with Nancy?&lt;/p&gt;

&lt;h2&gt;What is Nancy?&lt;/h2&gt;

&lt;p&gt;Nancy is a lightweight framework for building websites / services without getting in your way. It’s heavily inspired by a Ruby project called Sinatra, which happens to identify itself as not being a framework, since it doesn’t include all the plumbing of things such as an ORM, lots of configuration, etc.&lt;/p&gt;

&lt;h2&gt;Does it implement MVC?&lt;/h2&gt;

&lt;p&gt;Nancy does not force you to adhere to the model-view-controller pattern, or any other pattern. It’s nothing more than a service endpoint responding to HTTP verbs. Making it ideal for building Websites, Web Services and APIs.&lt;/p&gt;

</description><pubDate>Wed, 19 Dec 2012 00:00:00 Z</pubDate><a10:updated>2012-12-19T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;When a new project comes along why should you automatically choose ASP.NET MVC? Yes, its Microsoft based so you may have more of your peers fluent already in that architecture but is there an alternative, a better alternative?&lt;/p&gt;

&lt;p&gt;I believe so and its called &lt;a href="http://nancyfx.org/"&gt;NancyFX&lt;/a&gt;. Your first reaction, what is so special about Nancy? I also believe you’ll ask what is wrong with ASP.NET MVC but maybe you should look at it differently and ask what is right with Nancy?&lt;/p&gt;

&lt;h2&gt;What is Nancy?&lt;/h2&gt;

&lt;p&gt;Nancy is a lightweight framework for building websites / services without getting in your way. It’s heavily inspired by a Ruby project called Sinatra, which happens to identify itself as not being a framework, since it doesn’t include all the plumbing of things such as an ORM, lots of configuration, etc.&lt;/p&gt;

&lt;h2&gt;Does it implement MVC?&lt;/h2&gt;

&lt;p&gt;Nancy does not force you to adhere to the model-view-controller pattern, or any other pattern. It’s nothing more than a service endpoint responding to HTTP verbs. Making it ideal for building Websites, Web Services and APIs.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;That doesn’t mean you can’t apply the MVC pattern to Nancy. You can define Views and put them in a Views folder, create Models to return from your endpoints, and map requests to Models, just like you currently do with ASP.NET MVC.&lt;/p&gt;

&lt;h2&gt;Key Considerations&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Easier Testing&lt;/strong&gt; – Nancy provides a testing library that allows you to test the full request/response cycle so not only can you test that your request returns the model you expect you can test that when you pass in accept headers the response is in the format you expect. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Fact]
public void GetData_WhenRequested_ShouldReturnOKStatusCode()
{
    var browser = new Browser();
    var response = browser.Get("/GetData", (with) =&amp;gt;
    {
        with.Header("Authorization", "Bearer johnsmith");
        with.Header("Accept", "application/json");
        with.HttpRequest();
    });

    Assert.Equal(HttpStatusCode.Forbidden, response.StatusCode);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am unaware of how you would be able to test this in MVC without it being a full integration test whereas Nancy has no dependencies on System.Web or MVC so it can provide us with a Response without hitting a server.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automatic Dependency Resolution&lt;/strong&gt; – Nancy provides an in built IOC container called &lt;a href="https://github.com/grumpydev/TinyIoC"&gt;TinyIOC&lt;/a&gt; which will find all your dependencies automatically for you or if you want/need to configure something you can do so at various points in your application. This is done in a Bootstrapper class that exposes various methods and properties to allow you to configure Nancy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;protected override void ConfigureApplicationContainer(TinyIoCContainer container)
{
    base.ConfigureApplicationContainer(container);

    var store = new EmbeddableDocumentStore()
    {
        ConnectionStringName = "RavenDB"
    };

    store.Initialize();

    container.Register(store);
}

protected override void ConfigureRequestContainer(TinyIoCContainer container, NancyContext context)
{
    base.ConfigureRequestContainer(container, context);

    var store = container.Resolve();
    var documentSession = store.OpenSession();

    container.Register(documentSession);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here the IOC container is used in different places within an application’s lifecycle. Once at the startup and once per request. It registers a DocumentStore which should be done only once in an application and then on every request it finds the DocumentStore and uses it to open a session and registers it with the IOC. If you have a service that has a IDocumentSession dependency then it will come via this.&lt;/p&gt;

&lt;p&gt;If for some reason you’re being stubborn and want to use your preferred IOC container, Nancy supports all the main IOC players allowing you to register your dependencies with them instead.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Completely Customisable, Conventions &amp;amp; Better Extension Points&lt;/strong&gt; – One of Nancy’s core features is its extensibility. It it designed to allow you to replace any part you want. You can have custom model binders, view renderers, serializers in fact you can implement your own INancyEngine and completely change how Nancy handles requests etc. There are also a set of pre-defined conventions that you can swap in/out if you want Nancy to do something different than what comes as standard. Everything is complete customisable and very easy to modify Nancy’s behaviour which offers great extensibility points if you wanted to create a 3rd party library for example.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Terse Syntax &amp;amp; Less Ceremony&lt;/strong&gt; – Nancy provides a nice terse syntax that does not get in the way of your application and leaves you to write your code. What I have found is that due to the terse syntax it encourages you to make your application code nice and neat too. One example of less ceremony and terseness is that you can get a full Nancy application running inside a 140 character tweet!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HelloModule : NancyModule
{
    public HelloModule()
    {
        Get["/"] = parameters =&amp;gt; "Hello World";
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Runs on Mono&lt;/strong&gt; – Nancy does not tie itself down to Windows it works just as well on OSX and Linux under &lt;a href="http://www.mono-project.com/Main_Page"&gt;Mono&lt;/a&gt; which allows your team to work on multiple platforms. In fact Nancy can even run on a &lt;a href="http://www.raspberrypi.org/quick-start-guide"&gt;Raspberry Pi&lt;/a&gt; I would like to see ASP.NET MVC do that!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Content Negotiation&lt;/strong&gt; – Content Negotiation is built into Nancy and runs out of the box. This means Nancy can be used in an API type application as well as a website application. In fact if you wanted you could have it do both very easily.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Get["/"] = parameters =&amp;gt; {
    return Negotiate
    .WithModel(new RatPack {FirstName = "Nancy "})
    .WithMediaRangeModel("text/html", new RatPack {FirstName = "Nancy fancy pants"})
    .WithView("negotiatedview")
    .WithHeader("X-Custom", "SomeValue");
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This demo highlights that if you made a request to “/” in your application by a web browser it will return a specific model with a property name of “Nancy fancy pants”, return a view called “negotiatedview” and return a custom header. However, if your API client made a request to “/” it would return a model with “Nancy” and a custom header. The resulting model would then be serialized into JSON, JSONP, XML or any other variation specified in the Accept header from your client. This example is possibly contrived somewhat but Nancy supplies conneg from all routes so something like the below would be serialized based on the headers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Get["/"] = parameters =&amp;gt; {
    var model = MyModel();
    return model;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;No Config&lt;/strong&gt; – To get Nancy up and running there is no config required, no nasty XML files to modify, nothing. As its host agnostic you don’t have to modify anything in web.config to have it running via IIS for example.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Runs Anywhere&lt;/strong&gt; – As I just mentioned Nancy is host agnostic which means you can run it in IIS, WCF, embedded within a EXE, as a windows service or within a self hosted application. Pretty much everywhere!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pipeline Hooks&lt;/strong&gt; – Nancy allows you to modify the pipeline ie.the request and response before and after they are invoked. One simple example is saving your data at the end of a request.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;protected override void RequestStartup(TinyIoCContainer container, IPipelines pipelines, NancyContext context)
{
    base.RequestStartup(container, pipelines, context);

    pipelines.AfterRequest.AddItemToEndOfPipeline((ctx) =&amp;gt;
    {
        var documentSession = container.Resolve();

        if (ctx.Response.StatusCode != HttpStatusCode.InternalServerError)
        {
            documentSession.SaveChanges();
        }

        documentSession.Dispose();
    });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we configure the AfterRequest delegate to find the IDocumentSession used in the request, save the changes to the database and then dispose of the IDocumentSession (although TinyIOC would actually dispose of this for you).&lt;/p&gt;

&lt;p&gt;A more complex example could be that you modify the way the Request.Form is populated on a HTTP POST, it is that extensible and configurable you could do that quite easily.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No ties to System.Web and a Freely Designed Framework&lt;/strong&gt; – System.Web is the core DLL based in ASP.Net. It contains the whole kitchen sink of the framework so you get everything bundled into your application even if you only use 25% of the possibilities. Nancy is architected the other way in that there are &lt;a href="http://nuget.org/packages?q=nancy"&gt;numerous plugins&lt;/a&gt; that supply additional and alternative functionality. Nancy is also not bound to any specific implementation or framework and all requests and responses are built from the ground up allowing it to be loosely coupled and free. This also means that Nancy can run in the .Net client profile environments without the added requirement for .Net full profile that ASP.NET MVC does require.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Support &amp;amp; Community&lt;/strong&gt; – One of the great things about Nancy is its community and support. They have a very active &lt;a href="https://groups.google.com/forum/?fromgroups#!forum/nancy-web-framework"&gt;Google group&lt;/a&gt; and you’ll find loads of help in &lt;a href="http://jabbr.net/#/rooms/nancyfx"&gt;Jabbr&lt;/a&gt; to get your questions answered ASAP. There is a real feeling of community and support because people want to spread the good word about Nancy. It has over 100 contributors to the project but keep in mind the vision, impetus and most of the work is done by 2 guys, &lt;a href="http://twitter.com/TheCodeJunkie"&gt;@TheCodeJunkie&lt;/a&gt; and &lt;a href="http://twitter.com/GrumpyDev"&gt;@GrumpyDev&lt;/a&gt; not a huge team sitting in Redmond. One final thing the &lt;a href="http://nancyfx.spreadshirt.net/"&gt;swag &lt;/a&gt;is a lot more stylish than Microsoft t-shirts :)&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In one of my last &lt;a href="http://blog.jonathanchannon.com/2012/11/29/asp-net-web-api-testing/" title="ASP.NET Web API Testing"&gt;blog posts&lt;/a&gt; I described how you could test the full pipeline in ASP.NET Web API because Microsoft don’t supply a nice way to do it. This blog post got the attention of Microsoft and at the time of writing this blog post it has had over 3150 hits and appeared on the home page of ASP.NET. The core of the code in that post was taken out of Nancy. So please if you liked what you saw there, give Nancy a try I think you’ll find there many benefits described above as well as others I’ve not mentioned. It is your duty as software developers to try new things and investigate tools.&lt;/p&gt;

&lt;p&gt;So when your next project comes about and your manager says “Ok lets write our new app in ASP.NET MVC” your reactions should be reflected by these animated GIFs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Should we really use ASP.NET MVC?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/tIRwUHo.gif" alt="Should we use ASP.NET MVC?" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The boss says we can use Nancy!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/Vf8KDu6.gif" alt="All systems go" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We have our new Nancy app up and running in no time!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://i.imgur.com/pqv0Xld.gif" alt="Up and Running" /&gt;&lt;/p&gt;

&lt;p&gt;For more infomation on Nancy checkout the &lt;a href="http://nancyfx.org/"&gt;website&lt;/a&gt; and &lt;a href="https://github.com/NancyFx/Nancy/wiki/Documentation"&gt;documentation.&lt;/a&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/12/07/easily-publish-a-nuget-package/</guid><link>http://blog.jonathanchannon.com/2012/12/07/easily-publish-a-nuget-package/</link><a10:author><a10:name /></a10:author><category>.net</category><category>asp.net web api</category><category>community</category><category>nuget</category><category>oss</category><title>Easily publish a NuGet package</title><description>&lt;p&gt;I recently published &lt;a href="http://blog.jonathanchannon.com/2012/11/29/asp-net-web-api-testing/" title="ASP.NET Web API Testing"&gt;WebAPI.Testing&lt;/a&gt; on &lt;a href="http://nuget.org/packages/WebAPI.Testing"&gt;Nuget&lt;/a&gt; but found it a bit tricky to build a package ready for NuGet.&lt;/p&gt;

&lt;p&gt;There is &lt;a href="http://docs.nuget.org/docs/creating-packages/creating-and-publishing-a-package"&gt;documentation &lt;/a&gt;about how to do it but I found it hard to follow so I thought I’d document how I finally got my package ready.&lt;/p&gt;

&lt;p&gt;The easiest way I thought was to have something built into Visual Studio. I spoke to &lt;a href="https://twitter.com/davidfowl"&gt;David Fowler&lt;/a&gt; and he told me you can edit your *.csproj file and add &lt;code&gt;&amp;lt;BuildPackage&amp;gt;true&amp;lt;/BuildPackage&amp;gt;&lt;/code&gt; to it.&lt;/p&gt;

&lt;p&gt;When you build your project a *.nupkg is created ready for publishing with NuGet.&lt;/p&gt;

</description><pubDate>Fri, 07 Dec 2012 00:00:00 Z</pubDate><a10:updated>2012-12-07T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I recently published &lt;a href="http://blog.jonathanchannon.com/2012/11/29/asp-net-web-api-testing/" title="ASP.NET Web API Testing"&gt;WebAPI.Testing&lt;/a&gt; on &lt;a href="http://nuget.org/packages/WebAPI.Testing"&gt;Nuget&lt;/a&gt; but found it a bit tricky to build a package ready for NuGet.&lt;/p&gt;

&lt;p&gt;There is &lt;a href="http://docs.nuget.org/docs/creating-packages/creating-and-publishing-a-package"&gt;documentation &lt;/a&gt;about how to do it but I found it hard to follow so I thought I’d document how I finally got my package ready.&lt;/p&gt;

&lt;p&gt;The easiest way I thought was to have something built into Visual Studio. I spoke to &lt;a href="https://twitter.com/davidfowl"&gt;David Fowler&lt;/a&gt; and he told me you can edit your *.csproj file and add &lt;code&gt;&amp;lt;BuildPackage&amp;gt;true&amp;lt;/BuildPackage&amp;gt;&lt;/code&gt; to it.&lt;/p&gt;

&lt;p&gt;When you build your project a *.nupkg is created ready for publishing with NuGet.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;However if you have no AssemblyInfo.cs or *.nuspec file then that package won’t contain anything that useful about your package.&lt;/p&gt;

&lt;p&gt;So the easiest thing to do is amend your AssemblyInfo.cs file &lt;a href="http://docs.nuget.org/docs/creating-packages/creating-and-publishing-a-package#From_a_project"&gt;with information about your package&lt;/a&gt; if you have an AssemblyInfo.cs file. If you don’t have one its not a problem.&lt;/p&gt;

&lt;p&gt;Build your project, open the *.nupkg file that was created with &lt;a href="http://docs.nuget.org/docs/creating-packages/using-a-gui-to-build-packages"&gt;Nuget Package Explorer&lt;/a&gt; and edit the metadata adding any extra information you want about your project.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If your solution has NuGet restore turned on the build will pick up the dependencies required for your project. Cool eh!&lt;/p&gt;

&lt;p&gt;Click File – Save Metadata As, and save the *.nuspec file into the same place as your *.csproj file.&lt;/p&gt;

&lt;p&gt;Go to Visual Studio and include this *.nuspec file in your project. Open it and remove the &lt;code&gt;&amp;lt;files&amp;gt;&amp;lt;/files&amp;gt;&lt;/code&gt; node unless you are including other specific files with your package. I found it created a node that pointed to /lib/net45/myProject.dll which was unnecessary and resulted in my project not compiling.&lt;/p&gt;

&lt;p&gt;In the future you can edit this *.nuspec file for example when you have a new version and when you build your project it will create a nice new *.nupkg file based on the information provided in the *.nuspec file.&lt;/p&gt;

&lt;p&gt;Then go to &lt;a href="http://nuget.org/"&gt;nuget.org&lt;/a&gt; and register/logon and upload your *.nupkg file&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/11/29/asp-net-web-api-testing/</guid><link>http://blog.jonathanchannon.com/2012/11/29/asp-net-web-api-testing/</link><a10:author><a10:name /></a10:author><category>.net</category><category>asp.net web api</category><category>c#</category><category>community</category><category>github</category><category>nancyfx</category><category>oss</category><title>ASP.NET Web API Testing</title><description>&lt;p&gt;As the need arose to implement some kind of Web Service/HTTP API I thought I would evaluate &lt;a href="http://nancyfx.org/"&gt;NancyFX&lt;/a&gt;, &lt;a href="http://www.asp.net/web-api"&gt;ASP.NET Web API&lt;/a&gt; and &lt;a href="http://www.servicestack.net/"&gt;ServiceStack&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Suffice to say all performed as expected and I was actually surprised to find that implementing ASP.NET Web API was easier than ServiceStack (I know that might be a bit of a statement to make to the ServiceStack followers, sorry). I found Nancy easiest to implement. The very simple API demos can be found on &lt;a href="http://github.com/jchannon"&gt;my Github page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When it came to testing ASP.NET Web API I found it to be wanting slightly in comparison to Nancy. With WebAPI I could make direct calls to the controller methods to make sure data was returned correctly and I could mock a repository and test that the methods in the repository were being called but there was nothing I could see to test the HTTP response I would get.&lt;/p&gt;

</description><pubDate>Thu, 29 Nov 2012 00:00:00 Z</pubDate><a10:updated>2012-11-29T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;As the need arose to implement some kind of Web Service/HTTP API I thought I would evaluate &lt;a href="http://nancyfx.org/"&gt;NancyFX&lt;/a&gt;, &lt;a href="http://www.asp.net/web-api"&gt;ASP.NET Web API&lt;/a&gt; and &lt;a href="http://www.servicestack.net/"&gt;ServiceStack&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Suffice to say all performed as expected and I was actually surprised to find that implementing ASP.NET Web API was easier than ServiceStack (I know that might be a bit of a statement to make to the ServiceStack followers, sorry). I found Nancy easiest to implement. The very simple API demos can be found on &lt;a href="http://github.com/jchannon"&gt;my Github page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When it came to testing ASP.NET Web API I found it to be wanting slightly in comparison to Nancy. With WebAPI I could make direct calls to the controller methods to make sure data was returned correctly and I could mock a repository and test that the methods in the repository were being called but there was nothing I could see to test the HTTP response I would get.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;I found that you could configure a lot of stuff to get a HttpResponseMessage back as shown below however in my opinion it wasn’t particularly easy on the eye and seemed a bit over the top just to get a response back.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Arrange
var config = new HttpConfiguration();
var request = new HttpRequestMessage(HttpMethod.Post, "http://localhost/api/products");
var route = config.Routes.MapHttpRoute("DefaultApi", "api/{controller}/{id}");
var routeData = new HttpRouteData(route, new HttpRouteValueDictionary { { "controller", "products" } });
var controller = new ProductsController(repo);
controller.ControllerContext = new HttpControllerContext(config, routeData, request);
controller.Request = request;
controller.Request.Properties[HttpPropertyKeys.HttpConfigurationKey] = config;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you can call your controller method and assert against it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Act
var result = controller.PostProduct(new Product { Id = 1 });

// Assert
Assert.Equal(HttpStatusCode.Created, result.StatusCode);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can’t take credit for finding this out though. I found it on &lt;a href="http://www.peterprovost.org/blog/2012/06/16/unit-testing-asp-dot-net-web-api/"&gt;Peter Provost&lt;/a&gt; blog post where he says &lt;a href="http://bradwilson.typepad.com/"&gt;Brad Wilson&lt;/a&gt; helped him construct the code.&lt;/p&gt;

&lt;p&gt;I wasn’t positive whether the HTTP response returned was as pure as if the request was actually made to a server. By understanding the &lt;a href="https://github.com/NancyFx/Nancy/tree/master/src/Nancy.Testing"&gt;Nancy.Testing&lt;/a&gt; library I knew that the response given there was an exact copy of what would be given if hitting a server.&lt;/p&gt;

&lt;p&gt;I then investigated a bit more and found a great blog post by &lt;a href="http://www.strathweb.com/2012/06/asp-net-web-api-integration-testing-with-in-memory-hosting/"&gt;Filip W&lt;/a&gt; about in-memory hosting. This essentially mirrors a server in terms of receiving a request and issuing a response.&lt;/p&gt;

&lt;p&gt;Knowing what I knew from Nancy I thought I could apply it to Web API. What this meant was you could submit requests and test against the response. I’m not sure how you would classify it in terms of unit testing or integration testing because the tests run very quickly but I suppose you are hitting an actual server albeit in memory.&lt;/p&gt;

&lt;p&gt;Here’s what a simple test looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class GetDataTests
{
    [Fact]
    public void GetData_WhenRequested_ShouldReturnJSON()
    {
        var browser = new Browser();
        var response = browser.Get("/GetData", (with) =&amp;gt;
        {
            with.Header("Accept", "application/json");
            with.HttpRequest();
        });

        Assert.Equal("application/json", response.Content.Headers.ContentType.MediaType);
    }

    [Fact]
    public void GetData_WhenRequested_ShouldReturnOKStatusCode()
    {
        var browser = new Browser();
        var response = browser.Get("/GetData", (with) =&amp;gt;
        {
            with.Header("Authorization", "Bearer johnsmith");
            with.Header("Accept", "application/json");
            with.HttpRequest();
        });

        Assert.Equal(HttpStatusCode.Forbidden, response.StatusCode);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You define your test, in this case &lt;a href="http://xunit.codeplex.com/"&gt;xUnit&lt;/a&gt;. You create an instance of the Browser class. This is just a class name there is no browser, it doesn’t fire up IE or anything like that. You then call a method named after a HTTP verb with a path specified. You also have the ability to specify items within the request such as headers, form values and cookies using a delegate. The methods in the Browser class will return a HttpResponseMessage which is what Web API server returns and you can then assert against the response.&lt;/p&gt;

&lt;p&gt;Delving a little deeper into the code, the Browser class takes an optional HttpConfiguration constructor argument or if one is not supplied it uses the below configuration. It then creates an instance of HttpServer and passes the configuration into it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private readonly HttpServer _server;

public Browser()
{
    var config = new HttpConfiguration();
    config.Routes.MapHttpRoute(name: "Default", routeTemplate: "api/{controller}/{action}/{id}", defaults: new { id = RouteParameter.Optional });
    config.IncludeErrorDetailPolicy = IncludeErrorDetailPolicy.Always;
    _server = new HttpServer(config);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When a method is called it then builds up a HttpRequestMessage using the items defined in the delegate in the unit test and passes it to the server variable and the HttpResponseMessage is returned.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public HttpResponseMessage Get(string path, Action browserContext = null)
{
  return this.HandleRequest(HttpMethod.Get, path, browserContext);
}

private HttpResponseMessage HandleRequest(HttpMethod method, string path, Action browserContext)
{
    var request = CreateRequest(method, path, browserContext ?? this.DefaultBrowserContext);

    if (BrowserHttpClient == null)
        BrowserHttpClient = new HttpClient(_server);

    HttpResponseMessage response = BrowserHttpClient.SendAsync(request).Result;

    request.Dispose();

    if (_server != null)
    {
        _server.Dispose();
    }

    return response;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives you the ability to pretty much test the full pipeline of a request and response.&lt;/p&gt;

&lt;p&gt;The Github repository is located &lt;a href="https://github.com/jchannon/WebAPI.Testing"&gt;here&lt;/a&gt; and if you like what you see please take a closer look into &lt;a href="http://nancyfx.org/"&gt;NancyFX&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/nancy-horizontal-framed-bf-wb-620x240.png" alt="NancyFX" title="NancyFX" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;NancyFX&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE 7th Dec 2012:&lt;/strong&gt; The project is available on &lt;a href="http://nuget.org/packages/WebAPI.Testing"&gt;Nuget&lt;/a&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/11/23/the-for-loop-is-the-devil-in-disguise/</guid><link>http://blog.jonathanchannon.com/2012/11/23/the-for-loop-is-the-devil-in-disguise/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>learning</category><title>The For Loop is the devil in disguise</title><description>&lt;p&gt;I recently spoke to someone about the ‘for’ loop who opened my eyes to how unstructured the ‘for’ loop is.&lt;/p&gt;

&lt;p&gt;I have only ever used it in the traditional sense of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for(int i = 0; i &amp;lt; 10; i++)
{

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I looked into some more and thought I’d show anybody else who may not have known about this innocent little thing in the C# language. It may exist in other languages but I am explicitly talking about C#.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (; ; )
{
  Console.WriteLine("Hi");
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For some reason this compiles and executes! Who knew? Smarter people than me obviously. What do you expect it to output?&lt;/p&gt;

&lt;p&gt;The answer is it outputs “Hi” forever as there is nothing to determine when the loop should end however there is nothing to determine when it should start either.&lt;/p&gt;

</description><pubDate>Fri, 23 Nov 2012 00:00:00 Z</pubDate><a10:updated>2012-11-23T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;I recently spoke to someone about the ‘for’ loop who opened my eyes to how unstructured the ‘for’ loop is.&lt;/p&gt;

&lt;p&gt;I have only ever used it in the traditional sense of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for(int i = 0; i &amp;lt; 10; i++)
{

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I looked into some more and thought I’d show anybody else who may not have known about this innocent little thing in the C# language. It may exist in other languages but I am explicitly talking about C#.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (; ; )
{
  Console.WriteLine("Hi");
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For some reason this compiles and executes! Who knew? Smarter people than me obviously. What do you expect it to output?&lt;/p&gt;

&lt;p&gt;The answer is it outputs “Hi” forever as there is nothing to determine when the loop should end however there is nothing to determine when it should start either.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;I continued to investigate what other weird syntax the for loop would take.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (int i = 3; ; )
{
  Console.WriteLine("Number " + i);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will output “Number 3″ forever.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (int i = 0; ; )
{
  Console.WriteLine("Hi");
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will output “Hi” forever.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (int i = 0; ; Console.WriteLine("Hi"))
{

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will output “Hi” forever.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (; ; RunThis())
{

}

private void RunThis()
{
  Console.WriteLine("Hi");
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will output “Hi” forever.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (int i = 0; ; Console.WriteLine("Hi"))
{
  Console.WriteLine(" there");
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will output “Hi there” forever&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (string s = "Hi"; ; )
{
  Console.WriteLine(s);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will output “Hi” forever.&lt;/p&gt;

&lt;p&gt;What do you think the below will do?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (Initialize(); ; )
{

}

private void Initialize()
{
  Console.WriteLine("Initialized");
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It actually outputs “Initliazed” and sits in an infinite loop.&lt;/p&gt;

&lt;p&gt;The below probably does what you expect it to by looping 0-9 and exits yet still something I’d never considered doing in the past.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (int i = InitializeInt(); i &amp;lt; 10; i++)
{
  Console.WriteLine(i);
}

private static int InitializeInt()
{
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a slight deviation on the above but essentially the same just with a Func&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Func factory = () =&amp;gt; 0;

for (int i = factory(); i &amp;lt; 10; i++)
{
  Console.WriteLine(i);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again the below is possible and something I’d never do but still possible.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (var val = Init(); val.MyInt &amp;lt; 10; val.MyInt++)
{
  Console.WriteLine(val.MyString);
}

public struct MyStruct
{
  public int MyInt { get; set; }
  public string MyString { get; set; }
}

private MyStruct Init()
{
  return new MyStruct() { MyInt = 0, MyString = "Hi" };
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What do you think the below will do?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool ok = true;
for (; ok; )
{
  Console.WriteLine("Hi");
  ok = false;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It will enter the loop once and then exit. It acts as a normal loop just without the initialization of an int i and increasing the value of i on each iteration.&lt;/p&gt;

&lt;p&gt;There may be some other syntaxes it will accept but I’ve not found them.&lt;/p&gt;

&lt;p&gt;In essence the for loop has no expectations, it will execute with no arguments specified infinitely. In each 3 places of a for loop you can do pretty much what you want, I did find you couldn’t initialize an int and a string like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (string s="Hi", int i = 0; i &amp;lt; 10;i++ )
{

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Its only really the middle section that determines when the loop should end.&lt;/p&gt;

&lt;p&gt;I’d never thought of using the for loop in these ways but it seems its possible for some reason. I’m starting to wonder if I’m not a good programmer for not knowing this or whether its just one of those things that’s odd but available in the language and if places like Google ask you in interview “What does this do? Does it compile?” to try and almost trick you for some reason. I’m sure there would be some problem solving argument for asking but I’m not sure what that proves as if you were a good developer you would never use it anyway.&lt;/p&gt;

&lt;p&gt;My constant journey of learning continues.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/11/18/gitignore-not-working-fixed/</guid><link>http://blog.jonathanchannon.com/2012/11/18/gitignore-not-working-fixed/</link><a10:author><a10:name /></a10:author><category>community</category><category>git</category><category>github</category><category>oss</category><category>stackoverflow</category><title>.gitignore not working – fixed!</title><description>&lt;p&gt;This happens to me too often and I always end up googling the answer so this post is probably more of a location I know I can come to find the answer, although by writing it down hopefully it may sink in that I should stop getting too excited on a new project.&lt;/p&gt;

&lt;h3&gt;New project scenario&lt;/h3&gt;

&lt;p&gt;You’re all very excited about your new project and you think its about time you committed this to source control. Obviously you’re using &lt;a href="http://git-scm.com/"&gt;Git&lt;/a&gt; so you initialise a new repository and commit your files. You then setup a remote repository at &lt;a href="http://github.com"&gt;Github&lt;/a&gt; and it asks you whether you want it create a .gitignore file – you do. So now you have a repository remotely and locally. Easiest thing to do is pull from the remote, setup your remote and push to it. The other scenario might be you’ve committed locally and then realise you need to add a .gitignore file which you do and then commit.&lt;/p&gt;

</description><pubDate>Sun, 18 Nov 2012 00:00:00 Z</pubDate><a10:updated>2012-11-18T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;This happens to me too often and I always end up googling the answer so this post is probably more of a location I know I can come to find the answer, although by writing it down hopefully it may sink in that I should stop getting too excited on a new project.&lt;/p&gt;

&lt;h3&gt;New project scenario&lt;/h3&gt;

&lt;p&gt;You’re all very excited about your new project and you think its about time you committed this to source control. Obviously you’re using &lt;a href="http://git-scm.com/"&gt;Git&lt;/a&gt; so you initialise a new repository and commit your files. You then setup a remote repository at &lt;a href="http://github.com"&gt;Github&lt;/a&gt; and it asks you whether you want it create a .gitignore file – you do. So now you have a repository remotely and locally. Easiest thing to do is pull from the remote, setup your remote and push to it. The other scenario might be you’ve committed locally and then realise you need to add a .gitignore file which you do and then commit.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;In both cases you will now see the files in your &lt;a href="https://github.com/github/gitignore"&gt;standardised .gitignore file&lt;/a&gt; are not being ignored. After a few head scratches you realise its because the .gitignore file should be added to your repo first before any commits.&lt;/p&gt;

&lt;h2&gt;The solution!&lt;/h2&gt;

&lt;p&gt;Long story short you have to remove all tracked files and add them back in using the below commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git rm -r --cached .
git add .
git commit -m ".gitignore is now working"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first line unstages and removes the paths to your files from the git index recursively.&lt;/p&gt;

&lt;p&gt;The second line adds all your files back in but because the .gitignore is present it will not add files that should be ignored!&lt;/p&gt;

&lt;p&gt;The final line commits all your files back to the index.&lt;/p&gt;

&lt;p&gt;As much as I’d like to take credit for this knowledge I’m going to have to point you in the direction of the &lt;a href="http://stackoverflow.com/questions/1139762/gitignore-file-not-ignoring"&gt;stackoverflow post&lt;/a&gt; that has helped me in the past.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/11/06/publishing-to-windows-azure-from-github/</guid><link>http://blog.jonathanchannon.com/2012/11/06/publishing-to-windows-azure-from-github/</link><a10:author><a10:name /></a10:author><category>.net</category><category>deployment</category><category>dinnerparty</category><category>git</category><category>github</category><category>windowsazure</category><title>Publishing to Windows Azure from Github</title><description>&lt;p&gt;Back in &lt;a href="http://weblogs.asp.net/scottgu/archive/2012/09/17/announcing-great-improvements-to-windows-azure-web-sites.aspx"&gt;July 2012&lt;/a&gt; Microsoft announced improvements to Azure Web Sites. One of those improvements was to Git publishing so when you pushed changes to your Github repository Azure would automatically pick that up and deploy the project. I even mentioned it in my &lt;a href="http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me/" title="NancyFX, RavenDB, NerdDinner and Me"&gt;DinnerParty blog post&lt;/a&gt; but have only just looked at implementing it.&lt;/p&gt;

&lt;h2&gt;Preparation&lt;/h2&gt;

&lt;p&gt;As I said in my previous post Azure supported Git publishing but it was a two step process. You push to Github and then push to Azure and it gets deployed. If you already have Git setup on your Azure account there is nowhere in the dashboard that allows to you setup Github integration. I thought I was going to have reset my deployment credentials and set it all up again when I asked the question on &lt;a href="http://jabbr.net"&gt;Jabbr&lt;/a&gt;. Luckily &lt;a href="http://twitter.com/davidfowl"&gt;David Fowler&lt;/a&gt; was online. Why is that lucky? He wrote the Github integration feature of Azure.&lt;/p&gt;

&lt;p&gt;To setup your Azure account to enable Github integration you have to FTP into your Azure account and delete the deployment history by deleting all contents in the /site/deployments folder.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/deploymenthistory-620x604.png" alt="Deployment History" title="Deployment History" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deployment History&lt;/em&gt;&lt;/p&gt;

</description><pubDate>Tue, 06 Nov 2012 00:00:00 Z</pubDate><a10:updated>2012-11-06T00:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Back in &lt;a href="http://weblogs.asp.net/scottgu/archive/2012/09/17/announcing-great-improvements-to-windows-azure-web-sites.aspx"&gt;July 2012&lt;/a&gt; Microsoft announced improvements to Azure Web Sites. One of those improvements was to Git publishing so when you pushed changes to your Github repository Azure would automatically pick that up and deploy the project. I even mentioned it in my &lt;a href="http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me/" title="NancyFX, RavenDB, NerdDinner and Me"&gt;DinnerParty blog post&lt;/a&gt; but have only just looked at implementing it.&lt;/p&gt;

&lt;h2&gt;Preparation&lt;/h2&gt;

&lt;p&gt;As I said in my previous post Azure supported Git publishing but it was a two step process. You push to Github and then push to Azure and it gets deployed. If you already have Git setup on your Azure account there is nowhere in the dashboard that allows to you setup Github integration. I thought I was going to have reset my deployment credentials and set it all up again when I asked the question on &lt;a href="http://jabbr.net"&gt;Jabbr&lt;/a&gt;. Luckily &lt;a href="http://twitter.com/davidfowl"&gt;David Fowler&lt;/a&gt; was online. Why is that lucky? He wrote the Github integration feature of Azure.&lt;/p&gt;

&lt;p&gt;To setup your Azure account to enable Github integration you have to FTP into your Azure account and delete the deployment history by deleting all contents in the /site/deployments folder.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/deploymenthistory-620x604.png" alt="Deployment History" title="Deployment History" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deployment History&lt;/em&gt;&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h3&gt;Setup&lt;/h3&gt;

&lt;p&gt;Once the history is deleted, if you go to the Deployments tab in the Azure account you will now see a link to “Deploy from my GitHub repository”.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/afterdeploymentdelete-620x527.png" alt="Setup" title="Setup " /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Setup&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By clicking the link it will inform you that you need to authorize Azure to have access to your Github repository and that if there is code already in your repository it will be deployed again.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/setup-620x320.png" alt="Authorize" title="Authorize" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Authorize&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once you authorize Azure to read your Github account you need to tell it which repository it should watch.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/authorize.png" alt="Repository" title="Repository" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Repository&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As stated earlier, if there is code in the repo it will begin to deploy automatically and confirm deployment once this has been done.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/deploying-initial-620x197.png" alt="Deploying" title="Deploying" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deploying&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/deployed-620x202.png" alt="Deployed" title="Deployed" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deployed&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From now on, once you push changes to your Github repository Azure will pick them up and deploy the project to your web site! Could it get any easier?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE 7th Nov 2012:&lt;/strong&gt; After speaking to &lt;a href="http://twitter.com/davidfowl"&gt;David Fowler&lt;/a&gt; again he remembered that you could take the Git webservice hook and enter that into the Github repository account settings.&lt;/p&gt;

&lt;p&gt;Go to the Configure tab in Azure and copy the Git hoook URL.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/azurehook-620x135.png" alt="Azure Hook" title="Azure Hook" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Azure Hook&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Then go to your Github repo and into the Admin section and under Service Hooks enter the copied Azure URL&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/github-hook-620x260.png" alt="Github Hook" title="Github Hook" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Github Hook&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Away you go!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/10/25/simple-net-twittergooglefacebook-authentication/</guid><link>http://blog.jonathanchannon.com/2012/10/25/simple-net-twittergooglefacebook-authentication/</link><a10:author><a10:name /></a10:author><category>authentication</category><category>community</category><category>dinnerparty</category><category>nancyfx</category><category>oss</category><category>social</category><category>social authentication</category><title>Simple .Net Twitter,Google,Facebook Authentication</title><description>&lt;p&gt;Logging into websites is no longer a matter of typing in your username and password and clicking the login button. If you already have an account with the main social networks you can log into a site using your credentials from that website saving you having to register your details &lt;em&gt;again&lt;/em&gt;. This obviously makes things a bit easier as you don’t have to remember another password. (Although you should all be using a password manager such as &lt;a href="http://www.lastpass.com"&gt;LastPass&lt;/a&gt;.)&lt;/p&gt;

&lt;h2&gt;Current Social Login Providers&lt;/h2&gt;

&lt;p&gt;There are currently providers out there that allow you to use their services to integrate into your website to provide authentication via the social networks. The main two that I know of are &lt;a href="http://janrain.com"&gt;Janrain&lt;/a&gt; and &lt;a href="http://www.dotnetopenauth.net/"&gt;DotNetOpenAuth&lt;/a&gt;. I’ve not worked with DotNetOpenAuth but I have with Janrain when building &lt;a href="http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me/"&gt;DinnerParty&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The process was reasonably easy but not as simple as it could be.&lt;/p&gt;

</description><pubDate>Wed, 24 Oct 2012 23:00:00 Z</pubDate><a10:updated>2012-10-24T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Logging into websites is no longer a matter of typing in your username and password and clicking the login button. If you already have an account with the main social networks you can log into a site using your credentials from that website saving you having to register your details &lt;em&gt;again&lt;/em&gt;. This obviously makes things a bit easier as you don’t have to remember another password. (Although you should all be using a password manager such as &lt;a href="http://www.lastpass.com"&gt;LastPass&lt;/a&gt;.)&lt;/p&gt;

&lt;h2&gt;Current Social Login Providers&lt;/h2&gt;

&lt;p&gt;There are currently providers out there that allow you to use their services to integrate into your website to provide authentication via the social networks. The main two that I know of are &lt;a href="http://janrain.com"&gt;Janrain&lt;/a&gt; and &lt;a href="http://www.dotnetopenauth.net/"&gt;DotNetOpenAuth&lt;/a&gt;. I’ve not worked with DotNetOpenAuth but I have with Janrain when building &lt;a href="http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me/"&gt;DinnerParty&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The process was reasonably easy but not as simple as it could be.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h3&gt;.Net Simple Social Authentication&lt;/h3&gt;

&lt;p&gt;I was made aware of an OSS project by &lt;a href="http://twitter.com/philliphaydon"&gt;@philliphaydon&lt;/a&gt; whilst keeping up to speed with the latest &lt;a href="http://nancyfx.org/"&gt;NancyFX &lt;/a&gt;goings on in &lt;a href="http://jabbr.net"&gt;Jabbr&lt;/a&gt; that aims to provide a simple way of using social networks to log into a site.&lt;/p&gt;

&lt;p&gt;It started out aimed at ASP.Net MVC but after a bit of arm bending from &lt;a href="http://twitter.com/philliphaydon"&gt;@philliphaydon&lt;/a&gt; it was refactored to be web framework agnostic so it could be used with &lt;a href="http://nancyfx.org/"&gt;NancyFX &lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Being enthusiastic, I asked if any help was needed and I ended up making a simple demo website using &lt;a href="http://nancyfx.org/"&gt;NancyFX&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Code Examples&lt;/h2&gt;

&lt;p&gt;Let me see some code I hear you say so here we go:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bootstrapper:&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Bootstrapper : DefaultNancyBootstrapper
{
    private const string TwitterConsumerKey = "Rb7qNNPUPsRSYkznFTbF6Q";
    private const string TwitterConsumerSecret = "pP1jBdYOlmCzo08QFJjGIHY4YSyPdGLPO2m1q47hu9c";
    private const string FacebookAppId = "159181340893340";
    private const string FacebookAppSecret = "97c4e4d0fa548232cf8f9c68a7adcff9";
    private const string GoogleConsumerKey = "587140099194.apps.googleusercontent.com";
    private const string GoogleConsumerSecret = "npk1_gx-gqJmLiJRPFooxCEY";

   protected override void ApplicationStartup(TinyIoCContainer container, IPipelines pipelines)
   {
       RegisterAuthenticationProviders(container);

       base.ApplicationStartup(container, pipelines);

       CookieBasedSessions.Enable(pipelines);
   }

   private static void RegisterAuthenticationProviders(TinyIoCContainer container)
   {
       Condition.Requires(container).IsNotNull();

       var twitterProvider = new TwitterProvider(TwitterConsumerKey, TwitterConsumerSecret,
                                                 new Uri(
                                                     "http://localhost:6969/AuthenticateCallback?providerKey=Twitter"));

       var facebookProvider = new FacebookProvider(FacebookAppId, FacebookAppSecret,
                                                   new Uri(
                                                       "http://localhost:6969/AuthenticateCallback?providerKey=facebook"));

       var googleProvider = new GoogleProvider(GoogleConsumerKey, GoogleConsumerSecret,
                                               new Uri(
                                                   "http://localhost:6969/AuthenticateCallback?providerKey=google"));

       var authenticationService = new AuthenticationService();
       authenticationService.AddProvider(twitterProvider);
       authenticationService.AddProvider(facebookProvider);
       authenticationService.AddProvider(googleProvider);

       container.Register(authenticationService);
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;HomeModule&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeModule : NancyModule
{  
   private const string SessionGuidKey = "GUIDKey";

   public HomeModule(IAuthenticationService authenticationService)
   {
       Get["/"] = parameters =&amp;gt; View["login"];

       Get["/RedirectToAuthenticate/{providerKey}"] = parameters =&amp;gt;
              {
                  Session[SessionGuidKey] = Guid.NewGuid();
                  Uri uri =
                      authenticationService.
                          RedirectToAuthenticationProvider(
                              parameters.providerKey,
                              Session[SessionGuidKey].ToString());

                  return Response.AsRedirect(uri.AbsoluteUri);
              };

       Get["/AuthenticateCallback"] = parameters =&amp;gt;
              {
                  if (string.IsNullOrEmpty(Request.Query.providerKey))
                  {
                      throw new ArgumentNullException("providerKey");
                  }

                  // It's possible that a person might hit this resource directly, before any session value
                  // has been set. As such, we should just fake some state up, which will not match the
                  // CSRF check.
                  var existingState = (string)(Session[SessionGuidKey] ?? Guid.NewGuid().ToString());

                  var model = new AuthenticateCallbackViewModel();

                  var querystringParameters = new NameValueCollection();
                  foreach (var item in Request.Query)
                  {
                      querystringParameters.Add(item, Request.Query[item]);
                  }

                  try
                  {
                      model.AuthenticatedClient =
                          authenticationService.CheckCallback(Request.Query.providerKey,
                                                              querystringParameters,
                                                              existingState);
                  }
                  catch (Exception exception)
                  {
                      model.Exception = exception;
                  }

                  return View["AuthenticateCallback", model];
              };
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should hopefully be pretty self explanatory as that is the aim of the game but I will go through it.&lt;/p&gt;

&lt;p&gt;Firstly we need to setup our social network key and secret which was provided to us by registering an app at the relevant site. We then setup a IAuthenticationProvider class for Twitter, Google &amp;amp; Facebook with the key, secret and callback URL that the social network will make request to after login. We then register our providers with a IAuthenticationService class.&lt;/p&gt;

&lt;p&gt;We then setup our IOC container to use authenticationService so that when our modules like Home take a dependency of IAuthenticationService it will use that.&lt;/p&gt;

&lt;p&gt;Our root route provides a login page with hyperlinks to the RedirectToAuthenticate route. For example, using Twitter it will be &lt;strong&gt;&lt;em&gt;http://mydomain.com/RedirectToAuthenticate/Twitter&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We create an item in our session so that it can be retrieved later when &lt;a href="http://twitter.com"&gt;Twitter&lt;/a&gt; calls the callback URL. This is so we can validate(if needs be) that its a valid request and not some cheeky hacker. We then use the IAuthenticationService to redirect to the social network login page based on the value of providerKey argument. This could also be Facebook or Google.&lt;/p&gt;

&lt;p&gt;The user is redirected to &lt;a href="http://twitter.com"&gt;Twitter&lt;/a&gt; and logs in and then gets directed back to our website.&lt;/p&gt;

&lt;p&gt;Our AuthenticateCallback route then creates a NameValueCollection from the querystring parameters and passes the providerKey (Twitter,Facebook,Google), NameValueCollection and Session value to the IAuthenticationService to validate all is ok.&lt;/p&gt;

&lt;p&gt;If so it creates a view model and passes it to the view and in this example shows us the logged in user’s details.&lt;/p&gt;

&lt;p&gt;I think that was pretty simple.&lt;/p&gt;

&lt;h2&gt;I want to play&lt;/h2&gt;

&lt;p&gt;The source code is over at &lt;a href="https://github.com/PureKrome/World-Domination.Web.Authentication"&gt;Github&lt;/a&gt; and is written by Pure.Krome or &lt;a href="https://twitter.com/lara_eagle"&gt;Lara Eagle&lt;/a&gt; if you prefer (same person). It’s also available on &lt;a href="http://nuget.org/packages/World-Domination.Web.Authentication"&gt;Nuget&lt;/a&gt;. Its name you ask? World-Domination.Web.Authentication&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/10/25/testing-your-applications-email-logic/</guid><link>http://blog.jonathanchannon.com/2012/10/25/testing-your-applications-email-logic/</link><a10:author><a10:name /></a10:author><category>.net</category><category>c#</category><category>email</category><category>node.js</category><category>oss</category><category>papercut</category><title>Testing your application’s email logic</title><description>&lt;p&gt;If you’ve ever written an application that sends out email you may have written the code and executed it numerous times to check that the logic works and that the email appears as you hope. This obviously means you have to hit your SMTP server each time, open your email client and check your emails each time.&lt;/p&gt;

&lt;h3&gt;Papercut&lt;/h3&gt;

&lt;p&gt;Reading through my Twitter timeline I saw &lt;a href="http://twitter.com/TheCodeJunkie"&gt;@TheCodeJunkie&lt;/a&gt; asking about the app that you can use to test sending emails from your application.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/tweet.png" alt="TheCodeJunkie Tweet" /&gt;&lt;/p&gt;

&lt;p&gt;Intrigued, I kept an eye on my timeline and found that the application in question was &lt;a href="http://papercut.codeplex.com/"&gt;Papercut&lt;/a&gt;&lt;/p&gt;

</description><pubDate>Wed, 24 Oct 2012 23:00:00 Z</pubDate><a10:updated>2012-10-24T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;If you’ve ever written an application that sends out email you may have written the code and executed it numerous times to check that the logic works and that the email appears as you hope. This obviously means you have to hit your SMTP server each time, open your email client and check your emails each time.&lt;/p&gt;

&lt;h3&gt;Papercut&lt;/h3&gt;

&lt;p&gt;Reading through my Twitter timeline I saw &lt;a href="http://twitter.com/TheCodeJunkie"&gt;@TheCodeJunkie&lt;/a&gt; asking about the app that you can use to test sending emails from your application.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/tweet.png" alt="TheCodeJunkie Tweet" /&gt;&lt;/p&gt;

&lt;p&gt;Intrigued, I kept an eye on my timeline and found that the application in question was &lt;a href="http://papercut.codeplex.com/"&gt;Papercut&lt;/a&gt;&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;In simple terms you put Papercut on a machine (most likely your development machine) and in your code set the SMTP host to be the IP Address of where Papercut is running and fire up your application and watch Papercut notify you from the system tray that it’s recevied a message.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/Untitled.png" alt="Papertray System Tray Icon" title="Papertray System Tray Icon" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Papertray System Tray Icon&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lets open it up and see what it says!&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/raw-620x330.png" alt="Raw View" title="Raw View" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Raw View&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://blog.jonathanchannon.com/images/blogpostimages/body-620x330.png" alt="Body View" title="Body View" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Body View&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I’ve tested this with a C# application and a Node.js application. If you don’t believe me, here’s the code:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C#&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;static void Main(string[] args)
{
    MailMessage message = new MailMessage("Jonathan Channon ", "tim.cook@apple.com")
    {
        Subject = "iPhone",
        Body = "I would like a free iPhone for work. My boss won't let me have one. How about it?",
        BodyEncoding = Encoding.UTF8
    };

    message.Bcc.Add("boss@company.com");
    SmtpClient client = new SmtpClient("127.0.0.1");
    NetworkCredential info = new NetworkCredential("mail@jonathanchannon.com", "reallysecurepassword");
    client.DeliveryMethod = SmtpDeliveryMethod.Network;
    client.UseDefaultCredentials = false;
    client.Credentials = info;
    try
    {
        client.Send(message);
    }
    catch(Exception ex)
    {
        Console.WriteLine(ex.Message);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Node.js&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var nodemailer = require("nodemailer");

var smtpTransport = nodemailer.createTransport("SMTP");

var mailOptions = {
    from: "Jonathan Channon ",
    to: "tim.cook@apple.com",
    cc: "boss@company.com",
    subject: "iPhone",
    text: "I would like a free iPhone for work. My boss won't let me have one. How about it?",
    html: "I would like a free iPhone for work. My boss won't let me have one. How about it?"
};

smtpTransport.sendMail(mailOptions, function(error, response){
    if(error)
    {
        console.log(error);
    }else
    {
        console.log("Message sent: " %2B response.message);
    }

    smtpTransport.close();
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think this is a great little app and a real time saver. I highly recommend that you check it out!&lt;/p&gt;

&lt;p&gt;They also offer a web based version called &lt;a href="http://dummysmtp.com/"&gt;DummySMTP&lt;/a&gt; which allows you to test an almost live copy of your application but DummySMTP captures the emails sent out to customers, for example.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/</guid><link>http://blog.jonathanchannon.com/2012/10/08/node-js-express-hello-world-formula-1-style/</link><a10:author><a10:name /></a10:author><category>editor</category><category>express</category><category>f1</category><category>ide</category><category>jade</category><category>json</category><category>node.js</category><category>oss</category><category>st2</category><category>sublime text 2</category><title>Node.js, Express, Hello World Formula 1 Style</title><description>&lt;p&gt;In my ongoing efforts to be a better developer (plus I just like tinkering) I thought I would take a look at &lt;a href="http://nodejs.org"&gt;node.js&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I did play with node.js about a year ago where I setup a TCP listener to listen to a TCP Server on the network broadcasting XML messages, I then took these, formatted them to JSON and passed it to a browser using &lt;a href="http://socket.io/"&gt;Socket.IO&lt;/a&gt;. It was pretty cool but the project never came to anything.&lt;/p&gt;

&lt;p&gt;However, I thought I would re-visit and setup a proper development environment on my Mac at home.&lt;/p&gt;

&lt;h2&gt;Editors&lt;/h2&gt;

&lt;p&gt;There are many editors/IDE’s that you can use for node.js development such as Vim, Eclipse, WebStorm, Aptana Studio, Emacs and Cloud9 IDE.  As I have used &lt;a href="http://www.sublimetext.com/"&gt;Sublime Text 2&lt;/a&gt; (ST2) before I thought I would use this because I like it and all the cool kids use it!!&lt;/p&gt;

&lt;p&gt;Coming from a mainly IDE based background I started to find things a bit hard going however ST2 allows plugins to be used to make the user experience a lot nicer.  Below are a list of plugins I have installed:&lt;/p&gt;

</description><pubDate>Sun, 07 Oct 2012 23:00:00 Z</pubDate><a10:updated>2012-10-07T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;In my ongoing efforts to be a better developer (plus I just like tinkering) I thought I would take a look at &lt;a href="http://nodejs.org"&gt;node.js&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I did play with node.js about a year ago where I setup a TCP listener to listen to a TCP Server on the network broadcasting XML messages, I then took these, formatted them to JSON and passed it to a browser using &lt;a href="http://socket.io/"&gt;Socket.IO&lt;/a&gt;. It was pretty cool but the project never came to anything.&lt;/p&gt;

&lt;p&gt;However, I thought I would re-visit and setup a proper development environment on my Mac at home.&lt;/p&gt;

&lt;h2&gt;Editors&lt;/h2&gt;

&lt;p&gt;There are many editors/IDE’s that you can use for node.js development such as Vim, Eclipse, WebStorm, Aptana Studio, Emacs and Cloud9 IDE.  As I have used &lt;a href="http://www.sublimetext.com/"&gt;Sublime Text 2&lt;/a&gt; (ST2) before I thought I would use this because I like it and all the cool kids use it!!&lt;/p&gt;

&lt;p&gt;Coming from a mainly IDE based background I started to find things a bit hard going however ST2 allows plugins to be used to make the user experience a lot nicer.  Below are a list of plugins I have installed:&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/facelessuser/BracketHighlighter"&gt;Bracket Highlighter&lt;/a&gt; - highlights start/end brackets&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jdc0589/JsFormat"&gt;JsFormat&lt;/a&gt; - tidies code&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SublimeLinter/SublimeLinter"&gt;SublimeLinter&lt;/a&gt; using &lt;a href="https://github.com/jshint/node-jshint"&gt;node-jshint&lt;/a&gt; - tells you if your code is any good or not&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tanepiper/SublimeText-Nodejs"&gt;SublimeText-NodeJS&lt;/a&gt; – provides node.js autocomplete&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jfromaniello/Grandson-of-Obsidian"&gt;Grandson Of Obsidian&lt;/a&gt; - ST2 theme using Consolas font&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To set it up exactly how I have it you need to go to Preferences-&gt;Settings-User and copy &amp;amp; paste the below in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "color_scheme": "Packages/Color Scheme - Default/son-of-obsidian.tmTheme",
    "dictionary": "Packages/Language - English/en_GB.dic",
    "ignored_packages":
    [
      "Vintage"
    ],
    "font_face": "Consolas",
    "font_size": 13
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I did find setting up SublimeLinter a bit tricky as I wanted it to use the “use strict”; option. &lt;em&gt;Strict Mode is a new feature in ECMAScript 5 that allows you to place a program, or a function, in a “strict” operating context. This strict context prevents certain actions from being taken and throws more exceptions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function DoCode()
{
  message = "Hi";
  console.log(message);  //Without strict mode you would not get any warnings that message is not defined.
}

//With strict mode on, this is what you should have
function DoCode()
{
  var message = "Hi";
  console.log(message);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My settings for SublimeLinter are below and should be added to Preferences-&gt;Package Settings-&gt;SublimeLinter-&gt;Settings-User&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "sublimelinter_popup_errors_on_save": true,
    "jshint_options":
    {
        "globalstrict": true,
        "node": true,
        "es5": true
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration will popup warnings when you save the file and will allow you to put “use strict”; at the top of every file. This global definition is not allowed by default and it should be put in every function which I think is unnecessary. I also think “use strict”; should be turned on by default without the need for it to be declared but there may be more to it than that as I’m just a newbie.&lt;/p&gt;

&lt;h2&gt;From Scratch&lt;/h2&gt;

&lt;p&gt;If you are completely new to Node then its probably worth taking a look at their website but if you want to start developing web sites straight away I highly recommend taking a look at the course &lt;a href="http://pluralsight.com/training/courses/TableOfContents?courseName=expressjs"&gt;Web Development with ExpressJS&lt;/a&gt; by &lt;a href="http://twitter.com/hhariri"&gt;@hhariri&lt;/a&gt; on &lt;a href="http://Pluralsight.com"&gt;Pluralsight&lt;/a&gt;.  I found it very informative and helped me start this little project. Also read &lt;a href="http://twitter.com/robashton"&gt;@robashton&lt;/a&gt; blog post called &lt;a href="http://codebetter.com/robashton/2012/09/03/keeping-js-sane/"&gt;Keeping JS Sane&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Hello World&lt;/h2&gt;

&lt;p&gt;As with all new pet projects I come up with I find you always need an idea to write an app to use the technology you want to play with. I ended up choosing Formula 1 as I managed to find an API that provided lots of statistics. I’m still not entirely sure what this app will end up becoming so if anyone has any ideas and/or wants to use node.js get in touch and maybe we can collaborate.&lt;/p&gt;

&lt;p&gt;I’m not going to go into detail about how this &lt;a href="https://github.com/jchannon/NodeF1"&gt;app&lt;/a&gt; works as you can browse the source code and watch &lt;a href="http://twitter.com/hhariri"&gt;@hhariri&lt;/a&gt;‘s &lt;a href="http://pluralsight.com/training/courses/TableOfContents?courseName=expressjs"&gt;course&lt;/a&gt; to get further information but I’ll just show a couple of snippets to “whet the appetite” (yes, that is how you spell “whet”).&lt;/p&gt;

&lt;p&gt;In our main application file we set up the routes like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var express = require('express'),
    home = require('./routes/index.js');

app.configure(function(){
  app.set('view engine', 'jade');
});

var app = express();

app.get('/current-schedule', home.currentSchedule)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We define what we are going to use, we configure our app using express and setup a route that points to a function called currentSchedule in the ./routes/index.js file&lt;/p&gt;

&lt;p&gt;Then in our ./routes/index.js file we configure what the response will be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var request = require('request');

exports.currentSchedule = function(req, response) {

    var date = new Date();
    var year = date.getUTCFullYear();

    request.get("http://ergast.com/api/f1/" + year + ".json", function(err, res, body) {
        if(!err) {
            var model = JSON.parse(body);

            response.render("current/currentSchedule", {
                title: "Current Race Schedule",
                currentUrl: req.path,
                RaceData: model.MRData.RaceTable
            });
        }
    });

};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again we define what modules we are going to use. To expose our currentSchedule function we use the exports keyword. This function has a request and response argument that we can interrogate if needs be. We need to determine the current year, make a separate request to the API, which also exposes a error object, a response object but also the body of content returned. We put that into a JSON object and then call a view with a title, currentUrl and our data as a view model.&lt;/p&gt;

&lt;p&gt;Its then up to our view to display this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ul  
    each item in RaceData.Races
        - var raceDate = new Date(item.date).toDateString();
        li #{item.raceName},  #{item.Circuit.Location.locality}, #{item.Circuit.Location.country}
            = raceDate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our view uses the &lt;a href="http://jade-lang.com/"&gt;Jade&lt;/a&gt; view engine. This took me a while to get used to as it is based on indentation in your files. PRO Tip: Always use tabs to make things easier and use the View-&gt;Indentation-&gt;Convert Indentation to Tabs option in ST2. Lets just say it took me a few hours to get somethig working. Things were lined up but they were a mixture of spaces and tabs but I couldn’t see that in ST2. If anyone knows how to display all symbols in ST2 like &lt;a href="http://notepad-plus-plus.org/"&gt;Notepad++&lt;/a&gt; does then please let me know. It would have saved a lot of time and swear words.&lt;/p&gt;

&lt;p&gt;So we setup an unordered list and for each item in our RaceData.Races element in our view model we format the date and output various bits of information about the current schedule. The hyphen in Jade allows you to do code behind and the #{} is how you tell the view to output data from the view model. We don’t always have to do it like that, we could do li= item.raceName but in our scenario the above was needed, I assume because of the code behind logic.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully that gives you a bit of insight into node.js and Express. If you have any suggestions about moving my &lt;a href="https://github.com/jchannon/NodeF1"&gt;app&lt;/a&gt; forwards then let me know. I would like to add forms authentication, validation, unit tests and a database to see how these things work in node.js. Moving over to a text editor and a dynamic language is certainly a lot different to using Visual Studio and C#. Its a classic case of the bell arch learning curve, where you start out excited, then seriously frustrated and then I hope you peek and start to love it. I’m surprised at how much I have actually enjoyed it, a bit like taking the stabilisers off your bike, there’s certainly a freedom to not using Visual Studio. I think I have passed the extreme frustration stage in what I have done so far anyway but no doubt I’m sure I will hit more frustration as the application progresses but I think the key to it is definitely having a half decent editor to help in some areas.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE 9th Oct 2012:&lt;/strong&gt; I realised that I didn’t mention debugging. I will keep this short because I researched it and I believe this is the best way. You need to install &lt;a href="https://github.com/dannycoates/node-inspector"&gt;node-inspector&lt;/a&gt; which gives you a node debugger that uses the Google Chrome developer tools UI. You also need &lt;a href="https://github.com/remy/nodemon"&gt;nodemon&lt;/a&gt; which runs your app and when you make changes in your script it will restart node automatically. To run this side by side, you can fire up terminal and run &lt;em&gt;nodemon –debug app.js &amp;amp; node-inspector&lt;/em&gt; and that will work a treat. I wanted to streamline this a bit more so I looked into the &lt;a href="http://sublimetext.info/docs/en/reference/build_systems.html"&gt;Build Systems&lt;/a&gt; ST2 offers. This &lt;a href="http://addyosmani.com/blog/custom-sublime-text-build-systems-for-popular-tools-and-languages"&gt;link&lt;/a&gt; also provides some more information. Essentially what we want to do is have a key binding that will put our app into debug mode and allow us to step through our lines of code. In ST2, go to Project-&amp;gt;Save Project As and choose a name. This will create a project file which can add your build system for the project. (You do not have to do this as your build system can apply globally if you want.) Add the following piece of code in your .sublime-project file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;"build_systems":
[
    {
        "name": "GiveMeAName",
        "shell": true,
        "cmd": ["nodemon --debug $project_path/app.js &amp;amp;amp; node-inspector"],
        "file_regex": "^[ ]*File \"(...*?)\", line ([0-9]*)",
        "selector": "source.js"
    }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save it, in ST2, go to Tools-&amp;gt;Build System and select the option you entered in the “name” element. Now press Cmd%2BB and you’ll see a console appear in ST2 telling you that nodemon and node-inspector are running. If you really wanted, you could modify the key bindings ST2 offers and change Cmd%2BB to F5 but I’ll leave that one to you. Enjoy!&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/09/28/abstracting-the-file-system/</guid><link>http://blog.jonathanchannon.com/2012/09/28/abstracting-the-file-system/</link><a10:author><a10:name /></a10:author><category>file system</category><category>oss</category><category>SRP</category><category>System.IO.Abstractions</category><title>Abstracting the File System</title><description>&lt;p&gt;Following on from my post about &lt;a href="http://blog.jonathanchannon.com/2012/09/25/is-oss-good-for-your-career/"&gt;OSS&lt;/a&gt; I thought I would illustrate how cool OSS can be.&lt;/p&gt;

&lt;p&gt;The day before that post was published I was working on a program that required the file system. All you good developers are going to know that the file system is a dependency and dependencies are bad and this post will probably be a bit like preaching to the choir however I thought it was worth posting.&lt;/p&gt;

&lt;p&gt;So you have a a method similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void DoSomethingCool()
{
  //do some stuff now write to file

  FileInfo f = new FileInfo("C:\Mytext.txt")
  using(StreamWriter w = f.CreateText())
  {
    w.WriteLine("This blog post is cool");
    w.Close();
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You are writing to a file to record something and need to test your method. Remember, unit tests are supposed to be fast. Typically anything that writes to a database or a file system will be slow however, we also have the problem that our method is now dependent on the file system and dependencies are bad. Wouldn’t it be handy if we could make FileInfo a representation of an interface.&lt;/p&gt;

</description><pubDate>Thu, 27 Sep 2012 23:00:00 Z</pubDate><a10:updated>2012-09-27T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Following on from my post about &lt;a href="http://blog.jonathanchannon.com/2012/09/25/is-oss-good-for-your-career/"&gt;OSS&lt;/a&gt; I thought I would illustrate how cool OSS can be.&lt;/p&gt;

&lt;p&gt;The day before that post was published I was working on a program that required the file system. All you good developers are going to know that the file system is a dependency and dependencies are bad and this post will probably be a bit like preaching to the choir however I thought it was worth posting.&lt;/p&gt;

&lt;p&gt;So you have a a method similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void DoSomethingCool()
{
  //do some stuff now write to file

  FileInfo f = new FileInfo("C:\Mytext.txt")
  using(StreamWriter w = f.CreateText())
  {
    w.WriteLine("This blog post is cool");
    w.Close();
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You are writing to a file to record something and need to test your method. Remember, unit tests are supposed to be fast. Typically anything that writes to a database or a file system will be slow however, we also have the problem that our method is now dependent on the file system and dependencies are bad. Wouldn’t it be handy if we could make FileInfo a representation of an interface.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Simplest way is to hit F12 in Visual Studio and see what interface FileInfo implements.&lt;/p&gt;

&lt;p&gt;We get this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public sealed class FileInfo : FileSystemInfo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hmmm, OK lets hit F12 on FileSystemInfo and we get&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public abstract class FileSystemInfo : MarshalByRefObject, ISerializable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So it looks like the FileInfo class does not implement any interface so now we’re stuck, we have a hard coded dependency and our unit tests are going to be slow.&lt;/p&gt;

&lt;p&gt;The answer is we’ll have to create an interface for all the methods and properties that we are going to use that are in FileInfo. Not too bad I suppose, we’re only using CreateText in the example above so we create:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public interface IFileInfo
{
  StreamWriter CreateText();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now pass in IFileInfo to our class’ constructor and hopefully use it like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MyClass
{
  private readonly IFileInfo fileInfo;

  public MyClass(IFileInfo fileInfo)
  {
    this.fileInfo = fileInfo;
  }

  public void DoSomethingCool()
  {
    //do some stuff now write to file

    using(StreamWriter w = fileInfo.CreateText())
    {
      w.WriteLine("This blog post is cool");
      w.Close();
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could then use &lt;a href="https://code.google.com/p/moq/"&gt;Moq&lt;/a&gt; in our unit test to successfully mock the filesystem and ensure our method writes to the file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[TestFixture]
public class MyTestClass
{
  [Test]
  public void DoSomethingCool_WhenCalled_WritesToFile()
  {
    //Arrange
    var fileMock = new Moq.Mock&amp;lt;IFileInfo&amp;gt;();
    var myClass = new MyClass(fileMock.Object);

    //Act
    myClass.DoSomethingCool();

    //Assert
    fileMock.Verify(x =&amp;amp;gt; x.CreateText());
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you’ve probably spotted there are three issues here. Firstly FileInfo expects a path in its constructor which we haven’t supplied, secondly all we’ve actually verified in our test is that CreateText is called and finally DoSomethingCool still has a dependency on StreamWriter which uses the underlying file system. The solution is to abstract the StreamWriter yourself so the unit test is testing that data is written to the file and do something about the FileInfo dependency.&lt;/p&gt;

&lt;p&gt;It could be done, not a problem, but wouldn’t it be nice if someone has already done that for you? Luckily they have and its called &lt;a href="https://github.com/tathamoddie/System.IO.Abstractions"&gt;System.IO.Abstractions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A quote from the website "&lt;em&gt;At the core of the library is IFileSystem and FileSystem. Instead of calling methods like File.ReadAllText directly, use IFileSystem.File.ReadAllText. We have exactly the same API, except that ours is injectable and testable.&lt;/em&gt;"&lt;/p&gt;

&lt;p&gt;What this means is that you can now do:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MyClass
{
    private readonly IFileSystem fileSystem;

    public MyClass(IFileSystem  fileSystem)
    {
      this.fileSystem = fileSystem;
    }

    public void DoSomethingCool()
    {
      //do some stuff now write to file

      var file = fileSystem.FileInfo.FromFileName("C:\Mytext.txt");
      using(IStreamWriter writer = file.CreateText())
      {
        writer.WriteLine("This blog post is cool");
        writer.Close();
      }
    }
}

[TestFixture]
public class MyTestClass
{
    [Test]
    public void DoSomethingCool_WhenCalled_WritesToFile()
    {
      //Arrange
      var filesystemMock = new Moq.Mock&amp;lt;IFileSystem&amp;gt;();
      var fileinfoFactory = new Moq.Mock&amp;lt;IFileInfoFactory&amp;gt;();
      var fileinfoMock = new Moq.Mock&amp;lt;FileInfoBase&amp;gt;();
      var streamWriterMock = new Moq.Mock&amp;lt;IStreamWriter&amp;gt;();

      var myClass = new MyClass(filesystemMock.Object);

      fileinfoMock.Setup(x =&amp;gt; x.CreateText()).Returns(streamWriterMock.Object);
      fileinfoFactory.Setup(x =&amp;gt; x.FromFileName("C:\Mytext.txt")).Returns(fileinfoMock.Object);
      filesystemMock.Setup(x =&amp;gt; x.FileInfo).Returns(fileinfoFactory.Object);

      //Act
      myClass.DoSomethingCool();

      //Assert
      streamWriterMock.Verify(x =&amp;gt; x.WriteLine("This blog post is cool"));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can now still verify your calls to file are getting called but on top of that you can verify the content written to file all without the need for a file system.&lt;/p&gt;

&lt;p&gt;Now some of you may not like all the mocks arranged in the unit test so luckily for you System.IO.Abstractions has a set of its own mocks that you can use. This means we have to modify our class slightly to make the filesystem public so we can read what is written to file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class MyClass
{
    public readonly IFileSystem filesystem;

    public MyClass(IFileSystem filesystem)
    {
      this.filesystem = filesystem;
    }

    public void DoSomethingCool()
    {
      //do some stuff now write to file

      using(var w = filesystem.FileInfo.FromFileName("C:\\Mytext.txt").CreateText())
      {
        w.WriteLine("This blog post is cool");
        w.Close();
      }
    }
}

[TestFixture]
public class MyTestClass
{
    [Test]
    public void DoSomethingCool_WhenCalled_WritesToFile()
    {
      //Arrange
      var fileData = new MockFileData("");
      var fileSystem = new MockFileSystem(new Dictionary&amp;lt;string, MockFileData&amp;gt;
      {
          { "C:\\Mytext.txt", fileData }
      });

      var myClass = new MyClass(fileSystem);

      //Act
      myClass.DoSomethingCool();

      //Assert
      MockFileInfoFactory factory = (MockFileInfoFactory)myClass.filesystem.FileInfo;
      var result = factory.FileInfo.OpenText().ReadToEnd();
      Assert.AreEqual("This blog post is cool", result);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Its a matter of preference which you prefer but either way hopefully this illustrates how and why you should abstract the file system&lt;/p&gt;

&lt;p&gt;One thing to point out is that these file system calls should be in their own class and that DoSomethingCool should be calling something like IFileSystemLogger.Log() which is where the file system calls should be. This illustrates the &lt;a href="http://en.wikipedia.org/wiki/Single_responsibility_principle"&gt;Single Responsibility Principle&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some of the functionality outlined above is not available yet in the master branch of System.IO.Abstractions but it has been submitted as a pull request from me so hopefully it will be merged soon. I’ve already had one PR merged and it was quick, built and released on NuGet within the hour. I encourage you to take a look at this project and try and contribute. There are a few quirks, for example, I wanted to use Moq v4 syntax in my tests but couldn’t work it out, not sure if that’s me or System.IO.Abstractions. Anyhow the more people that get involved the better it will become.&lt;/strong&gt;&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/09/25/is-oss-good-for-your-career/</guid><link>http://blog.jonathanchannon.com/2012/09/25/is-oss-good-for-your-career/</link><a10:author><a10:name /></a10:author><category>career</category><category>community</category><category>oss</category><title>Is OSS good for your career?</title><description>&lt;p&gt;Got your attention? Good.&lt;/p&gt;

&lt;p&gt;Let me start by pointing out there are many opinions about the answer to this question. You will have yours and I have mine, that’s called freedom of speech. I would like to hear your opinions so leave it in the comments.&lt;/p&gt;

&lt;p&gt;Let me explain that I have had 4 jobs in the last 10-11 years. 3 of those were via recruitment agents. They check your skills, tick them off and pass you over to the employer if they match and hopefully you get an interview. That process has happened to me in each of those 3 times.&lt;/p&gt;

&lt;p&gt;If you have read my previous blog posts you’ll know I have spent the last year reading a lot of other peoples code and learning all the best practices I can in a bid to become a better software developer as well as give back to the developer community where I can.&lt;/p&gt;

&lt;p&gt;After spending a couple of months porting &lt;a href="http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me"&gt;NerdDinner over to NancyFX&lt;/a&gt; I realised that I had not looked at &lt;a href="http://www.asp.net/mvc"&gt;ASP.Net MVC 4&lt;/a&gt; and the new features built into it. I felt slightly strange at that point as I was usually an early adopter of these things keen to check out the new stuff. I think this was partly due to the fact I had spent a lot of time learning &lt;a href="http://www.nancyfx.org"&gt;NancyFX&lt;/a&gt; and really quite enjoying the framework and interacting with the small community of people who use Nancy.&lt;/p&gt;

</description><pubDate>Mon, 24 Sep 2012 23:00:00 Z</pubDate><a10:updated>2012-09-24T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;Got your attention? Good.&lt;/p&gt;

&lt;p&gt;Let me start by pointing out there are many opinions about the answer to this question. You will have yours and I have mine, that’s called freedom of speech. I would like to hear your opinions so leave it in the comments.&lt;/p&gt;

&lt;p&gt;Let me explain that I have had 4 jobs in the last 10-11 years. 3 of those were via recruitment agents. They check your skills, tick them off and pass you over to the employer if they match and hopefully you get an interview. That process has happened to me in each of those 3 times.&lt;/p&gt;

&lt;p&gt;If you have read my previous blog posts you’ll know I have spent the last year reading a lot of other peoples code and learning all the best practices I can in a bid to become a better software developer as well as give back to the developer community where I can.&lt;/p&gt;

&lt;p&gt;After spending a couple of months porting &lt;a href="http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me"&gt;NerdDinner over to NancyFX&lt;/a&gt; I realised that I had not looked at &lt;a href="http://www.asp.net/mvc"&gt;ASP.Net MVC 4&lt;/a&gt; and the new features built into it. I felt slightly strange at that point as I was usually an early adopter of these things keen to check out the new stuff. I think this was partly due to the fact I had spent a lot of time learning &lt;a href="http://www.nancyfx.org"&gt;NancyFX&lt;/a&gt; and really quite enjoying the framework and interacting with the small community of people who use Nancy.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;However, I had recognized that my skill set was missing a checkbox to tick off but after using Nancy there were a few areas of MVC that I no longer liked but I knew I would have to learn MVC 4 to ensure I stay up to date. This realization had me thinking and I asked &lt;a href="http://twitter.com/GrumpyDev"&gt;@GrumpyDev&lt;/a&gt;, one of the lead developers of NancyFX that during his day to day development if he was in a position as an architect to make his developers use NancyFX instead of MVC. Part of his response was &lt;em&gt;“we very much suffer from using new and shiny Microsoft stuff, and following "best practices" without having a clue why”&lt;/em&gt;. This reinforced my growing suspicion that maybe OSS work was for the tinkerer developer, the modern day man who sits at a computer tinkering away instead of doing stuff in his shed because as far as I can tell most companies recruiting people want their experience to be on a MS stack. Unfortunately you don’t see many adverts asking for NancyFX, FubuMVC or even RavenDB experience. As @GrumpyDev states, I’m not sure why this is either because within the community there is a push for people to learn as much as possible and use the latest, greatest and alternative technologies whilst those that are employing just want their developers to have a full understanding of Microsoft technologies.&lt;/p&gt;

&lt;p&gt;I kept these thoughts floating around my little brain for a week or two and whilst in Jabbr &lt;a href="http://twitter.com/invalid_arg"&gt;@invalid_arg&lt;/a&gt; was discussing how he was playing with &lt;a href="http://nodejs.org/"&gt;Node.js&lt;/a&gt; and I asked him if he thought there was many opportunities out there for Node.js developers and it started a myriad of opinions about OSS, new languages/frameworks on the block and career possibilites which I’ll try and outline. The reason I make reference to this is because the discussion was not just about OSS but all the new and alternative frameworks and languages out there so the title of this post was a bit of a headline grabber.&lt;/p&gt;

&lt;p&gt;There were some that believed that if you were smart you would be able to get a job even if you had no experience which was counterattacked with you would never have been asked to the interview if the employer saw you had no experience.&lt;/p&gt;

&lt;p&gt;It was pointed out that if you worked at Microsoft they drop you in at the deep end and ask you to build something even if you had no experience in that area but is Microsoft a fair example of “most” companies employing you and then asking you to build something on the fly?&lt;/p&gt;

&lt;p&gt;One guy posted “if you have a heart problem would you go to a doctor who has never done surgery before?” implying that the doctor was smart so could therefore pick up heart surgery skills.&lt;/p&gt;

&lt;p&gt;Another point of view said that “I would rather employ a smart guy with enthusiasm than a dumb guy with 5 years experience and no enthusiasm” which I think a lot of people agreed with however, how do you measure whether a guy is smart enough in a typical interview environment? Would he have even been asked to interview?&lt;/p&gt;

&lt;p&gt;There was also an opinion that if he was smart he could be moulded to become a good developer in that specific area of software development.&lt;/p&gt;

&lt;p&gt;Using Microsoft as an example it was noted that forward thinking employers may take the smart guy approach because they can see the potential and with investment will have a great return. I wonder how many companies like this exist though.&lt;/p&gt;

&lt;p&gt;In another argument if a developer who only knows C# gets fired today and has a mortgage and family to look after he does not have the luxury of spending two months to learn another language to get another job, he needs one now. There may be companies out there but I believe in rare circumstances that may employ you and ask you to build something even if you don’t have the experience to do it which would provide you the ability to learn on the job and gain more experience.&lt;/p&gt;

&lt;p&gt;There are obviously a lot of opinions around this subject and every one is valid.&lt;/p&gt;

&lt;p&gt;I did spot that a few people said they had been head hunted because they were known from their OSS work obviously by developers in a company keeping their eyes and ears out. It could be argued that potentially that this might be a good place to work if they know you by these skills and are open and embracing of your OSS mindset. Luckily my current employer knows I like to code outside of work hours and my boss is accepting of this even though I do badger him about adopting new things constantly.&lt;/p&gt;

&lt;p&gt;What is my opinion of whether OSS is good for your career? I believe that it exposes you to new things which is beneficial, you gain more experience, you adopt best practices and if you’re lucky you become well known for your OSS work and people can see you have a certain skill and/or are a smart person which may open you up to being head hunted however, there is only a finite amount of time and my current experience has taught me I need to keep on top of the MS stack to improve my career so therefore I have to try and find a balance for MS/main stream learning and other OSS areas of interest.&lt;/p&gt;

&lt;p&gt;Let me know your experience and opinions.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me/</guid><link>http://blog.jonathanchannon.com/2012/09/21/nancyfx-ravendb-nerddinner-and-me/</link><a10:author><a10:name /></a10:author><category>community</category><category>dinner party</category><category>nancyfx</category><category>oss</category><category>ravendb</category><category>windows azure</category><title>NancyFX, RavenDB, NerdDinner and Me</title><description>&lt;p&gt;As I said in my &lt;a href="http://blog.jonathanchannon.com/2012/09/17/ive-started-blogging-why/" title="I’ve started blogging. Why?"&gt;first post&lt;/a&gt;, NancyFX was my first port of call in my OSS adventure.  The reason I had come across it was by following &lt;a href="http://twitter.com/squidge"&gt;@squidge&lt;/a&gt; and &lt;a href="http://twitter.com/cranialstrain"&gt;@cranialstrain&lt;/a&gt; on Twitter.  At the time they were talking about it quite a bit so I thought I’d take a look.  I was also keeping track of lots of people talking about &lt;a href="http://ravendb.net"&gt;RavenDB&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;What is NancyFX?&lt;/h3&gt;

&lt;p&gt;From the official docs this explains NancyFX:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Nancy is a lightweight, low-ceremony, framework for building HTTP based services on .Net and Mono. The goal of the framework is to stay out of the way as much as possible and provide a super-duper-happy-path to all interactions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This means that everything in Nancy is setup to have sensible defaults and conventions, instead of making you jump through hoops and go through configuration hell just to get up and running. With Nancy you can go from zero to website in a matter of minutes. Literally.&lt;/em&gt;&lt;/p&gt;

</description><pubDate>Thu, 20 Sep 2012 23:00:00 Z</pubDate><a10:updated>2012-09-20T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;As I said in my &lt;a href="http://blog.jonathanchannon.com/2012/09/17/ive-started-blogging-why/" title="I’ve started blogging. Why?"&gt;first post&lt;/a&gt;, NancyFX was my first port of call in my OSS adventure.  The reason I had come across it was by following &lt;a href="http://twitter.com/squidge"&gt;@squidge&lt;/a&gt; and &lt;a href="http://twitter.com/cranialstrain"&gt;@cranialstrain&lt;/a&gt; on Twitter.  At the time they were talking about it quite a bit so I thought I’d take a look.  I was also keeping track of lots of people talking about &lt;a href="http://ravendb.net"&gt;RavenDB&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;What is NancyFX?&lt;/h3&gt;

&lt;p&gt;From the official docs this explains NancyFX:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Nancy is a lightweight, low-ceremony, framework for building HTTP based services on .Net and Mono. The goal of the framework is to stay out of the way as much as possible and provide a super-duper-happy-path to all interactions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This means that everything in Nancy is setup to have sensible defaults and conventions, instead of making you jump through hoops and go through configuration hell just to get up and running. With Nancy you can go from zero to website in a matter of minutes. Literally.&lt;/em&gt;&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;When ASP.Net MVC was first introduced to the world I was hooked on the framework, it seemed so easy and logical in comparison to ASP.Net Webforms plus it had lots more &lt;a href="http://www.asp.net/mvc"&gt;benefits&lt;/a&gt;.  NancyFX took these benefits and added lots more to them. For example creating a website was very easy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HelloModule : NancyModule
{
  public HelloModule()
  {
    Get["/"] = parameters =&amp;gt; "Hello World";
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have experience using MVC and want to check out how Nancy compares check out &lt;a href="http://twitter.com/jhovgaard"&gt;@jhovgaard&lt;/a&gt; series of posts titled &lt;em&gt;“From ASP.Net MVC to Nancy”&lt;/em&gt; (&lt;a href="http://jhovgaard.net/from-aspnet-mvc-to-nancy-part-1"&gt;Part1&lt;/a&gt;, &lt;a href="http://jhovgaard.net/from-aspnet-mvc-to-nancy-part-2"&gt;2&lt;/a&gt; &amp;amp; &lt;a href="http://jhovgaard.net/from-aspnet-mvc-to-nancy-part-3"&gt;3&lt;/a&gt;)&lt;/p&gt;

&lt;h3&gt;What is RavenDB&lt;/h3&gt;

&lt;p&gt;RavenDB is a transactional, open-source Document Database written in .NET. Data in RavenDB is stored schema-less as JSON documents, and can be queried efficiently using Linq queries from .NET code or using RESTful API using other tools.&lt;/p&gt;

&lt;p&gt;What this means is that you no longer have to rely on a RDBMS as your backend data store for your .Net projects. RavenDB is part of a &lt;a href="http://en.wikipedia.org/wiki/NoSQL"&gt;NoSQL&lt;/a&gt; movement that, in simple terms, stores key-value pairs and does not store data in relational tables. It is also very quick in data retrieval.&lt;/p&gt;

&lt;h3&gt;Two birds, one stone&lt;/h3&gt;

&lt;p&gt;As with anything software related the best way to learn something is to write an application that uses that specific piece of technology. So I sent out a tweet something along the lines of &lt;em&gt;“I need to find a reason to use NancyFX and RavenDB”.&lt;/em&gt;Two minutes later I got a response from &lt;a href="http://twitter.com/TheCodeJunkie"&gt;@TheCodeJunkie&lt;/a&gt;, one of the lead developers of NancyFX saying he had wanted to port &lt;a href="http://nerddinner.com"&gt;NerdDinner&lt;/a&gt; to use NancyFX but had not had the time and wondered if I would be interested in doing that and using RavenDB as the data store.  I instantly said yes.&lt;/p&gt;

&lt;p&gt;This was my first jump into actually contributing to OSS and I was very excited and honoured.  I thought this was amazing, I thought I’d made it and believed this was going to change everything.  I went home and told my wife that my newest pet project was going to be this and she gave her usual supportive but ambiguous “that’s nice dear” type response.&lt;/p&gt;

&lt;p&gt;I got stuck in by browsing the NerdDinner source code and began getting the Razor views working with Nancy. I started converting all the ASP.Net MVC controllers into Nancy Modules and then had an idea for Nancy.  A domain specific language (DSL).  Nancy uses various ways to capture querystring arguments, one of them being regex.  To cut a long story short I created &lt;a href="https://github.com/jchannon/Nancy.RouteHelpers"&gt;Nancy.Routehelpers&lt;/a&gt; and put it on NuGet.  It was a way to define routes in the modules without the need for using Regex because lets be honest no-one &lt;em&gt;likes&lt;/em&gt; using Regex!&lt;/p&gt;

&lt;p&gt;It meant I could use this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class HomeModule : NancyModule
{
  public HomeModule()
  {
    //Yay! Readable routes :)
    Get[Route.Root().AnyIntAtLeastOnce("id")] = parameters =&amp;amp;gt;
    {
      return View["Index"];
    };

    //Boooo! Regex :(
    Get["/(?[\d]%2B)"] = parameters =&amp;amp;gt;
    {
      return View["Index"];
    };
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now by helping out and learning stuff at the same time I was already contributing to the community with my own ideas, even though in the small league however, it was all very exciting!&lt;/p&gt;

&lt;p&gt;As the logic started to wind up and I needed to put in the data store I was having teething trouble with Raven.  A quick tweet asking for help and &lt;a href="http://twitter.com/@philjoness88"&gt;@philjones88&lt;/a&gt; to the rescue.  As we were chatting we both realised we only live 40 miles apart! Small world but no matter how large, another great sign of the active community willing to help. I’ve gone on to chat to Phil numerous times and he’s been a huge help not just in RavenDB. I would highly recommend him for anyone needing any contract work as he is a true professional.&lt;/p&gt;

&lt;p&gt;Luckily integrating RavenDB was fairly painless, if I did have any quick questions I would pester &lt;a href="http://twitter.com/@philjoness88"&gt;@philjones88&lt;/a&gt; or use RavenDB’s &lt;a href="http://jabbr.net/#/rooms/RavenDB"&gt;Jabbr&lt;/a&gt; room or use the official &lt;a href="https://groups.google.com/forum/?fromgroups#!forum/ravendb"&gt;RavenDB Google group&lt;/a&gt;. The only two areas that stand out from memory that caused a few scratches of the head were indexing and clearing out documents of a certain age. The latter was needed due to the database being on a free account with &lt;a href="https://ravenhq.com/"&gt;RavenHQ&lt;/a&gt; as it had a database size limit of 15mb.  I’ll post the snippet here just in case anyone comes by and wants to  use it to delete all their documents from RavenDB:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private void CleanUpDB(IDocumentSession DocSession)
{
    var configInfo = DocSession.Load("DinnerParty/Config");

    if (configInfo == null)
    {
        configInfo = [new][18] Config();
        configInfo.Id = "DinnerParty/Config";
        configInfo.LastTruncateDate = DateTime.Now.AddHours(-48); //No need to delete data if config doesnt exist but setup ready for next time

        DocSession.Store(configInfo);
        DocSession.SaveChanges();

        return;
    }
    else
    {
        if ((DateTime.Now - configInfo.LastTruncateDate).TotalHours &amp;lt; 24)
            return;

        configInfo.LastTruncateDate = DateTime.Now;
        DocSession.SaveChanges();

        //If database size &amp;gt; 15mb or 1000 documents delete documents older than a week

#if DEBUG
        var jsonData = RavenSessionProvider.DocumentStore.JsonRequestFactory.CreateHttpJsonRequest(null, "http://localhost:8080/database/size", "GET", RavenSessionProvider.DocumentStore.Credentials, RavenSessionProvider.DocumentStore.Conventions).ReadResponseJson();

#else
        var jsonData = RavenSessionProvider.DocumentStore.JsonRequestFactory.CreateHttpJsonRequest(null, "https://1.ravenhq.com/databases/DinnerParty-DinnerPartyDB/database/size", "GET", RavenSessionProvider.DocumentStore.Credentials, RavenSessionProvider.DocumentStore.Conventions).ReadResponseJson();

#endif
        int dbSize = int.Parse(jsonData.SelectToken("DatabaseSize").ToString());
        long docCount = RavenSessionProvider.DocumentStore.DatabaseCommands.GetStatistics().CountOfDocuments;


        if (docCount &amp;gt; 1000 || dbSize &amp;gt; 15000000) //its actually 14.3mb but goood enough
        {

            RavenSessionProvider.DocumentStore.DatabaseCommands.DeleteByIndex("Raven/DocumentsByEntityName",
                new IndexQuery
                {
                    Query = DocSession.Advanced.LuceneQuery()
                    .WhereEquals("Tag", "Dinners")
                    .AndAlso()
                    .WhereLessThan("LastModified", DateTime.Now.AddDays(-7)).ToString()
                },
                false);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One neat feature of the indexes within RavenDB although initially a tad confusing is the ability to define them in C# so when your application starts you can make a call to &lt;strong&gt;Raven.Client.Indexes.IndexCreation.CreateIndexes&lt;/strong&gt; and it will apply them to your database. Slightly cooler than a RDBMS way of creating an index.&lt;/p&gt;

&lt;h3&gt;It’s all in the name&lt;/h3&gt;

&lt;p&gt;As development was winding up we needed to decide upon a name instead of NerdDinner. A couple of ideas were thrown about, FancyNancy, Party with Nancy, Dinner with Nancy, all pretty lame so &lt;a href="http://twitter.com/thecodejunkie"&gt;@TheCodeJunkie&lt;/a&gt; came up with DinnerParty and that’s what we went with plus its a bit more sophisticated than NerdDinner &lt;img src="http://blog.jonathanchannon.com/wp-includes/images/smilies/icon_wink.gif" alt=";)" /&gt;&lt;/p&gt;

&lt;p&gt;The next issue was having a logo for DinnerParty, a large feat for someone who has no design skills whatsoever. I appreciate good design but have no ability in that area, in fact I’m pretty sure a 3 year old has more skills than me. I went through various rubbish designs and it went back and forth in deliberation via Jabbr but in the end the decision was mine and we went what we have today. I apologise!&lt;/p&gt;

&lt;h3&gt;Hosting&lt;/h3&gt;

&lt;p&gt;At the same time I was finishing development &lt;a href="http://www.windowsazure.com"&gt;Windows Azure&lt;/a&gt; had made a big change to its offerings and were doing all sorts of cool things including free websites! This was another tech toy I could play with so I signed up for an account so I could host DinnerParty. It was all very shiny and fun however I had found a &lt;a href="http://social.msdn.microsoft.com/Forums/en-US/windowsazurewebsitespreview/thread/9ee30e74-7546-4c1e-ac8c-0235e1589920"&gt;bug&lt;/a&gt;. It wouldn’t let me store a connection string to an external RavenDB database which was hosted at &lt;a href="https://ravenhq.com/"&gt;RavenHQ&lt;/a&gt;. Up went a forum post and a few back and forths and conversations on Twitter with &lt;a href="https://twitter.com/davidebbo"&gt;@davidebbo&lt;/a&gt; and he managed to get it working!&lt;/p&gt;

&lt;p&gt;One of the other cool features that Windows Azure was touting was “&lt;a href="http://www.windowsazure.com/en-us/develop/net/common-tasks/publishing-with-git/#Step5"&gt;Git Deploy&lt;/a&gt;“, the ability to use Git to deploy the website. Essentially it allowed me to push my source code to Windows Azure and then it would build the code and publish the compiled project to the website. Absolutely brilliant! No more FTP-ing, no more Web Deploy projects, this was genius at work. However there were two steps involved to keep the Github repository and the live website up to date, i) Push changes to Github ii) Push changes to Azure. Not a major problem at all however, those eager beavers at Microsoft have managed to get this down to one step with improved &lt;a href="http://weblogs.asp.net/scottgu/archive/2012/09/17/announcing-great-improvements-to-windows-azure-web-sites.aspx"&gt;Continuous Deployment&lt;/a&gt; so that when your changes are pushed to Github your Windows Azure account will pick this up, build the project and publish to your website. I highly recommend people check out the new Windows Azure offerings even if you just use the free websites as they offer a lot of cool functionality.&lt;/p&gt;

&lt;h3&gt;Go Live!&lt;/h3&gt;

&lt;p&gt;The code was done, the database was wired up, source control was working, website hosting was in place and a deployment strategy configured. D-day! Excited and nervous at the same time I made DinnerParty live to the world. That moment when it might all go horribly wrong, people would look at the code and laugh or use the website and it would fail. Oh well, no going back now, my time to shine! Luckily it all went well, it was publicised on Jabbr and Twitter and people started to look at it and the odd person played with the website. In my excitement I approached &lt;a href="http://ayende.com/blog"&gt;Oren Eini&lt;/a&gt; (or Ayende Rahien as others may know him as) about some publicity about this new port of NerdDinner. Oren is the lead developer of RavenDB so I thought he may be interested. He was very open to the idea and offered to publicise it via a &lt;a href="http://ayende.com/blog/156609/reviewing-dinner-party-ndash-nerd-dinner-ported-to-ravendb-on-ravenhq"&gt;blog post&lt;/a&gt; which I thought was nice of him. I read his article a week before it was due to be published and he pointed out a few things which I took on-board and changed before the blog post went live. At that point I was slightly nervous due to the things he had spotted, what else were people going to find? The day came when it was published and apparently I had faired well in the review according to other people as he is known for speaking his mind.&lt;/p&gt;

&lt;p&gt;I also got word of another &lt;a href="http://www.dvloop.com/effective-ravendb-session-management/"&gt;review &lt;/a&gt;from the guy who had helped design the NancyFX logo. He was looking at the way DinnerParty was using the per request/per RavenDB session architecture and offered an alternative approach if one was using service layers. DinnerParty only needed access to the RavenDB session in the modules so it didn’t take that approach.&lt;/p&gt;

&lt;p&gt;In reviews of developers code their natural instinct is to go defensive however, after counting to ten and looking at both reviews there was room for improvements and room for discussion in DinnerParty. Its hard to sit back and do that when you spend a lot of time developing something of your own and a lot of people have the tendency to just ignore criticism but I feel this is one area that you can progress as a developer. Letting others look at your code and explain why another approach maybe better is one of the key ways you learn. You don’t have to agree with an alternative approach but it helps your perception if there is more than one way you can look at it rather than stick with the “my way or the highway” attitude.&lt;/p&gt;

&lt;h3&gt;DinnerParty Impact&lt;/h3&gt;

&lt;p&gt;A few months have passed since DinnerParty went live and after the initial reviews and feeling of excitement what has happened? Well various large multi-national corporations approached me to go and work for them with a salary of $10m, a private jet, speedboat, holiday villas and unlimited Apple products. Not bad eh!?&lt;/p&gt;

&lt;p&gt;Unfortunately my imagination took hold there. DinnerParty the website is still up and running at , the &lt;a href="https://github.com/NancyFx/DinnerParty/"&gt;Github repository&lt;/a&gt; was moved under the NancyFX central repository and I think its become a minor port of call as a reference app for new users using NancyFX to look at, see &lt;a href="http://stackoverflow.com/questions/11532523/nancy-framework-sample-application"&gt;here&lt;/a&gt; and &lt;a href="http://stackoverflow.com/a/11253466/84539"&gt;here&lt;/a&gt;. I have also gained a lot of experience using RavenDB and NancyFX and that was my goal at the start of the project so I think I can feel happy about that.&lt;/p&gt;

&lt;p&gt;Had I made it? No.  I wasn’t suddenly being pestered by lots of people offering me highly paid jobs but it felt good to have done something for the community and to be part of something for a while.  That I believe is the key to it all.  You’re giving something which makes you feel good but you are also apart of something with other people interested in the same thing as you and also hopefully making a difference that adds to the feeling of belonging and by rolling all of that into one defines the word ‘community’.&lt;/p&gt;
</a10:content></item><item><guid isPermaLink="true">http://blog.jonathanchannon.com/2012/09/17/ive-started-blogging-why/</guid><link>http://blog.jonathanchannon.com/2012/09/17/ive-started-blogging-why/</link><a10:author><a10:name /></a10:author><category>community</category><category>first blog post</category><category>nancyfx</category><category>oss</category><title>I’ve started blogging. Why?</title><description>&lt;p&gt;So here I am, Jonathan Channon, blogger.  I never saw the point of blogging before so why now?&lt;/p&gt;

&lt;p&gt;I have read and spoken to many people regarding blogging who were all in favour of it.  I would always argue that I don’t have time, I have nothing to blog about or I’m not that insane to start blogging about stuff I barely understand just so the masses can come and hurl abuse at my ignorance.  However I recently read an &lt;a href="http://buildstarted.com/2012/08/28/how-i-learned-to-stop-worrying-and-love-my-community/"&gt;article&lt;/a&gt; by &lt;a href="http://twitter.com/buildstarted"&gt;Ben Dornis&lt;/a&gt; titled “How I learned to stop worrying and love my community”.  He outlined all the reasons why a lot of people don’t publish their code and don’t publish their thoughts online. I thought he was being modest, the man is clearly talented at what he does so he didn’t have to worry however, someone like me still had these fears.&lt;/p&gt;

</description><pubDate>Sun, 16 Sep 2012 23:00:00 Z</pubDate><a10:updated>2012-09-16T23:00:00Z</a10:updated><a10:content type="html">&lt;p&gt;So here I am, Jonathan Channon, blogger.  I never saw the point of blogging before so why now?&lt;/p&gt;

&lt;p&gt;I have read and spoken to many people regarding blogging who were all in favour of it.  I would always argue that I don’t have time, I have nothing to blog about or I’m not that insane to start blogging about stuff I barely understand just so the masses can come and hurl abuse at my ignorance.  However I recently read an &lt;a href="http://buildstarted.com/2012/08/28/how-i-learned-to-stop-worrying-and-love-my-community/"&gt;article&lt;/a&gt; by &lt;a href="http://twitter.com/buildstarted"&gt;Ben Dornis&lt;/a&gt; titled “How I learned to stop worrying and love my community”.  He outlined all the reasons why a lot of people don’t publish their code and don’t publish their thoughts online. I thought he was being modest, the man is clearly talented at what he does so he didn’t have to worry however, someone like me still had these fears.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Over the last year I have made an effort to start contributing to open source software and to take the forked road away from Microsoft.  I found lots of things and this increased my appetite for learning. If you’re not learning you become stagnant in my opinion and you become bored,  more bored and so on until you find yourself completely out of touch from the peers around you.  On the whole its not good to be ignorant in software development, there are fashions that come and go but you have a duty to keep up to date with today’s best practices.  I have learnt many things from dipping a toe in OSS, I have begun following more and more people in the software development industry on Twitter listening to their discussions and at times joining in.  I realised that I’m not actually scared of asking questions, I just have to word them a certain way just to protect my ego.  In fact I think I’m possibly one of those annoying people who just keep asking questions until I get something straight in my head.  Unfortunately that’s the way I am, things pop up in my mind and then I try to find the answer and if I don’t find it quickly I start badgering people until I get that answer or blurting out things on Twitter.  That’s because I’m terribly impatient!&lt;/p&gt;

&lt;p&gt;In my attempts to be part of the software community I came across &lt;a href="http://jabbr.net"&gt;Jabbr&lt;/a&gt;, essentially an online chat room made up of various rooms discussing different things.  This appealed to the impatient me, I could ask questions and get the answers quickly.  The more I used it the more I was hooked not just in consuming answers to my questions but reciprocally answering any questions others might have had.&lt;/p&gt;

&lt;p&gt;What I found from this new path was that I was no longer scared of publishing code online and asking questions.  I had been using &lt;a href="http://stackoverflow.com"&gt;Stackoverflow&lt;/a&gt; for a few years and had never received any abuse or criticism from the questions I have asked so why did I think blogging would be any different?  The stuff I have put on Github has not resulted in any moments that made me think “I really need to take that code down” so I have bitten the bullet and here I am.&lt;/p&gt;

&lt;p&gt;Putting yourself out there is daunting at first but the more you do the more you realise that there are a great bunch of people out there only too happy to share which encourages you to continue your learning.  One prime example I would like to single out in this area is the support given to the community by &lt;a href="https://twitter.com/grumpydev"&gt;@Grumpydev&lt;/a&gt; and &lt;a href="https://twitter.com/thecodejunkie"&gt;@TheCodeJunkie&lt;/a&gt;, the lead developers of &lt;a href="http://nancyfx.org/"&gt;NancyFX&lt;/a&gt;.  I won’t go into depth about what NancyFX is, that’s another blog post, but what these guys illustrate is a dedication to encourage and support people wanting to learn NancyFX.  What this results in is more and more people wanting to help out the community which can only be a good thing.&lt;/p&gt;

&lt;p&gt;As my wise father used to say to me over my teenage years, “the more you put in, the more you get out” so I’m hoping that my continued presence on Twitter and now this blog and my meagre contributions to OSS will enable me to learn as much as I can handle and make me a better software developer.&lt;/p&gt;
</a10:content></item></channel></rss>